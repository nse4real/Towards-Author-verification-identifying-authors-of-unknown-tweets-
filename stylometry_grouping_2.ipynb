{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stylometry_grouping_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nse4real/myrepo/blob/master/stylometry_grouping_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GROUPING OF 2 STYLOMETRY"
      ],
      "metadata": {
        "id": "XXd5xjTr71GK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bpTT2fEv1U2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd105753-e136-4a3d-eb97-9fab2d419f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') "
      ],
      "metadata": {
        "id": "bEHBUm_Q6PYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed3df1f-22a1-4885-e2c9-d295ce7cca92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORT NEEDED LIBRARIES\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re, string, unicodedata, math\n",
        "import collections as coll\n",
        "from nltk.corpus import words as nltk_words\n",
        "# Import other models needed for Classification models\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Sequential\n",
        "#from keras.layers.core import Dense\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop\n",
        "from keras.utils import np_utils\n",
        "from imutils import paths\n",
        "import argparse\n",
        "import random\n",
        "import pickle\n",
        "import cv2\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import csv\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from tabulate import tabulate\n",
        "from matplotlib import style\n",
        "style.use(\"ggplot\")"
      ],
      "metadata": {
        "id": "E29Mh3a558zm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path= '/content/drive/MyDrive/Colab Notebooks/'\n",
        "filepath= '/content/drive/MyDrive/Colab Notebooks/dataset.csv'\n",
        "print(filepath)"
      ],
      "metadata": {
        "id": "-vvC6NOJ6WI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83b248b-8e92-4a39-b919-ee7880ed9c36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the csv file\n",
        "df = pd.read_csv(filepath)\n",
        "\n",
        "# print number df rows and columns\n",
        "print(df.shape)\n",
        "  \n",
        "# print the content\n",
        "print('Content:')\n",
        "#display(df)\n",
        "\n",
        "#print the column names\n",
        "print(df.columns)\n",
        "print()\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IcW619O-6cw5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "a293e630-be4f-49ca-ca2e-9f982fe84610"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4560, 18)\n",
            "Content:\n",
            "Index(['input.access_token', 'input.exclude', 'input.tweet_fields',\n",
            "       'input.user_id', 'input.pagination_token',\n",
            "       'data.public_metrics.retweet_count', 'data.public_metrics.reply_count',\n",
            "       'data.public_metrics.like_count', 'data.public_metrics.quote_count',\n",
            "       'data.source', 'data.text', 'data.id', 'data.created_at',\n",
            "       'meta.oldest_id', 'meta.newest_id', 'meta.result_count',\n",
            "       'meta.next_token', 'meta.previous_token'],\n",
            "      dtype='object')\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  input.access_token     input.exclude  \\\n",
              "0  AAAAAAAAAAAAAAAAAAAAAAyUYgEAAAAAYT3ZR%2Bw6cIB8...  retweets,replies   \n",
              "1  AAAAAAAAAAAAAAAAAAAAAAyUYgEAAAAAYT3ZR%2Bw6cIB8...  retweets,replies   \n",
              "2  AAAAAAAAAAAAAAAAAAAAAAyUYgEAAAAAYT3ZR%2Bw6cIB8...  retweets,replies   \n",
              "3  AAAAAAAAAAAAAAAAAAAAAAyUYgEAAAAAYT3ZR%2Bw6cIB8...  retweets,replies   \n",
              "4  AAAAAAAAAAAAAAAAAAAAAAyUYgEAAAAAYT3ZR%2Bw6cIB8...  retweets,replies   \n",
              "\n",
              "                      input.tweet_fields  input.user_id  \\\n",
              "0  public_metrics,created_at,text,source       27699224   \n",
              "1  public_metrics,created_at,text,source       27699224   \n",
              "2  public_metrics,created_at,text,source       27699224   \n",
              "3  public_metrics,created_at,text,source       27699224   \n",
              "4  public_metrics,created_at,text,source       27699224   \n",
              "\n",
              "  input.pagination_token  data.public_metrics.retweet_count  \\\n",
              "0                    NaN                                  0   \n",
              "1                    NaN                                  2   \n",
              "2                    NaN                                  7   \n",
              "3                    NaN                                  0   \n",
              "4                    NaN                                  0   \n",
              "\n",
              "   data.public_metrics.reply_count  data.public_metrics.like_count  \\\n",
              "0                                1                              15   \n",
              "1                                4                              66   \n",
              "2                                3                              67   \n",
              "3                                1                              23   \n",
              "4                                0                              20   \n",
              "\n",
              "   data.public_metrics.quote_count         data.source  \\\n",
              "0                                0  Twitter for iPhone   \n",
              "1                                0  Twitter for iPhone   \n",
              "2                                1  Twitter for iPhone   \n",
              "3                                1     Twitter Web App   \n",
              "4                                0  Twitter for iPhone   \n",
              "\n",
              "                                           data.text              data.id  \\\n",
              "0  FYI for other authors…this raised $1,460 which...  1498672192766328838   \n",
              "1  Twenty years from now, this is gonna read like...  1498655705351479296   \n",
              "2  If you donate $20 or more to @WCKitchen and DM...  1498048242336120832   \n",
              "3  Our long international nightmare is just begin...  1496922333512351744   \n",
              "4                 Nostalgia. https://t.co/oh9hsNGL0U  1495551477057732609   \n",
              "\n",
              "            data.created_at       meta.oldest_id       meta.newest_id  \\\n",
              "0  2022-03-01T14:51:09.000Z  1396442583505805316  1498672192766328838   \n",
              "1  2022-03-01T13:45:38.000Z  1396442583505805316  1498672192766328838   \n",
              "2  2022-02-27T21:31:48.000Z  1396442583505805316  1498672192766328838   \n",
              "3  2022-02-24T18:57:50.000Z  1396442583505805316  1498672192766328838   \n",
              "4  2022-02-21T00:10:32.000Z  1396442583505805316  1498672192766328838   \n",
              "\n",
              "   meta.result_count                                meta.next_token  \\\n",
              "0                100  7140dibdnow9c7btw3w4sioouy7v4owjrs89d2hqa0yly   \n",
              "1                100  7140dibdnow9c7btw3w4sioouy7v4owjrs89d2hqa0yly   \n",
              "2                100  7140dibdnow9c7btw3w4sioouy7v4owjrs89d2hqa0yly   \n",
              "3                100  7140dibdnow9c7btw3w4sioouy7v4owjrs89d2hqa0yly   \n",
              "4                100  7140dibdnow9c7btw3w4sioouy7v4owjrs89d2hqa0yly   \n",
              "\n",
              "  meta.previous_token  \n",
              "0                 NaN  \n",
              "1                 NaN  \n",
              "2                 NaN  \n",
              "3                 NaN  \n",
              "4                 NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7eb78914-4aad-472a-bcb0-043527b880d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input.access_token</th>\n",
              "      <th>input.exclude</th>\n",
              "      <th>input.tweet_fields</th>\n",
              "      <th>input.user_id</th>\n",
              "      <th>input.pagination_token</th>\n",
              "      <th>data.public_metrics.retweet_count</th>\n",
              "      <th>data.public_metrics.reply_count</th>\n",
              "      <th>data.public_metrics.like_count</th>\n",
              "      <th>data.public_metrics.quote_count</th>\n",
              "      <th>data.source</th>\n",
              "      <th>data.text</th>\n",
              "      <th>data.id</th>\n",
              "      <th>data.created_at</th>\n",
              "      <th>meta.oldest_id</th>\n",
              "      <th>meta.newest_id</th>\n",
              "      <th>meta.result_count</th>\n",
              "      <th>meta.next_token</th>\n",
              "      <th>meta.previous_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAAAAAAAAAAAAAAAAAAAAAyUYgEAAAAAYT3ZR%2Bw6cIB8...</td>\n",
              "      <td>retweets,replies</td>\n",
              "      <td>public_metrics,created_at,text,source</td>\n",
              "      <td>27699224</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>FYI for other authors…this raised $1,460 which...</td>\n",
              "      <td>1498672192766328838</td>\n",
              "      <td>2022-03-01T14:51:09.000Z</td>\n",
              "      <td>1396442583505805316</td>\n",
              "      <td>1498672192766328838</td>\n",
              "      <td>100</td>\n",
              "      <td>7140dibdnow9c7btw3w4sioouy7v4owjrs89d2hqa0yly</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAAAAAAAAAAAAAAAAAAAAAyUYgEAAAAAYT3ZR%2Bw6cIB8...</td>\n",
              "      <td>retweets,replies</td>\n",
              "      <td>public_metrics,created_at,text,source</td>\n",
              "      <td>27699224</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>Twenty years from now, this is gonna read like...</td>\n",
              "      <td>1498655705351479296</td>\n",
              "      <td>2022-03-01T13:45:38.000Z</td>\n",
              "      <td>1396442583505805316</td>\n",
              "      <td>1498672192766328838</td>\n",
              "      <td>100</td>\n",
              "      <td>7140dibdnow9c7btw3w4sioouy7v4owjrs89d2hqa0yly</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAAAAAAAAAAAAAAAAAAAAAyUYgEAAAAAYT3ZR%2Bw6cIB8...</td>\n",
              "      <td>retweets,replies</td>\n",
              "      <td>public_metrics,created_at,text,source</td>\n",
              "      <td>27699224</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>If you donate $20 or more to @WCKitchen and DM...</td>\n",
              "      <td>1498048242336120832</td>\n",
              "      <td>2022-02-27T21:31:48.000Z</td>\n",
              "      <td>1396442583505805316</td>\n",
              "      <td>1498672192766328838</td>\n",
              "      <td>100</td>\n",
              "      <td>7140dibdnow9c7btw3w4sioouy7v4owjrs89d2hqa0yly</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAAAAAAAAAAAAAAAAAAAAAyUYgEAAAAAYT3ZR%2Bw6cIB8...</td>\n",
              "      <td>retweets,replies</td>\n",
              "      <td>public_metrics,created_at,text,source</td>\n",
              "      <td>27699224</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>Our long international nightmare is just begin...</td>\n",
              "      <td>1496922333512351744</td>\n",
              "      <td>2022-02-24T18:57:50.000Z</td>\n",
              "      <td>1396442583505805316</td>\n",
              "      <td>1498672192766328838</td>\n",
              "      <td>100</td>\n",
              "      <td>7140dibdnow9c7btw3w4sioouy7v4owjrs89d2hqa0yly</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAAAAAAAAAAAAAAAAAAAAAyUYgEAAAAAYT3ZR%2Bw6cIB8...</td>\n",
              "      <td>retweets,replies</td>\n",
              "      <td>public_metrics,created_at,text,source</td>\n",
              "      <td>27699224</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>Nostalgia. https://t.co/oh9hsNGL0U</td>\n",
              "      <td>1495551477057732609</td>\n",
              "      <td>2022-02-21T00:10:32.000Z</td>\n",
              "      <td>1396442583505805316</td>\n",
              "      <td>1498672192766328838</td>\n",
              "      <td>100</td>\n",
              "      <td>7140dibdnow9c7btw3w4sioouy7v4owjrs89d2hqa0yly</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7eb78914-4aad-472a-bcb0-043527b880d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7eb78914-4aad-472a-bcb0-043527b880d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7eb78914-4aad-472a-bcb0-043527b880d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DROP UNNECCESSARY COLUMNS\n",
        "columns_to_drop = ['input.access_token', 'input.exclude',\n",
        "       'input.tweet_fields', 'input.pagination_token', 'meta.oldest_id',\n",
        "       'meta.newest_id', 'meta.result_count', 'meta.next_token',\n",
        "       'meta.previous_token', 'data.public_metrics.retweet_count',\n",
        "       'data.public_metrics.reply_count', 'data.public_metrics.like_count',\n",
        "       'data.public_metrics.quote_count', 'data.source',\n",
        "       'data.id', 'data.created_at']\n",
        "df=df.drop(columns=columns_to_drop)\n",
        "print(df.columns)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nfBzY15B6j5O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "90c646df-59eb-4170-a76b-ad4c7b94138b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['input.user_id', 'data.text'], dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   input.user_id                                          data.text\n",
              "0       27699224  FYI for other authors…this raised $1,460 which...\n",
              "1       27699224  Twenty years from now, this is gonna read like...\n",
              "2       27699224  If you donate $20 or more to @WCKitchen and DM...\n",
              "3       27699224  Our long international nightmare is just begin...\n",
              "4       27699224                 Nostalgia. https://t.co/oh9hsNGL0U"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-178f72d6-e351-4a51-ada4-c67e16d79c69\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input.user_id</th>\n",
              "      <th>data.text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27699224</td>\n",
              "      <td>FYI for other authors…this raised $1,460 which...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27699224</td>\n",
              "      <td>Twenty years from now, this is gonna read like...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27699224</td>\n",
              "      <td>If you donate $20 or more to @WCKitchen and DM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27699224</td>\n",
              "      <td>Our long international nightmare is just begin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27699224</td>\n",
              "      <td>Nostalgia. https://t.co/oh9hsNGL0U</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-178f72d6-e351-4a51-ada4-c67e16d79c69')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-178f72d6-e351-4a51-ada4-c67e16d79c69 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-178f72d6-e351-4a51-ada4-c67e16d79c69');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RENAME THE COLUMNS NAMES TO SHORTER NAMES\n",
        "new_columns={'input.user_id':'user_id', 'data.text':'tweet'}\n",
        "df=df.rename(columns=new_columns)\n",
        "# print(df.head())\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "zLTrb6J56v46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e43394f-c0df-446e-cf6d-14e413f1c27d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['user_id', 'tweet'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ADD NEW COLUMN CALLED USERNAME\n",
        "authors = {\n",
        "  426044697: \"@Shteyngart\",\n",
        "  18903971: \"@harlancoben \",\n",
        "  27699224: \"@askanyone\",\n",
        "  17790667: \"@megcabot\",\n",
        "  5520952: \"@paulocoelho\",\n",
        "  83876527: \"@tejucole\"\n",
        "}\n",
        "#print(authors)\n",
        "\n",
        "df['username'] = \"\" #Add an empty column\n",
        "#df.info()\n",
        "\n",
        "for id, username in authors.items():\n",
        "  #print(df[df.user_id==id]);\n",
        "  df.loc[df.user_id==id, ['username']] = username\n",
        "\n",
        "#print userid and username columns\n",
        "print(df[['user_id','username']].head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "9cqUnPq87lH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830a946c-d73b-41bc-a079-ee6b57d5fbfc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    user_id    username\n",
            "0  27699224  @askanyone\n",
            "1  27699224  @askanyone\n",
            "2  27699224  @askanyone\n",
            "3  27699224  @askanyone\n",
            "4  27699224  @askanyone\n",
            "(4560, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "metadata": {
        "id": "KLdnIdOQ7ugd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "7ssO80E57mzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b87349-d073-468a-d908-26e2e4f0386d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4560 entries, 0 to 4559\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   user_id   4560 non-null   int64 \n",
            " 1   tweet     4560 non-null   object\n",
            " 2   username  4560 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 107.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## preprocessing functions\n",
        "def remove_URL(tweet):\n",
        "    \"\"\"Remove URLs from a sample string\"\"\"\n",
        "    return re.sub(r\"http\\S+\", \"\", tweet)\n",
        "\n",
        "def remove_numbers(tweet):\n",
        "  \"\"\"Remove words with numbers\"\"\"\n",
        "  return re.sub(\"[0-9]\",\"\",tweet)   \n",
        "\n",
        "def remove_non_ascii(tweet):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    return unicodedata.normalize('NFKD', tweet.encode('ascii', 'ignore').decode('utf-8', 'ignore'))"
      ],
      "metadata": {
        "id": "oHnnlT9r8KuN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use apply function to apply preprocessing\n",
        "df[\"tweet\"] = df[\"tweet\"].apply(lambda t: remove_URL(t)) #remove urls\n",
        "df[\"tweet\"] = df[\"tweet\"].apply(lambda t: remove_numbers(t))\n",
        "df[\"tweet\"] = df[\"tweet\"].apply(lambda t: remove_non_ascii(t))\n",
        "df[\"tweet\"] = df[\"tweet\"].apply(lambda t: t.strip())\n",
        "#print(df.shape)"
      ],
      "metadata": {
        "id": "ekDYbszJ8Sps"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## C0nvert username to categorical type\n",
        "df[\"username\"] = df[\"username\"].astype('category')\n",
        "df[\"username_code\"] = df[\"username\"].cat.codes\n",
        "ucat = df[\"username\"].values\n",
        "print(ucat.categories)\n",
        "print(np.unique(ucat.codes))"
      ],
      "metadata": {
        "id": "9ZH86EuT8ej8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47b4122-3920-4f7c-edea-2e3178ea0083"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['@Shteyngart', '@askanyone', '@harlancoben ', '@megcabot',\n",
            "       '@paulocoelho', '@tejucole'],\n",
            "      dtype='object')\n",
            "[0 1 2 3 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK FOR EMPTY TWEET\n",
        "df_empty_tweet = df[df['tweet'].str.len()<1]\n",
        "print(\"Number of empty tweet: \"+str(df_empty_tweet.shape[0]))\n",
        "df_empty_tweet.shape"
      ],
      "metadata": {
        "id": "tmMquH2m8pey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667ddf4a-ccc6-4003-fb97-4791c3d9ed95"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of empty tweet: 159\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(159, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE EMPTY TWEETS\n",
        "df2 = df[df['tweet'].str.len()>0]\n",
        "print(\"Cleaned dataset size: {0}\",df2.shape[0])\n",
        "print(df2.shape)\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "iE3Q3XW78qV8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "ed98ce15-6781-4fbf-c2f7-67319af46da6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned dataset size: {0} 4401\n",
            "(4401, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    user_id                                              tweet    username  \\\n",
              "0  27699224  FYI for other authorsthis raised $, which is a...  @askanyone   \n",
              "1  27699224  Twenty years from now, this is gonna read like...  @askanyone   \n",
              "2  27699224  If you donate $ or more to @WCKitchen and DM m...  @askanyone   \n",
              "3  27699224  Our long international nightmare is just begin...  @askanyone   \n",
              "4  27699224                                         Nostalgia.  @askanyone   \n",
              "\n",
              "   username_code  \n",
              "0              1  \n",
              "1              1  \n",
              "2              1  \n",
              "3              1  \n",
              "4              1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a217e8c2-5e0c-413f-925f-95aa088e6e54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>username</th>\n",
              "      <th>username_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27699224</td>\n",
              "      <td>FYI for other authorsthis raised $, which is a...</td>\n",
              "      <td>@askanyone</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27699224</td>\n",
              "      <td>Twenty years from now, this is gonna read like...</td>\n",
              "      <td>@askanyone</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27699224</td>\n",
              "      <td>If you donate $ or more to @WCKitchen and DM m...</td>\n",
              "      <td>@askanyone</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27699224</td>\n",
              "      <td>Our long international nightmare is just begin...</td>\n",
              "      <td>@askanyone</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27699224</td>\n",
              "      <td>Nostalgia.</td>\n",
              "      <td>@askanyone</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a217e8c2-5e0c-413f-925f-95aa088e6e54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a217e8c2-5e0c-413f-925f-95aa088e6e54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a217e8c2-5e0c-413f-925f-95aa088e6e54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform grouping"
      ],
      "metadata": {
        "id": "wX8dAsEo7zwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the columns in the dataframe,#create a new dataframe\n",
        "group_2_df = pd.DataFrame(columns=df2.columns)\n",
        "total_rows = 0;\n",
        "#for each author,\n",
        "for username_code in range(6):\n",
        "  author_tweets = df2.loc[df2.username_code==username_code]\n",
        "  #Get the total number of rows in the dataset\n",
        "  rows =author_tweets.shape[0]\n",
        "  tweet_index = author_tweets.index\n",
        "  print(f\"The number of rows for author: {username_code} is {rows}\")\n",
        "  #loop through the df and merge two consercutive tweets\n",
        "  i = 0\n",
        "  while i < (rows-1):\n",
        "    #add row to the new dataframe\n",
        "    row = author_tweets.loc[tweet_index[i]]\n",
        "    tweet = author_tweets.loc[tweet_index[i],[\"tweet\"]] + \" \" + author_tweets.loc[tweet_index[i+1],[\"tweet\"]]\n",
        "    #row[\"tweet\"] = tweet\n",
        "    group_2_df.loc[total_rows] = row\n",
        "    group_2_df.loc[total_rows,[\"tweet\"]] = tweet\n",
        "    i+=2\n",
        "    total_rows +=1\n",
        "\n",
        "print(\"The grouped df shape is: \", group_2_df.shape)\n",
        "\n",
        "group_2_df[\"username\"] = group_2_df[\"username\"].astype('category')\n",
        "group_2_df[\"username_code\"] = group_2_df[\"username\"].cat.codes\n",
        "ucat = group_2_df[\"username\"].values\n",
        "print(ucat.categories)\n",
        "print(np.unique(ucat.codes))\n",
        "group_2_df.head()"
      ],
      "metadata": {
        "id": "7cBpw4zF-aU6",
        "outputId": "07d17b0f-a11c-4c8e-ba96-fa051af10a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of rows for author: 0 is 840\n",
            "The number of rows for author: 1 is 761\n",
            "The number of rows for author: 2 is 754\n",
            "The number of rows for author: 3 is 697\n",
            "The number of rows for author: 4 is 651\n",
            "The number of rows for author: 5 is 698\n",
            "The grouped df shape is:  (2199, 4)\n",
            "Index(['@Shteyngart', '@askanyone', '@harlancoben ', '@megcabot',\n",
            "       '@paulocoelho', '@tejucole'],\n",
            "      dtype='object')\n",
            "[0 1 2 3 4 5]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     user_id                                              tweet     username  \\\n",
              "0  426044697  A brilliant look at Ukraines history through a...  @Shteyngart   \n",
              "1  426044697  There might just be a God. I was wondering whe...  @Shteyngart   \n",
              "2  426044697  Question: How can we remove Russia from its pe...  @Shteyngart   \n",
              "3  426044697  My math is terrible, but if  troops were kille...  @Shteyngart   \n",
              "4  426044697  To the barricades! At yesterdays PEN vigil for...  @Shteyngart   \n",
              "\n",
              "   username_code  \n",
              "0              0  \n",
              "1              0  \n",
              "2              0  \n",
              "3              0  \n",
              "4              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-121e2cb2-9d54-48a4-bce2-1066885ccb93\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>username</th>\n",
              "      <th>username_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>426044697</td>\n",
              "      <td>A brilliant look at Ukraines history through a...</td>\n",
              "      <td>@Shteyngart</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>426044697</td>\n",
              "      <td>There might just be a God. I was wondering whe...</td>\n",
              "      <td>@Shteyngart</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>426044697</td>\n",
              "      <td>Question: How can we remove Russia from its pe...</td>\n",
              "      <td>@Shteyngart</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>426044697</td>\n",
              "      <td>My math is terrible, but if  troops were kille...</td>\n",
              "      <td>@Shteyngart</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>426044697</td>\n",
              "      <td>To the barricades! At yesterdays PEN vigil for...</td>\n",
              "      <td>@Shteyngart</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-121e2cb2-9d54-48a4-bce2-1066885ccb93')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-121e2cb2-9d54-48a4-bce2-1066885ccb93 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-121e2cb2-9d54-48a4-bce2-1066885ccb93');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction"
      ],
      "metadata": {
        "id": "XmAmrv7fHu-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dictionary words\n",
        "dictionary = nltk_words.words();\n",
        "english_words = set(word.strip().lower() for word in dictionary)\n",
        "\n",
        "# characters or numbers appearing 3 or more consecutive times in a string  \n",
        "# Compile the ReGex //\"\\\\b([a-zA-Z0-9])\\\\1\\\\1+\\\\b\"\n",
        "repeating_re = re.compile(r\"(.)\\1{2,}\")\n",
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")"
      ],
      "metadata": {
        "id": "lJoZ7BtlHrgJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Lexical and syntactic Feature Extraction\n",
        "def lexical_feature_extration(tweet):\n",
        "  tk_no_puntuation = tokenizer.tokenize(tweet)\n",
        "  tk_punctuation = tweet.split(\" \")\n",
        "  \n",
        "  # number of words per tweet\n",
        "  num_words = len(tk_punctuation)\n",
        "\n",
        "  # number of sentences\n",
        "  sentences = nltk.sent_tokenize(tweet)\n",
        "  num_sentences = len(sentences)\n",
        "\n",
        "  #number of begining words capitalized, #number of punctuation per sentence, #number of uppercase letters\n",
        "  num__beginning_sent_capital = 0\n",
        "  num_punct_sent = 0\n",
        "  num_uppers_sent = 0\n",
        "  for s in sentences:\n",
        "    if s[0].isupper():\n",
        "      num__beginning_sent_capital +=1\n",
        "    for char in s:\n",
        "      if char in string.punctuation:\n",
        "          num_punct_sent +=1\n",
        "      if char.isupper():\n",
        "        num_uppers_sent +=1\n",
        "\n",
        "  # number of words for per sentences\n",
        "  num_sent_words = [len(s.split(\" \")) for s in sentences]\n",
        "  word_per_sent = sum(num_sent_words)/num_sentences\n",
        "\n",
        "  #Frequency of dictionary words\n",
        "  freq_dict_words = sum([1 if word.lower() in english_words else 0 for word in tk_no_puntuation])\n",
        "\n",
        "  #Frequency of word extensions\n",
        "  freq_words_ex = sum([1 if re.search(repeating_re, word) else 0 for word in tk_no_puntuation])\n",
        "\n",
        "  #lexical diversity\n",
        "  #print(tk_punctuation)\n",
        "  lex_diversity = len(tk_no_puntuation)/len(set(tk_no_puntuation)) if len(tk_no_puntuation)>0 else 0\n",
        "\n",
        "  return pd.Series([num_words,num_sentences,word_per_sent,freq_dict_words,freq_words_ex,lex_diversity,\n",
        "                    num__beginning_sent_capital,num_punct_sent,num_uppers_sent])"
      ],
      "metadata": {
        "id": "_0uBqigAH8KJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply lexical feature extraction\n",
        "group_2_df[['num_words','num_sentences','word_per_sent','freq_dict_words','freq_words_ex','lex_diversity','num__beginning_sent_capital','num_punct_sent','num_uppers_sent']] = group_2_df['tweet'].apply(lexical_feature_extration)\n",
        "\n",
        "#add columns mimicry_by_length and mimicry_by_word to list\n",
        "group_2_df['mimicry_by_length'] = np.nan\n",
        "group_2_df['mimicry_by_word'] = np.nan\n",
        "\n",
        "#mimicry by length code\n",
        "#get each authors tweet\n",
        "for username in authors.values():\n",
        "  #print(df[df.user_id==id]);\n",
        "  author_data = group_2_df[group_2_df.username==username]\n",
        "  #sort it\n",
        "  author_tweets = author_data['tweet']\n",
        "  #get the list of keys\n",
        "  tweet_index_list = author_tweets.index\n",
        "  #get mimicry length and mimicry by word\n",
        "  mimicry_length = []\n",
        "  for i in range(len(tweet_index_list)-1):\n",
        "      currentKey = tweet_index_list[i]\n",
        "      nextKey = tweet_index_list[i+1]\n",
        "      #mimicry by length\n",
        "      group_2_df.loc[currentKey,'mimicry_by_length'] = len(author_tweets[currentKey])/len(author_tweets[nextKey])\n",
        "      #mimicry by word\n",
        "      current_tweet_words = set(tokenizer.tokenize(author_tweets[currentKey]))\n",
        "      next_tweet_words = set(tokenizer.tokenize(author_tweets[nextKey]))\n",
        "      group_2_df.loc[currentKey,'mimicry_by_word'] = len(current_tweet_words.intersection(next_tweet_words))\n",
        "      \n",
        "  group_2_df.loc[tweet_index_list[-1],'mimicry_by_length'] = 0\n",
        "  group_2_df.loc[tweet_index_list[-1],'mimicry_by_word'] = 0\n",
        "  \n",
        "group_2_df.head()"
      ],
      "metadata": {
        "id": "OkabUg1gIDDA",
        "outputId": "27bcd6b8-f15e-4426-f128-c8b3d22b9c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     user_id                                              tweet     username  \\\n",
              "0  426044697  A brilliant look at Ukraines history through a...  @Shteyngart   \n",
              "1  426044697  There might just be a God. I was wondering whe...  @Shteyngart   \n",
              "2  426044697  Question: How can we remove Russia from its pe...  @Shteyngart   \n",
              "3  426044697  My math is terrible, but if  troops were kille...  @Shteyngart   \n",
              "4  426044697  To the barricades! At yesterdays PEN vigil for...  @Shteyngart   \n",
              "\n",
              "   username_code  num_words  num_sentences  word_per_sent  freq_dict_words  \\\n",
              "0              0       17.0            3.0       5.666667             16.0   \n",
              "1              0       39.0            3.0      13.333333             36.0   \n",
              "2              0       25.0            4.0       6.750000             25.0   \n",
              "3              0       32.0            2.0      16.000000             25.0   \n",
              "4              0        9.0            2.0       4.500000              6.0   \n",
              "\n",
              "   freq_words_ex  lex_diversity  num__beginning_sent_capital  num_punct_sent  \\\n",
              "0            0.0       1.000000                          3.0             4.0   \n",
              "1            0.0       1.081081                          3.0             3.0   \n",
              "2            0.0       1.000000                          4.0             6.0   \n",
              "3            0.0       1.035714                          2.0             4.0   \n",
              "4            0.0       1.000000                          2.0             2.0   \n",
              "\n",
              "   num_uppers_sent  mimicry_by_length  mimicry_by_word  \n",
              "0              4.0           0.490385              1.0  \n",
              "1              8.0           1.118280              4.0  \n",
              "2              8.0           1.141104              2.0  \n",
              "3              6.0           2.963636              1.0  \n",
              "4              6.0           0.327381              1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c67c1c7f-0f78-4627-876a-1ded6f3b59f7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>username</th>\n",
              "      <th>username_code</th>\n",
              "      <th>num_words</th>\n",
              "      <th>num_sentences</th>\n",
              "      <th>word_per_sent</th>\n",
              "      <th>freq_dict_words</th>\n",
              "      <th>freq_words_ex</th>\n",
              "      <th>lex_diversity</th>\n",
              "      <th>num__beginning_sent_capital</th>\n",
              "      <th>num_punct_sent</th>\n",
              "      <th>num_uppers_sent</th>\n",
              "      <th>mimicry_by_length</th>\n",
              "      <th>mimicry_by_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>426044697</td>\n",
              "      <td>A brilliant look at Ukraines history through a...</td>\n",
              "      <td>@Shteyngart</td>\n",
              "      <td>0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.666667</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.490385</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>426044697</td>\n",
              "      <td>There might just be a God. I was wondering whe...</td>\n",
              "      <td>@Shteyngart</td>\n",
              "      <td>0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13.333333</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.081081</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.118280</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>426044697</td>\n",
              "      <td>Question: How can we remove Russia from its pe...</td>\n",
              "      <td>@Shteyngart</td>\n",
              "      <td>0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.750000</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.141104</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>426044697</td>\n",
              "      <td>My math is terrible, but if  troops were kille...</td>\n",
              "      <td>@Shteyngart</td>\n",
              "      <td>0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.035714</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.963636</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>426044697</td>\n",
              "      <td>To the barricades! At yesterdays PEN vigil for...</td>\n",
              "      <td>@Shteyngart</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.327381</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c67c1c7f-0f78-4627-876a-1ded6f3b59f7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c67c1c7f-0f78-4627-876a-1ded6f3b59f7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c67c1c7f-0f78-4627-876a-1ded6f3b59f7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing classification"
      ],
      "metadata": {
        "id": "26FFU6KqJC_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def balanceData(balanceType, trainX, trainY):\n",
        "    # [\"None\", \"SMOTE\", \"RandomOver\", \"RandomUnder\"]\n",
        "    if balanceType == \"None\":\n",
        "        x_train_res = trainX\n",
        "        y_train_res = trainY\n",
        "    if balanceType == \"SMOTE\":\n",
        "        sm = SMOTE(random_state=12)\n",
        "        x_train_res, y_train_res = sm.fit_resample(trainX, trainY)\n",
        "    if balanceType == \"RandomOver\":\n",
        "        sm = RandomOverSampler(random_state=12)\n",
        "        x_train_res, y_train_res = sm.fit_sample(trainX, trainY)\n",
        "    if balanceType == \"RandomUnder\":\n",
        "        sm = RandomUnderSampler(random_state=12)\n",
        "        x_train_res, y_train_res = sm.fit_sample(trainX, trainY)\n",
        "\n",
        "    return x_train_res, y_train_res\n",
        "        \n",
        "def initNeuralNetModel(trainX, trainY,numFeatures):\n",
        "    train_Y = np_utils.to_categorical(trainY)\n",
        "    inputShape = (numFeatures,)\n",
        "    class_weight1 = {0: 1.,1: 20.}\n",
        "    y_ints = [y.argmax() for y in train_Y]\n",
        "    train_classes = np.unique(y_ints)\n",
        "    num_classes = len(train_classes)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_shape=inputShape))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dense(32))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation(tf.nn.softmax))\n",
        "\n",
        "    #opt = RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    # opt = RMSprop(0.001)\n",
        "\n",
        "    # Let's train the model using RMSprop\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def doNeuralNetAnalysis(model, trainX, trainY, testX, testY, numFeatures):\n",
        "    print(\"Doing analysis with fully connected neural network\")\n",
        "    train_Y = np_utils.to_categorical(trainY)\n",
        "    test_Y = np_utils.to_categorical(testY)\n",
        "    inputShape = (numFeatures,)\n",
        "    class_weight1 = {0: 1.,1: 20.}\n",
        "    y_ints = [y.argmax() for y in train_Y]\n",
        "    train_classes = np.unique(y_ints)\n",
        "    chanDim = 1\n",
        "    num_classes = len(train_classes)\n",
        "\n",
        "    # initialize our initial learning rate and # of epochs to train for\n",
        "    INIT_LR = 0.01\n",
        "    EPOCHS = 4\n",
        "\n",
        "    # compile the model using SGD as our optimizer and categorical\n",
        "    # cross-entropy loss (you'll want to use binary_crossentropy\n",
        "    # for 2-class classification)\n",
        "    print(\"[INFO] training network...\")\n",
        "    # opt = SGD(lr=INIT_LR)\n",
        "    # model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "    #              metrics=[\"accuracy\"])\n",
        "    # model.compile(loss='binary_crossentropy',\n",
        "    #              optimizer=opt,\n",
        "    #              metrics=[sensitivity, specificity])\n",
        "    class_weights = compute_class_weight(class_weight = \"balanced\",classes=train_classes, y = y_ints)\n",
        "    class_weights = dict(zip(np.unique(train_classes), class_weights))\n",
        "    #print(class_weights)\n",
        "\n",
        "    # train the neural network\n",
        "    model.fit(trainX, train_Y, validation_data=(testX, test_Y), epochs=EPOCHS, batch_size=32,\n",
        "                  class_weight=class_weights, verbose=0)\n",
        "\n",
        "    # evaluate the network\n",
        "    print(\"[INFO] evaluating network...\")\n",
        "    #preds = model.predict(testX, verbose=0)\n",
        "    #predictions = preds.argmax(axis=1)\n",
        "    #print(classification_report(testY, predictions, target_names=[\"0\", \"1\"]))\n",
        "    #print(confusion_matrix(testY, predictions))\n",
        "    return model\n",
        "\n",
        "def getClassifierCode(classifierType):\n",
        "    #[\"ANN\", \"SVM\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"Naive Bayes\"]\n",
        "    code = \"\"\n",
        "    if classifierType == \"ANN\":\n",
        "        return \"ann\"\n",
        "    if classifierType == \"SVM\":\n",
        "        return \"svm\"\n",
        "    if classifierType == \"Decision Tree\":\n",
        "        return \"dt\"\n",
        "    if classifierType == \"Random Forest\":\n",
        "        return \"rf\"\n",
        "    if classifierType == \"Naive Bayes\":\n",
        "        return \"NB\"\n",
        "    if classifierType == \"Logistic Regression\":\n",
        "      return \"LR\"   \n",
        "\n",
        "    return code\n",
        "\n",
        "def getBalanceCode(balanceType):\n",
        "    #[\"None\", \"SMOTE\", \"RandomOver\", \"RandomUnder\"]\n",
        "    code = \"\"\n",
        "    if balanceType == \"None\":\n",
        "        code = '-'\n",
        "    if balanceType == \"SMOTE\":\n",
        "        code = 's'\n",
        "    if balanceType == \"RandomOver\":\n",
        "        code = 'ro'\n",
        "    if balanceType == \"RandomUnder\":\n",
        "        code = 'ru'\n",
        "    return code"
      ],
      "metadata": {
        "id": "STIiorH6I_Ty"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(group_2_df.columns)\n",
        "# We did not consider the non-stylometric feature 'retweet_count', 'reply_count', 'like_count', 'quote_count','tweet_device_code'\n",
        "data = group_2_df[['num_words', 'num_sentences','word_per_sent','freq_dict_words','freq_words_ex','lex_diversity','mimicry_by_length','mimicry_by_word','num__beginning_sent_capital', 'num_punct_sent', 'num_uppers_sent']]\n",
        "labels = group_2_df['username_code']\n",
        "numFeatures = 11"
      ],
      "metadata": {
        "id": "bcF-nnh1JbW9",
        "outputId": "4c3bb5b4-a57a-4aa9-cb94-4b3879433887",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['user_id', 'tweet', 'username', 'username_code', 'num_words',\n",
            "       'num_sentences', 'word_per_sent', 'freq_dict_words', 'freq_words_ex',\n",
            "       'lex_diversity', 'num__beginning_sent_capital', 'num_punct_sent',\n",
            "       'num_uppers_sent', 'mimicry_by_length', 'mimicry_by_word'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### FEATURE RANKING\n",
        "### https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
        "## ANOVA F-value\n",
        "\n",
        "# feature extraction\n",
        "test = SelectKBest(score_func=f_classif, k=\"all\")\n",
        "fit = test.fit(data, labels)\n",
        "# summarize scores\n",
        "np.set_printoptions(precision=3)\n",
        "print(\"List of Features and the scores\\n\")\n",
        "\n",
        "print(tabulate([fit.scores_], headers=data.columns))\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "features = fit.transform(data)\n",
        "# summarize selected features\n",
        "print(tabulate(features[0:5,:], headers = data.columns))\n",
        "print(\"The number of row, columns \",data.shape)"
      ],
      "metadata": {
        "id": "ytEtSPshJl2v",
        "outputId": "96b57e7c-8ccc-4346-987f-29a0c40263c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of Features and the scores\n",
            "\n",
            "  num_words    num_sentences    word_per_sent    freq_dict_words    freq_words_ex    lex_diversity    mimicry_by_length    mimicry_by_word    num__beginning_sent_capital    num_punct_sent    num_uppers_sent\n",
            "-----------  ---------------  ---------------  -----------------  ---------------  ---------------  -------------------  -----------------  -----------------------------  ----------------  -----------------\n",
            "    57.3864          40.9296            67.34            84.1164          1.80724          13.4121              2.59821             62.021                        47.5485            11.072            43.6816\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  num_words    num_sentences    word_per_sent    freq_dict_words    freq_words_ex    lex_diversity    mimicry_by_length    mimicry_by_word    num__beginning_sent_capital    num_punct_sent    num_uppers_sent\n",
            "-----------  ---------------  ---------------  -----------------  ---------------  ---------------  -------------------  -----------------  -----------------------------  ----------------  -----------------\n",
            "         17                3          5.66667                 16                0          1                   0.490385                  1                              3                 4                  4\n",
            "         39                3         13.3333                  36                0          1.08108             1.11828                   4                              3                 3                  8\n",
            "         25                4          6.75                    25                0          1                   1.1411                    2                              4                 6                  8\n",
            "         32                2         16                       25                0          1.03571             2.96364                   1                              2                 4                  6\n",
            "          9                2          4.5                      6                0          1                   0.327381                  1                              2                 2                  6\n",
            "The number of row, columns  (2199, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open all files for writing everytime the program is ran from the beginning\n",
        "filenames = ['output-group-2.csv','pca-output-group-2.csv']\n",
        "#Header\n",
        "header = ['Experiment-Number','Grouping-Number','Balance-Code','Classifier-Code','Split-Number','Accuracy','Total-F1','cm']\n",
        "for outputFilename in filenames:\n",
        "  # open the file using open() function\n",
        "  with open(path+outputFilename, 'w', newline='') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(header)\n",
        "\n",
        "test_header = ['Experiment-Number','Grouping-Number','Balance-Code','Classifier-Code','Accuracy','Total-F1','cm']\n",
        "for outputFilename in filenames:\n",
        "  name = path+\"test-\"+outputFilename\n",
        "  with open(name, 'w', newline='') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(test_header)    "
      ],
      "metadata": {
        "id": "cCv3FIBgKHT4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deal with data imbalance ofter stratified k fold split for cross validation\n",
        "balanceTypes = [\"None\", \"SMOTE\", \"RandomOver\", \"RandomUnder\"]\n",
        "analysisTypes = [\"ANN\", \"SVM\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"Naive Bayes\"]\n",
        "resultsList = []\n",
        "testResultList = []\n",
        "\n",
        "def perform_experiment(groupingCode, experimentNumber, data, labels, outputFilename):  \n",
        "  #Split the overall dataset into  training and testing\n",
        "  training_data, testing_data = train_test_split(data, test_size=0.1, random_state=34)\n",
        "  training_labels = labels[training_data.index]\n",
        "  testing_labels = labels[testing_data.index]\n",
        "\n",
        "  print(f\"No. of training examples: {training_data.shape[0]}\")\n",
        "  print(f\"No. of testing examples: {testing_data.shape[0]}\")\n",
        "\n",
        "  models=[initNeuralNetModel(training_data, training_labels,numFeatures),\n",
        "          SVC(kernel='rbf',gamma=\"auto\"),\n",
        "          LogisticRegression(random_state=34, C=1, multi_class='auto', solver='lbfgs',max_iter=100),\n",
        "          DecisionTreeClassifier(criterion=\"gini\", random_state=34, max_depth=5, min_samples_leaf=5),\n",
        "          RandomForestClassifier(n_estimators=100), GaussianNB()]\n",
        "\n",
        "  # Running classifiers for balance type = SMOTE\n",
        "  balanceType = \"SMOTE\"\n",
        "  resultsList = []\n",
        "  testResultList = []\n",
        "  # Using 5 splits for k-fold cross validation.\n",
        "  skf = StratifiedKFold(n_splits=4,random_state=25, shuffle=True)\n",
        "  split = -1\n",
        "  for train, test in skf.split(training_data, training_labels):\n",
        "      split = split + 1\n",
        "      trainX = np.take(training_data, train, axis=0)\n",
        "      trainY = np.take(training_labels, train, axis=0)\n",
        "      testX = np.take(training_data, test, axis=0)\n",
        "      testY = np.take(training_labels, test, axis=0)\n",
        "\n",
        "      # deal with the imbalance on the training set alone (so duplication doesn't bleed across test/train split)\n",
        "      trainX, trainY = balanceData(balanceType, trainX, trainY)\n",
        "\n",
        "      for analysisType in analysisTypes:\n",
        "          print(\"Doing analysis with {}\".format(analysisType))\n",
        "          if analysisType == \"All\":\n",
        "              #print(\"Doing analysis with Linear Regression\")\n",
        "              #print(\"But it's not actually implemented...\")\n",
        "              preditions = testY + 1\n",
        "          else:\n",
        "              model = None\n",
        "              if analysisType == \"ANN\":\n",
        "                models[0] = doNeuralNetAnalysis(models[0], trainX, trainY, testX, testY, numFeatures)\n",
        "                preds = models[0].predict(testX, verbose=0)\n",
        "                predictions = preds.argmax(axis=1)\n",
        "                #predictions = doNeuralNetAnalysis(trainX, trainY, testX, testY, numFeatures)\n",
        "              elif analysisType == \"SVM\":\n",
        "                  #svclassifier = SVC(kernel='linear')\n",
        "                  #svclassifier = SVC(kernel='poly', degree=8)\n",
        "                  #model = SVC(kernel='rbf', gamma=\"auto\")\n",
        "                  models[1].fit(trainX, trainY)\n",
        "                  predictions = models[1].predict(testX)\n",
        "                  #svclassifier = SVC(kernel='sigmoid')\n",
        "              elif analysisType == \"Logistic Regression\":\n",
        "                  #model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "                  models[2].fit(trainX, trainY)\n",
        "                  predictions = models[2].predict(testX)   \n",
        "              elif analysisType == \"Decision Tree\":\n",
        "                  #model = DecisionTreeClassifier(criterion=\"gini\", random_state=100, max_depth=3, min_samples_leaf=5)\n",
        "                  models[3].fit(trainX, trainY)\n",
        "                  predictions = models[3].predict(testX)\n",
        "              elif analysisType == \"Random Forest\":\n",
        "                  #model = RandomForestClassifier(n_estimators=100)\n",
        "                  models[4].fit(trainX, trainY)\n",
        "                  predictions = models[4].predict(testX)\n",
        "              elif analysisType == \"Naive Bayes\":\n",
        "                  #model = GaussianNB()\n",
        "                  models[5].fit(trainX, trainY)\n",
        "                  predictions = models[5].predict(testX)\n",
        "              \n",
        "              #scores = cross_val_score(model, data, labels, cv=5)\n",
        "              print(classification_report(testY, predictions))\n",
        "              cm = confusion_matrix(testY, predictions)\n",
        "              print(cm)\n",
        "              accuracy = accuracy_score(testY, predictions)\n",
        "              print(f\"Accuracy: {accuracy}\")\n",
        "              f1s = f1_score(testY, predictions, average=None)\n",
        "              total_f1s = np.sum(f1s)\n",
        "              print(\"Total f1 over classes: {}\".format(total_f1s))\n",
        "              balanceCode = getBalanceCode(balanceType)\n",
        "              classifierCode = getClassifierCode(analysisType)\n",
        "              turple = [experimentNumber, groupingCode, balanceCode, classifierCode, split, accuracy, total_f1s,cm.reshape([1,36]).tolist()]\n",
        "              #print(turple)\n",
        "              resultsList.append(turple)\n",
        "\n",
        "  for i in range(len(models)):\n",
        "      predictions = []\n",
        "      print(\"Doing test analysis with {}\".format(analysisTypes[i]))\n",
        "      if analysisTypes[i] == \"ANN\":\n",
        "        preds = models[0].predict(testing_data, verbose=0)\n",
        "        predictions = preds.argmax(axis=1)\n",
        "      else:\n",
        "         predictions = models[i].predict(testing_data)\n",
        "\n",
        "      #print(predictions)\n",
        "      print(classification_report(testing_labels, predictions))\n",
        "      cm = confusion_matrix(testing_labels, predictions)\n",
        "      print(cm)\n",
        "      accuracy = accuracy_score(testing_labels, predictions)\n",
        "      print(f\"Accuracy: {accuracy}\")\n",
        "      f1s = f1_score(testing_labels, predictions, average=None)\n",
        "      total_f1s = np.sum(f1s)\n",
        "      print(\"Total f1 over classes: {}\".format(total_f1s))\n",
        "      #featureCode = getFeatureCode(dataFiles, addBorders)\n",
        "      balanceCode = getBalanceCode(balanceType)\n",
        "      classifierCode = getClassifierCode(analysisTypes[i])\n",
        "      turple = [experimentNumber, groupingCode, balanceCode, classifierCode, accuracy, total_f1s,cm.reshape([1,36]).tolist()]\n",
        "      #print(turple)\n",
        "      testResultList.append(turple)\n",
        "\n",
        "  #print(\"****************************************************************\")\n",
        "  #print(\"EXPERIMENT VALIDATION RESULT\")\n",
        "  #print(tabulate(resultsList, headers = header))\n",
        "  with open(path+outputFilename, 'a+', newline='') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerows(resultsList)\n",
        "\n",
        "\n",
        "  #print(\"****************************************************************\")\n",
        "  #print(\"EXPERIMENT TEST RESULT\")\n",
        "  #print(tabulate(resultsList, headers = header))\n",
        "  name = path+\"test-\"+outputFilename\n",
        "  with open(name, 'a+', newline='') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerows(testResultList)     \n",
        "\n",
        "perform_experiment('G-1', 1,data, labels,filenames[0])\n",
        "perform_experiment('G-1', 2,data, labels,filenames[0])\n",
        "perform_experiment('G-1', 3,data, labels,filenames[0])\n",
        "perform_experiment('G-1', 4,data, labels,filenames[0])\n",
        "perform_experiment('G-1', 5,data, labels,filenames[0])"
      ],
      "metadata": {
        "id": "jCZKnhAuKNj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3950000e-46af-4a1d-b07c-39d9dd71e1ca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples: 1979\n",
            "No. of testing examples: 220\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.15      0.21        96\n",
            "           1       0.45      0.41      0.43        85\n",
            "           2       0.21      0.52      0.30        84\n",
            "           3       0.46      0.08      0.13        79\n",
            "           4       0.38      0.58      0.46        72\n",
            "           5       0.23      0.13      0.16        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.35      0.31      0.28       495\n",
            "weighted avg       0.35      0.31      0.28       495\n",
            "\n",
            "[[14 15 41  0 14 12]\n",
            " [ 5 35 27  1 13  4]\n",
            " [ 5  4 44  4 17 10]\n",
            " [10  4 36  6 19  4]\n",
            " [ 2  8 14  2 42  4]\n",
            " [ 4 12 46  0  7 10]]\n",
            "Accuracy: 0.30505050505050507\n",
            "Total f1 over classes: 1.6862582164710018\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.38        96\n",
            "           1       0.29      0.49      0.37        85\n",
            "           2       0.25      0.17      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.39      0.28      0.33        72\n",
            "           5       0.31      0.29      0.30        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.32      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[37 19 13 10  4 13]\n",
            " [16 42  8  9  3  7]\n",
            " [11 16 14 18  6 19]\n",
            " [10 23  9 22 10  5]\n",
            " [ 5 29  4  6 20  8]\n",
            " [18 15  8  7  8 23]]\n",
            "Accuracy: 0.3191919191919192\n",
            "Total f1 over classes: 1.8655271954040469\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.29      0.31        96\n",
            "           1       0.41      0.47      0.44        85\n",
            "           2       0.35      0.23      0.28        84\n",
            "           3       0.24      0.16      0.20        79\n",
            "           4       0.44      0.54      0.49        72\n",
            "           5       0.29      0.44      0.35        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.35      0.36      0.34       495\n",
            "weighted avg       0.35      0.35      0.34       495\n",
            "\n",
            "[[28 20  8  6  9 25]\n",
            " [19 40  2  8  6 10]\n",
            " [ 6  8 19 14 10 27]\n",
            " [12 10 15 13 17 12]\n",
            " [ 3 10  2  8 39 10]\n",
            " [15  9  8  5  7 35]]\n",
            "Accuracy: 0.3515151515151515\n",
            "Total f1 over classes: 2.0642959957520572\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.42      0.39        96\n",
            "           1       0.37      0.51      0.43        85\n",
            "           2       0.35      0.14      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.38      0.47      0.42        72\n",
            "           5       0.38      0.35      0.37        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.36      0.36      0.35       495\n",
            "weighted avg       0.36      0.36      0.35       495\n",
            "\n",
            "[[40 25  5  8  5 13]\n",
            " [19 43  3  5  4 11]\n",
            " [13 13 12 19 14 13]\n",
            " [12 10  9 22 20  6]\n",
            " [13 15  1  6 34  3]\n",
            " [14 10  4 11 12 28]]\n",
            "Accuracy: 0.3616161616161616\n",
            "Total f1 over classes: 2.0994306106535414\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.41      0.41        96\n",
            "           1       0.53      0.59      0.56        85\n",
            "           2       0.28      0.18      0.22        84\n",
            "           3       0.32      0.30      0.31        79\n",
            "           4       0.47      0.57      0.52        72\n",
            "           5       0.36      0.42      0.39        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.40      0.41      0.40       495\n",
            "weighted avg       0.40      0.41      0.40       495\n",
            "\n",
            "[[39 19 14 11  1 12]\n",
            " [15 50  5  3  4  8]\n",
            " [ 9  5 15 20 15 20]\n",
            " [11  9  9 24 15 11]\n",
            " [ 7  7  2  8 41  7]\n",
            " [14  5  8  8 11 33]]\n",
            "Accuracy: 0.4080808080808081\n",
            "Total f1 over classes: 2.400594675850099\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.19      0.20        96\n",
            "           1       0.43      0.42      0.43        85\n",
            "           2       0.44      0.14      0.22        84\n",
            "           3       0.27      0.08      0.12        79\n",
            "           4       0.32      0.65      0.43        72\n",
            "           5       0.27      0.44      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.32      0.29       495\n",
            "weighted avg       0.32      0.31      0.28       495\n",
            "\n",
            "[[18 12  3  2 23 38]\n",
            " [19 36  2  1 18  9]\n",
            " [12  8 12  4 22 26]\n",
            " [12 12  8  6 25 16]\n",
            " [ 6  7  1  5 47  6]\n",
            " [17  8  1  4 14 35]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.7238671221566724\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.33      0.38        97\n",
            "           1       0.47      0.71      0.56        85\n",
            "           2       0.30      0.33      0.31        83\n",
            "           3       0.36      0.25      0.30        79\n",
            "           4       0.48      0.43      0.45        72\n",
            "           5       0.39      0.41      0.40        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.41      0.41      0.40       495\n",
            "weighted avg       0.41      0.41      0.40       495\n",
            "\n",
            "[[32 22 18  5  3 17]\n",
            " [ 9 60  2  3  4  7]\n",
            " [10 10 27 16  8 12]\n",
            " [ 9 15 20 20  8  7]\n",
            " [ 5 14  7  8 31  7]\n",
            " [ 7  8 17  4 11 32]]\n",
            "Accuracy: 0.4080808080808081\n",
            "Total f1 over classes: 2.3961572847618338\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.28      0.51      0.36        85\n",
            "           2       0.21      0.16      0.18        83\n",
            "           3       0.30      0.24      0.27        79\n",
            "           4       0.42      0.31      0.35        72\n",
            "           5       0.38      0.29      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.31      0.30       495\n",
            "weighted avg       0.32      0.31      0.31       495\n",
            "\n",
            "[[34 23 17  8  6  9]\n",
            " [17 43  5  3  4 13]\n",
            " [15 27 13 17  8  3]\n",
            " [13 26  7 19  8  6]\n",
            " [ 6 22  8  7 22  7]\n",
            " [16 15 11  9  5 23]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.82901343439319\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.31      0.33        97\n",
            "           1       0.44      0.61      0.51        85\n",
            "           2       0.31      0.19      0.24        83\n",
            "           3       0.30      0.20      0.24        79\n",
            "           4       0.46      0.51      0.48        72\n",
            "           5       0.38      0.52      0.44        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.37      0.39      0.37       495\n",
            "weighted avg       0.37      0.39      0.37       495\n",
            "\n",
            "[[30 26  9  5  8 19]\n",
            " [12 52  2  4  4 11]\n",
            " [16 11 16 17 11 12]\n",
            " [12 15 13 16 14  9]\n",
            " [ 5  7  2  6 37 15]\n",
            " [ 9  8  9  5  7 41]]\n",
            "Accuracy: 0.3878787878787879\n",
            "Total f1 over classes: 2.2470461926220233\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.45      0.36        97\n",
            "           1       0.39      0.46      0.42        85\n",
            "           2       0.00      0.00      0.00        83\n",
            "           3       0.27      0.19      0.22        79\n",
            "           4       0.43      0.40      0.42        72\n",
            "           5       0.31      0.49      0.38        79\n",
            "\n",
            "    accuracy                           0.34       495\n",
            "   macro avg       0.28      0.33      0.30       495\n",
            "weighted avg       0.28      0.34      0.30       495\n",
            "\n",
            "[[44 20  0  7  2 24]\n",
            " [21 39  0  4  7 14]\n",
            " [22 14  0 20 11 16]\n",
            " [19 16  1 15 10 18]\n",
            " [17  5  0  6 29 15]\n",
            " [23  5  0  4  8 39]]\n",
            "Accuracy: 0.33535353535353535\n",
            "Total f1 over classes: 1.8060291753243651\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.42      0.39        97\n",
            "           1       0.47      0.59      0.52        85\n",
            "           2       0.24      0.14      0.18        83\n",
            "           3       0.32      0.22      0.26        79\n",
            "           4       0.46      0.53      0.49        72\n",
            "           5       0.43      0.47      0.45        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.38       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[41 14  9  7  8 18]\n",
            " [16 50  3  3  7  6]\n",
            " [22 16 12 14  9 10]\n",
            " [18 15 11 17 10  8]\n",
            " [ 4  8  7  7 38  8]\n",
            " [14  4  9  5 10 37]]\n",
            "Accuracy: 0.3939393939393939\n",
            "Total f1 over classes: 2.283595647387834\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.03      0.06        97\n",
            "           1       0.43      0.31      0.36        85\n",
            "           2       0.47      0.08      0.14        83\n",
            "           3       0.45      0.06      0.11        79\n",
            "           4       0.18      0.93      0.31        72\n",
            "           5       0.37      0.14      0.20        79\n",
            "\n",
            "    accuracy                           0.24       495\n",
            "   macro avg       0.36      0.26      0.20       495\n",
            "weighted avg       0.36      0.24      0.19       495\n",
            "\n",
            "[[ 3 12  2  2 74  4]\n",
            " [ 0 26  2  0 55  2]\n",
            " [ 2  9  7  4 56  5]\n",
            " [ 4  7  2  5 55  6]\n",
            " [ 0  3  0  0 67  2]\n",
            " [ 3  4  2  0 59 11]]\n",
            "Accuracy: 0.2404040404040404\n",
            "Total f1 over classes: 1.1729494445342126\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.23      0.29        97\n",
            "           1       0.40      0.78      0.53        86\n",
            "           2       0.28      0.40      0.33        83\n",
            "           3       0.50      0.09      0.15        78\n",
            "           4       0.55      0.56      0.55        72\n",
            "           5       0.41      0.35      0.38        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.42      0.40      0.37       495\n",
            "weighted avg       0.42      0.40      0.37       495\n",
            "\n",
            "[[22 36 23  2  5  9]\n",
            " [ 6 67  2  0  5  6]\n",
            " [ 8 17 33  4  6 15]\n",
            " [ 4 11 39  7  9  8]\n",
            " [ 6 17  5  1 40  3]\n",
            " [ 7 18 18  0  8 28]]\n",
            "Accuracy: 0.397979797979798\n",
            "Total f1 over classes: 2.2324789471416158\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.31      0.33        97\n",
            "           1       0.34      0.63      0.44        86\n",
            "           2       0.20      0.14      0.17        83\n",
            "           3       0.19      0.19      0.19        78\n",
            "           4       0.44      0.33      0.38        72\n",
            "           5       0.39      0.28      0.32        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.31      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[30 22  8 23  5  9]\n",
            " [ 9 54  5  9  6  3]\n",
            " [16 25 12 14  7  9]\n",
            " [11 22 15 15  5 10]\n",
            " [12 20  4  8 24  4]\n",
            " [ 7 16 16 10  8 22]]\n",
            "Accuracy: 0.31717171717171716\n",
            "Total f1 over classes: 1.830883794251098\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.37      0.40        97\n",
            "           1       0.52      0.59      0.55        86\n",
            "           2       0.30      0.23      0.26        83\n",
            "           3       0.37      0.26      0.30        78\n",
            "           4       0.47      0.65      0.54        72\n",
            "           5       0.38      0.46      0.41        79\n",
            "\n",
            "    accuracy                           0.42       495\n",
            "   macro avg       0.41      0.43      0.41       495\n",
            "weighted avg       0.41      0.42      0.41       495\n",
            "\n",
            "[[36 16 11 11  9 14]\n",
            " [11 51  3  2 12  7]\n",
            " [11 11 19 15  9 18]\n",
            " [ 5  5 19 20 14 15]\n",
            " [ 6  7  4  2 47  6]\n",
            " [13  8  8  4 10 36]]\n",
            "Accuracy: 0.4222222222222222\n",
            "Total f1 over classes: 2.472897339933953\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37        97\n",
            "           1       0.45      0.56      0.50        86\n",
            "           2       0.27      0.11      0.16        83\n",
            "           3       0.34      0.23      0.27        78\n",
            "           4       0.36      0.44      0.40        72\n",
            "           5       0.42      0.38      0.40        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.36      0.36      0.35       495\n",
            "weighted avg       0.36      0.37      0.35       495\n",
            "\n",
            "[[44 21  2  8  7 15]\n",
            " [15 48  3  1 15  4]\n",
            " [24 10  9 13 13 14]\n",
            " [19 11 13 18 10  7]\n",
            " [23  6  2  8 32  1]\n",
            " [17 10  4  5 13 30]]\n",
            "Accuracy: 0.3656565656565657\n",
            "Total f1 over classes: 2.0932441393135925\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.29      0.30        97\n",
            "           1       0.56      0.63      0.59        86\n",
            "           2       0.22      0.20      0.21        83\n",
            "           3       0.36      0.32      0.34        78\n",
            "           4       0.53      0.60      0.56        72\n",
            "           5       0.44      0.43      0.44        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.40      0.41      0.41       495\n",
            "weighted avg       0.40      0.41      0.40       495\n",
            "\n",
            "[[28 20 17 13  7 12]\n",
            " [13 54  2  4  7  6]\n",
            " [17  9 17 17  9 14]\n",
            " [ 9  6 23 25  7  8]\n",
            " [12  7  3  4 43  3]\n",
            " [13  1 16  7  8 34]]\n",
            "Accuracy: 0.40606060606060607\n",
            "Total f1 over classes: 2.4334671319493757\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.43      0.37        97\n",
            "           1       0.49      0.47      0.48        86\n",
            "           2       0.26      0.10      0.14        83\n",
            "           3       0.26      0.12      0.16        78\n",
            "           4       0.35      0.72      0.47        72\n",
            "           5       0.41      0.35      0.38        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.35      0.36      0.33       495\n",
            "weighted avg       0.35      0.36      0.33       495\n",
            "\n",
            "[[42 12  3  8 21 11]\n",
            " [19 40  1  3 21  2]\n",
            " [21  9  8 12 17 16]\n",
            " [18 10 14  9 19  8]\n",
            " [ 7  6  1  2 52  4]\n",
            " [22  5  4  0 20 28]]\n",
            "Accuracy: 0.3616161616161616\n",
            "Total f1 over classes: 1.9957839018737946\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.27      0.35        97\n",
            "           1       0.41      0.66      0.51        85\n",
            "           2       0.39      0.28      0.32        83\n",
            "           3       0.36      0.43      0.39        79\n",
            "           4       0.48      0.58      0.53        72\n",
            "           5       0.45      0.38      0.42        78\n",
            "\n",
            "    accuracy                           0.43       494\n",
            "   macro avg       0.43      0.43      0.42       494\n",
            "weighted avg       0.44      0.43      0.42       494\n",
            "\n",
            "[[26 27 13 16  7  8]\n",
            " [ 4 56  3  9  9  4]\n",
            " [ 8 13 23 16 10 13]\n",
            " [ 8 12 12 34  6  7]\n",
            " [ 3 10  1 12 42  4]\n",
            " [ 2 18  7  8 13 30]]\n",
            "Accuracy: 0.4271255060728745\n",
            "Total f1 over classes: 2.517855494800193\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.26      0.28        97\n",
            "           1       0.32      0.55      0.40        85\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.23      0.24      0.23        79\n",
            "           4       0.48      0.32      0.38        72\n",
            "           5       0.34      0.26      0.29        78\n",
            "\n",
            "    accuracy                           0.31       494\n",
            "   macro avg       0.32      0.31      0.31       494\n",
            "weighted avg       0.32      0.31      0.30       494\n",
            "\n",
            "[[25 25 15 14  4 14]\n",
            " [ 8 47  5 14  6  5]\n",
            " [20  9 19 19  3 13]\n",
            " [16 21 12 19  6  5]\n",
            " [ 5 26  9  7 23  2]\n",
            " [ 7 20 15 10  6 20]]\n",
            "Accuracy: 0.3097165991902834\n",
            "Total f1 over classes: 1.8347107194008774\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.30      0.32        97\n",
            "           1       0.44      0.58      0.50        85\n",
            "           2       0.30      0.17      0.22        83\n",
            "           3       0.31      0.32      0.31        79\n",
            "           4       0.49      0.50      0.50        72\n",
            "           5       0.38      0.49      0.43        78\n",
            "\n",
            "    accuracy                           0.39       494\n",
            "   macro avg       0.38      0.39      0.38       494\n",
            "weighted avg       0.38      0.39      0.38       494\n",
            "\n",
            "[[29 23 12 12  5 16]\n",
            " [14 49  2  8  7  5]\n",
            " [14  8 14 19  7 21]\n",
            " [12 10 13 25  7 12]\n",
            " [ 6  8  1 14 36  7]\n",
            " [ 8 13  5  3 11 38]]\n",
            "Accuracy: 0.3866396761133603\n",
            "Total f1 over classes: 2.276037092818215\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.26      0.28        97\n",
            "           1       0.36      0.51      0.42        85\n",
            "           2       0.25      0.22      0.23        83\n",
            "           3       0.25      0.04      0.07        79\n",
            "           4       0.46      0.46      0.46        72\n",
            "           5       0.31      0.54      0.39        78\n",
            "\n",
            "    accuracy                           0.33       494\n",
            "   macro avg       0.32      0.34      0.31       494\n",
            "weighted avg       0.32      0.33      0.31       494\n",
            "\n",
            "[[25 23 11  2  3 33]\n",
            " [13 43  7  3  7 12]\n",
            " [17  7 18  3 11 27]\n",
            " [14 18 25  3  6 13]\n",
            " [ 7 15  7  1 33  9]\n",
            " [ 8 12  4  0 12 42]]\n",
            "Accuracy: 0.3319838056680162\n",
            "Total f1 over classes: 1.8489372423892056\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.38      0.35        97\n",
            "           1       0.47      0.49      0.48        85\n",
            "           2       0.23      0.18      0.20        83\n",
            "           3       0.31      0.23      0.26        79\n",
            "           4       0.55      0.58      0.57        72\n",
            "           5       0.43      0.51      0.47        78\n",
            "\n",
            "    accuracy                           0.39       494\n",
            "   macro avg       0.39      0.40      0.39       494\n",
            "weighted avg       0.38      0.39      0.38       494\n",
            "\n",
            "[[37 17 17  9  5 12]\n",
            " [13 42  6 10  4 10]\n",
            " [22  9 15 14  7 16]\n",
            " [17 10 17 18  8  9]\n",
            " [11  6  3  4 42  6]\n",
            " [12  6  6  4 10 40]]\n",
            "Accuracy: 0.39271255060728744\n",
            "Total f1 over classes: 2.334422008393895\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.31      0.34        97\n",
            "           1       0.40      0.44      0.42        85\n",
            "           2       0.29      0.12      0.17        83\n",
            "           3       0.34      0.15      0.21        79\n",
            "           4       0.28      0.61      0.39        72\n",
            "           5       0.29      0.36      0.32        78\n",
            "\n",
            "    accuracy                           0.33       494\n",
            "   macro avg       0.33      0.33      0.31       494\n",
            "weighted avg       0.33      0.33      0.31       494\n",
            "\n",
            "[[30 17  6  8 20 16]\n",
            " [11 37  4  3 22  8]\n",
            " [10 11 10  7 24 21]\n",
            " [13 10  8 12 21 15]\n",
            " [ 8  6  3  4 44  7]\n",
            " [ 9 12  3  1 25 28]]\n",
            "Accuracy: 0.3259109311740891\n",
            "Total f1 over classes: 1.843939809739709\n",
            "Doing test analysis with ANN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.39      0.44        33\n",
            "           1       0.54      0.64      0.59        39\n",
            "           2       0.38      0.30      0.33        44\n",
            "           3       0.30      0.42      0.35        33\n",
            "           4       0.51      0.57      0.54        37\n",
            "           5       0.37      0.29      0.33        34\n",
            "\n",
            "    accuracy                           0.44       220\n",
            "   macro avg       0.44      0.44      0.43       220\n",
            "weighted avg       0.44      0.44      0.43       220\n",
            "\n",
            "[[13  7  3  4  3  3]\n",
            " [ 2 25  0  7  4  1]\n",
            " [ 3  5 13 13  6  4]\n",
            " [ 4  1  8 14  2  4]\n",
            " [ 0  3  3  5 21  5]\n",
            " [ 4  5  7  3  5 10]]\n",
            "Accuracy: 0.43636363636363634\n",
            "Total f1 over classes: 2.5830073642200655\n",
            "Doing test analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.39      0.33        33\n",
            "           1       0.38      0.49      0.43        39\n",
            "           2       0.17      0.16      0.17        44\n",
            "           3       0.25      0.27      0.26        33\n",
            "           4       0.50      0.32      0.39        37\n",
            "           5       0.42      0.29      0.34        34\n",
            "\n",
            "    accuracy                           0.32       220\n",
            "   macro avg       0.33      0.32      0.32       220\n",
            "weighted avg       0.33      0.32      0.32       220\n",
            "\n",
            "[[13  4  6  3  1  6]\n",
            " [ 8 19  3  5  2  2]\n",
            " [11  8  7 11  4  3]\n",
            " [ 7  4  7  9  3  3]\n",
            " [ 3 11  8  3 12  0]\n",
            " [ 4  4  9  5  2 10]]\n",
            "Accuracy: 0.3181818181818182\n",
            "Total f1 over classes: 1.9218866572272386\n",
            "Doing test analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.39      0.39        33\n",
            "           1       0.47      0.49      0.48        39\n",
            "           2       0.32      0.18      0.23        44\n",
            "           3       0.22      0.24      0.23        33\n",
            "           4       0.45      0.46      0.45        37\n",
            "           5       0.33      0.47      0.39        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.37      0.37      0.36       220\n",
            "weighted avg       0.37      0.37      0.36       220\n",
            "\n",
            "[[13  8  2  1  3  6]\n",
            " [ 6 19  1  5  5  3]\n",
            " [ 5  5  8 15  3  8]\n",
            " [ 4  1  7  8  5  8]\n",
            " [ 2  3  3  5 17  7]\n",
            " [ 3  4  4  2  5 16]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.1822974038816287\n",
            "Doing test analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.39      0.37        33\n",
            "           1       0.41      0.46      0.43        39\n",
            "           2       0.43      0.30      0.35        44\n",
            "           3       0.30      0.09      0.14        33\n",
            "           4       0.42      0.46      0.44        37\n",
            "           5       0.27      0.47      0.34        34\n",
            "\n",
            "    accuracy                           0.36       220\n",
            "   macro avg       0.36      0.36      0.35       220\n",
            "weighted avg       0.37      0.36      0.35       220\n",
            "\n",
            "[[13  6  2  0  2 10]\n",
            " [ 3 18  4  4  5  5]\n",
            " [ 7  5 13  3  5 11]\n",
            " [ 3  7  5  3  6  9]\n",
            " [ 4  3  5  0 17  8]\n",
            " [ 7  5  1  0  5 16]]\n",
            "Accuracy: 0.36363636363636365\n",
            "Total f1 over classes: 2.081694209323707\n",
            "Doing test analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.48      0.39        33\n",
            "           1       0.52      0.41      0.46        39\n",
            "           2       0.30      0.25      0.27        44\n",
            "           3       0.27      0.24      0.25        33\n",
            "           4       0.57      0.43      0.49        37\n",
            "           5       0.39      0.50      0.44        34\n",
            "\n",
            "    accuracy                           0.38       220\n",
            "   macro avg       0.39      0.39      0.38       220\n",
            "weighted avg       0.39      0.38      0.38       220\n",
            "\n",
            "[[16  6  3  0  1  7]\n",
            " [10 16  3  6  2  2]\n",
            " [10  4 11 10  2  7]\n",
            " [ 5  3  7  8  4  6]\n",
            " [ 3  2  7  4 16  5]\n",
            " [ 6  0  6  2  3 17]]\n",
            "Accuracy: 0.38181818181818183\n",
            "Total f1 over classes: 2.2964633462625432\n",
            "Doing test analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.36      0.38        33\n",
            "           1       0.53      0.44      0.48        39\n",
            "           2       0.35      0.14      0.20        44\n",
            "           3       0.31      0.12      0.17        33\n",
            "           4       0.32      0.70      0.44        37\n",
            "           5       0.28      0.38      0.32        34\n",
            "\n",
            "    accuracy                           0.35       220\n",
            "   macro avg       0.36      0.36      0.33       220\n",
            "weighted avg       0.37      0.35      0.33       220\n",
            "\n",
            "[[12  4  0  0  9  8]\n",
            " [ 5 17  2  2 11  2]\n",
            " [ 5  6  6  5 12 10]\n",
            " [ 2  1  7  4  9 10]\n",
            " [ 2  2  1  2 26  4]\n",
            " [ 4  2  1  0 14 13]]\n",
            "Accuracy: 0.35454545454545455\n",
            "Total f1 over classes: 1.9921255957653536\n",
            "No. of training examples: 1979\n",
            "No. of testing examples: 220\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.33      0.34        96\n",
            "           1       0.40      0.48      0.44        85\n",
            "           2       0.32      0.21      0.26        84\n",
            "           3       0.30      0.28      0.29        79\n",
            "           4       0.49      0.49      0.49        72\n",
            "           5       0.31      0.41      0.35        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.36      0.37      0.36       495\n",
            "weighted avg       0.36      0.36      0.36       495\n",
            "\n",
            "[[32 18 12  7  4 23]\n",
            " [19 41  1 10  4 10]\n",
            " [ 8 10 18 17 10 21]\n",
            " [12 11 13 22 13  8]\n",
            " [ 4 12  5  7 35  9]\n",
            " [15 10  7 10  5 32]]\n",
            "Accuracy: 0.36363636363636365\n",
            "Total f1 over classes: 2.1703640778143924\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.38        96\n",
            "           1       0.29      0.49      0.37        85\n",
            "           2       0.25      0.17      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.39      0.28      0.33        72\n",
            "           5       0.31      0.29      0.30        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.32      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[37 19 13 10  4 13]\n",
            " [16 42  8  9  3  7]\n",
            " [11 16 14 18  6 19]\n",
            " [10 23  9 22 10  5]\n",
            " [ 5 29  4  6 20  8]\n",
            " [18 15  8  7  8 23]]\n",
            "Accuracy: 0.3191919191919192\n",
            "Total f1 over classes: 1.8655271954040469\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.29      0.31        96\n",
            "           1       0.41      0.47      0.44        85\n",
            "           2       0.35      0.23      0.28        84\n",
            "           3       0.24      0.16      0.20        79\n",
            "           4       0.44      0.54      0.49        72\n",
            "           5       0.29      0.44      0.35        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.35      0.36      0.34       495\n",
            "weighted avg       0.35      0.35      0.34       495\n",
            "\n",
            "[[28 20  8  6  9 25]\n",
            " [19 40  2  8  6 10]\n",
            " [ 6  8 19 14 10 27]\n",
            " [12 10 15 13 17 12]\n",
            " [ 3 10  2  8 39 10]\n",
            " [15  9  8  5  7 35]]\n",
            "Accuracy: 0.3515151515151515\n",
            "Total f1 over classes: 2.0642959957520572\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.42      0.39        96\n",
            "           1       0.37      0.51      0.43        85\n",
            "           2       0.35      0.14      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.38      0.47      0.42        72\n",
            "           5       0.38      0.35      0.37        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.36      0.36      0.35       495\n",
            "weighted avg       0.36      0.36      0.35       495\n",
            "\n",
            "[[40 25  5  8  5 13]\n",
            " [19 43  3  5  4 11]\n",
            " [13 13 12 19 14 13]\n",
            " [12 10  9 22 20  6]\n",
            " [13 15  1  6 34  3]\n",
            " [14 10  4 11 12 28]]\n",
            "Accuracy: 0.3616161616161616\n",
            "Total f1 over classes: 2.0994306106535414\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.42      0.42        96\n",
            "           1       0.49      0.55      0.52        85\n",
            "           2       0.30      0.21      0.25        84\n",
            "           3       0.33      0.30      0.32        79\n",
            "           4       0.49      0.54      0.52        72\n",
            "           5       0.37      0.43      0.40        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.40      0.41      0.40       495\n",
            "weighted avg       0.40      0.41      0.40       495\n",
            "\n",
            "[[40 21 14  6  0 15]\n",
            " [15 47  9  4  3  7]\n",
            " [ 8  6 18 21 14 17]\n",
            " [12  8 10 24 15 10]\n",
            " [ 7  7  2  8 39  9]\n",
            " [12  7  8 10  8 34]]\n",
            "Accuracy: 0.4080808080808081\n",
            "Total f1 over classes: 2.418672094010888\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.19      0.20        96\n",
            "           1       0.43      0.42      0.43        85\n",
            "           2       0.44      0.14      0.22        84\n",
            "           3       0.27      0.08      0.12        79\n",
            "           4       0.32      0.65      0.43        72\n",
            "           5       0.27      0.44      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.32      0.29       495\n",
            "weighted avg       0.32      0.31      0.28       495\n",
            "\n",
            "[[18 12  3  2 23 38]\n",
            " [19 36  2  1 18  9]\n",
            " [12  8 12  4 22 26]\n",
            " [12 12  8  6 25 16]\n",
            " [ 6  7  1  5 47  6]\n",
            " [17  8  1  4 14 35]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.7238671221566724\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.15      0.21        97\n",
            "           1       0.40      0.67      0.50        85\n",
            "           2       0.28      0.28      0.28        83\n",
            "           3       0.41      0.18      0.25        79\n",
            "           4       0.49      0.49      0.49        72\n",
            "           5       0.32      0.49      0.39        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.37      0.38      0.35       495\n",
            "weighted avg       0.37      0.37      0.35       495\n",
            "\n",
            "[[15 27 19  0  4 32]\n",
            " [ 7 57  1  3  7 10]\n",
            " [ 7 17 23 11  7 18]\n",
            " [ 6 18 23 14  8 10]\n",
            " [ 4  8  8  3 35 14]\n",
            " [ 6 14  7  3 10 39]]\n",
            "Accuracy: 0.3696969696969697\n",
            "Total f1 over classes: 2.1196169032642582\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.28      0.51      0.36        85\n",
            "           2       0.21      0.16      0.18        83\n",
            "           3       0.30      0.24      0.27        79\n",
            "           4       0.42      0.31      0.35        72\n",
            "           5       0.38      0.29      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.31      0.30       495\n",
            "weighted avg       0.32      0.31      0.31       495\n",
            "\n",
            "[[34 23 17  8  6  9]\n",
            " [17 43  5  3  4 13]\n",
            " [15 27 13 17  8  3]\n",
            " [13 26  7 19  8  6]\n",
            " [ 6 22  8  7 22  7]\n",
            " [16 15 11  9  5 23]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.82901343439319\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.31      0.33        97\n",
            "           1       0.44      0.61      0.51        85\n",
            "           2       0.31      0.19      0.24        83\n",
            "           3       0.30      0.20      0.24        79\n",
            "           4       0.46      0.51      0.48        72\n",
            "           5       0.38      0.52      0.44        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.37      0.39      0.37       495\n",
            "weighted avg       0.37      0.39      0.37       495\n",
            "\n",
            "[[30 26  9  5  8 19]\n",
            " [12 52  2  4  4 11]\n",
            " [16 11 16 17 11 12]\n",
            " [12 15 13 16 14  9]\n",
            " [ 5  7  2  6 37 15]\n",
            " [ 9  8  9  5  7 41]]\n",
            "Accuracy: 0.3878787878787879\n",
            "Total f1 over classes: 2.2470461926220233\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.45      0.36        97\n",
            "           1       0.39      0.46      0.42        85\n",
            "           2       0.00      0.00      0.00        83\n",
            "           3       0.27      0.19      0.22        79\n",
            "           4       0.43      0.40      0.42        72\n",
            "           5       0.31      0.49      0.38        79\n",
            "\n",
            "    accuracy                           0.34       495\n",
            "   macro avg       0.28      0.33      0.30       495\n",
            "weighted avg       0.28      0.34      0.30       495\n",
            "\n",
            "[[44 20  0  7  2 24]\n",
            " [21 39  0  4  7 14]\n",
            " [22 14  0 20 11 16]\n",
            " [19 16  1 15 10 18]\n",
            " [17  5  0  6 29 15]\n",
            " [23  5  0  4  8 39]]\n",
            "Accuracy: 0.33535353535353535\n",
            "Total f1 over classes: 1.8060291753243651\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.41      0.37        97\n",
            "           1       0.45      0.54      0.49        85\n",
            "           2       0.22      0.16      0.18        83\n",
            "           3       0.36      0.19      0.25        79\n",
            "           4       0.44      0.50      0.47        72\n",
            "           5       0.39      0.44      0.42        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.37      0.37      0.36       495\n",
            "weighted avg       0.36      0.37      0.36       495\n",
            "\n",
            "[[40 15 11  4  9 18]\n",
            " [17 46  5  3  6  8]\n",
            " [23 14 13 14 10  9]\n",
            " [19 14 12 15 10  9]\n",
            " [ 8  9  5  4 36 10]\n",
            " [14  5 13  2 10 35]]\n",
            "Accuracy: 0.37373737373737376\n",
            "Total f1 over classes: 2.1746215569994805\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.03      0.06        97\n",
            "           1       0.43      0.31      0.36        85\n",
            "           2       0.47      0.08      0.14        83\n",
            "           3       0.45      0.06      0.11        79\n",
            "           4       0.18      0.93      0.31        72\n",
            "           5       0.37      0.14      0.20        79\n",
            "\n",
            "    accuracy                           0.24       495\n",
            "   macro avg       0.36      0.26      0.20       495\n",
            "weighted avg       0.36      0.24      0.19       495\n",
            "\n",
            "[[ 3 12  2  2 74  4]\n",
            " [ 0 26  2  0 55  2]\n",
            " [ 2  9  7  4 56  5]\n",
            " [ 4  7  2  5 55  6]\n",
            " [ 0  3  0  0 67  2]\n",
            " [ 3  4  2  0 59 11]]\n",
            "Accuracy: 0.2404040404040404\n",
            "Total f1 over classes: 1.1729494445342126\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.52      0.44        97\n",
            "           1       0.66      0.50      0.57        86\n",
            "           2       0.42      0.12      0.19        83\n",
            "           3       0.29      0.50      0.37        78\n",
            "           4       0.57      0.49      0.53        72\n",
            "           5       0.41      0.41      0.41        79\n",
            "\n",
            "    accuracy                           0.42       495\n",
            "   macro avg       0.45      0.42      0.42       495\n",
            "weighted avg       0.45      0.42      0.42       495\n",
            "\n",
            "[[50  7  5 23  3  9]\n",
            " [22 43  0 10  4  7]\n",
            " [21  4 10 29  4 15]\n",
            " [13  3  5 39  6 12]\n",
            " [ 8  6  2 17 35  4]\n",
            " [18  2  2 16  9 32]]\n",
            "Accuracy: 0.4222222222222222\n",
            "Total f1 over classes: 2.492437143313762\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.31      0.33        97\n",
            "           1       0.34      0.63      0.44        86\n",
            "           2       0.20      0.14      0.17        83\n",
            "           3       0.19      0.19      0.19        78\n",
            "           4       0.44      0.33      0.38        72\n",
            "           5       0.39      0.28      0.32        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.31      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[30 22  8 23  5  9]\n",
            " [ 9 54  5  9  6  3]\n",
            " [16 25 12 14  7  9]\n",
            " [11 22 15 15  5 10]\n",
            " [12 20  4  8 24  4]\n",
            " [ 7 16 16 10  8 22]]\n",
            "Accuracy: 0.31717171717171716\n",
            "Total f1 over classes: 1.830883794251098\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.37      0.40        97\n",
            "           1       0.52      0.59      0.55        86\n",
            "           2       0.30      0.23      0.26        83\n",
            "           3       0.37      0.26      0.30        78\n",
            "           4       0.47      0.65      0.54        72\n",
            "           5       0.38      0.46      0.41        79\n",
            "\n",
            "    accuracy                           0.42       495\n",
            "   macro avg       0.41      0.43      0.41       495\n",
            "weighted avg       0.41      0.42      0.41       495\n",
            "\n",
            "[[36 16 11 11  9 14]\n",
            " [11 51  3  2 12  7]\n",
            " [11 11 19 15  9 18]\n",
            " [ 5  5 19 20 14 15]\n",
            " [ 6  7  4  2 47  6]\n",
            " [13  8  8  4 10 36]]\n",
            "Accuracy: 0.4222222222222222\n",
            "Total f1 over classes: 2.472897339933953\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37        97\n",
            "           1       0.45      0.56      0.50        86\n",
            "           2       0.27      0.11      0.16        83\n",
            "           3       0.34      0.23      0.27        78\n",
            "           4       0.36      0.44      0.40        72\n",
            "           5       0.42      0.38      0.40        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.36      0.36      0.35       495\n",
            "weighted avg       0.36      0.37      0.35       495\n",
            "\n",
            "[[44 21  2  8  7 15]\n",
            " [15 48  3  1 15  4]\n",
            " [24 10  9 13 13 14]\n",
            " [19 11 13 18 10  7]\n",
            " [23  6  2  8 32  1]\n",
            " [17 10  4  5 13 30]]\n",
            "Accuracy: 0.3656565656565657\n",
            "Total f1 over classes: 2.0932441393135925\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.31      0.32        97\n",
            "           1       0.53      0.64      0.58        86\n",
            "           2       0.18      0.17      0.18        83\n",
            "           3       0.34      0.26      0.29        78\n",
            "           4       0.54      0.65      0.59        72\n",
            "           5       0.47      0.46      0.46        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.40      0.41      0.40       495\n",
            "weighted avg       0.39      0.41      0.40       495\n",
            "\n",
            "[[30 20 15 16  6 10]\n",
            " [13 55  4  2  7  5]\n",
            " [20 13 14 13 10 13]\n",
            " [ 8  6 26 20  9  9]\n",
            " [10  6  2  4 47  3]\n",
            " [12  4 15  4  8 36]]\n",
            "Accuracy: 0.4080808080808081\n",
            "Total f1 over classes: 2.4185193715415054\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.43      0.37        97\n",
            "           1       0.49      0.47      0.48        86\n",
            "           2       0.26      0.10      0.14        83\n",
            "           3       0.26      0.12      0.16        78\n",
            "           4       0.35      0.72      0.47        72\n",
            "           5       0.41      0.35      0.38        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.35      0.36      0.33       495\n",
            "weighted avg       0.35      0.36      0.33       495\n",
            "\n",
            "[[42 12  3  8 21 11]\n",
            " [19 40  1  3 21  2]\n",
            " [21  9  8 12 17 16]\n",
            " [18 10 14  9 19  8]\n",
            " [ 7  6  1  2 52  4]\n",
            " [22  5  4  0 20 28]]\n",
            "Accuracy: 0.3616161616161616\n",
            "Total f1 over classes: 1.9957839018737946\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.23      0.32        97\n",
            "           1       0.46      0.58      0.51        85\n",
            "           2       0.26      0.24      0.25        83\n",
            "           3       0.36      0.33      0.34        79\n",
            "           4       0.65      0.46      0.54        72\n",
            "           5       0.35      0.64      0.45        78\n",
            "\n",
            "    accuracy                           0.40       494\n",
            "   macro avg       0.43      0.41      0.40       494\n",
            "weighted avg       0.43      0.40      0.40       494\n",
            "\n",
            "[[22 20 22  5  4 24]\n",
            " [ 5 49  4  6  7 14]\n",
            " [ 4 10 20 21  3 25]\n",
            " [ 6  9 17 26  1 20]\n",
            " [ 2  8  9  9 33 11]\n",
            " [ 3 11  6  5  3 50]]\n",
            "Accuracy: 0.4048582995951417\n",
            "Total f1 over classes: 2.4068173114568\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.26      0.28        97\n",
            "           1       0.32      0.55      0.40        85\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.23      0.24      0.23        79\n",
            "           4       0.48      0.32      0.38        72\n",
            "           5       0.34      0.26      0.29        78\n",
            "\n",
            "    accuracy                           0.31       494\n",
            "   macro avg       0.32      0.31      0.31       494\n",
            "weighted avg       0.32      0.31      0.30       494\n",
            "\n",
            "[[25 25 15 14  4 14]\n",
            " [ 8 47  5 14  6  5]\n",
            " [20  9 19 19  3 13]\n",
            " [16 21 12 19  6  5]\n",
            " [ 5 26  9  7 23  2]\n",
            " [ 7 20 15 10  6 20]]\n",
            "Accuracy: 0.3097165991902834\n",
            "Total f1 over classes: 1.8347107194008774\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.30      0.32        97\n",
            "           1       0.44      0.58      0.50        85\n",
            "           2       0.30      0.17      0.22        83\n",
            "           3       0.31      0.32      0.31        79\n",
            "           4       0.49      0.50      0.50        72\n",
            "           5       0.38      0.49      0.43        78\n",
            "\n",
            "    accuracy                           0.39       494\n",
            "   macro avg       0.38      0.39      0.38       494\n",
            "weighted avg       0.38      0.39      0.38       494\n",
            "\n",
            "[[29 23 12 12  5 16]\n",
            " [14 49  2  8  7  5]\n",
            " [14  8 14 19  7 21]\n",
            " [12 10 13 25  7 12]\n",
            " [ 6  8  1 14 36  7]\n",
            " [ 8 13  5  3 11 38]]\n",
            "Accuracy: 0.3866396761133603\n",
            "Total f1 over classes: 2.276037092818215\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.26      0.28        97\n",
            "           1       0.36      0.51      0.42        85\n",
            "           2       0.25      0.22      0.23        83\n",
            "           3       0.25      0.04      0.07        79\n",
            "           4       0.46      0.46      0.46        72\n",
            "           5       0.31      0.54      0.39        78\n",
            "\n",
            "    accuracy                           0.33       494\n",
            "   macro avg       0.32      0.34      0.31       494\n",
            "weighted avg       0.32      0.33      0.31       494\n",
            "\n",
            "[[25 23 11  2  3 33]\n",
            " [13 43  7  3  7 12]\n",
            " [17  7 18  3 11 27]\n",
            " [14 18 25  3  6 13]\n",
            " [ 7 15  7  1 33  9]\n",
            " [ 8 12  4  0 12 42]]\n",
            "Accuracy: 0.3319838056680162\n",
            "Total f1 over classes: 1.8489372423892056\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.28      0.27        97\n",
            "           1       0.45      0.47      0.46        85\n",
            "           2       0.23      0.19      0.21        83\n",
            "           3       0.30      0.27      0.28        79\n",
            "           4       0.58      0.56      0.57        72\n",
            "           5       0.43      0.53      0.47        78\n",
            "\n",
            "    accuracy                           0.37       494\n",
            "   macro avg       0.38      0.38      0.38       494\n",
            "weighted avg       0.37      0.37      0.37       494\n",
            "\n",
            "[[27 18 18 14  2 18]\n",
            " [14 40  8 10  5  8]\n",
            " [21  8 16 16  7 15]\n",
            " [19  9 14 21  8  8]\n",
            " [ 8  7  6  5 40  6]\n",
            " [12  6  7  5  7 41]]\n",
            "Accuracy: 0.37449392712550605\n",
            "Total f1 over classes: 2.264321588522401\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.31      0.34        97\n",
            "           1       0.40      0.44      0.42        85\n",
            "           2       0.29      0.12      0.17        83\n",
            "           3       0.34      0.15      0.21        79\n",
            "           4       0.28      0.61      0.39        72\n",
            "           5       0.29      0.36      0.32        78\n",
            "\n",
            "    accuracy                           0.33       494\n",
            "   macro avg       0.33      0.33      0.31       494\n",
            "weighted avg       0.33      0.33      0.31       494\n",
            "\n",
            "[[30 17  6  8 20 16]\n",
            " [11 37  4  3 22  8]\n",
            " [10 11 10  7 24 21]\n",
            " [13 10  8 12 21 15]\n",
            " [ 8  6  3  4 44  7]\n",
            " [ 9 12  3  1 25 28]]\n",
            "Accuracy: 0.3259109311740891\n",
            "Total f1 over classes: 1.843939809739709\n",
            "Doing test analysis with ANN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.24      0.30        33\n",
            "           1       0.50      0.51      0.51        39\n",
            "           2       0.37      0.34      0.35        44\n",
            "           3       0.32      0.27      0.30        33\n",
            "           4       0.52      0.41      0.45        37\n",
            "           5       0.31      0.56      0.40        34\n",
            "\n",
            "    accuracy                           0.39       220\n",
            "   macro avg       0.40      0.39      0.38       220\n",
            "weighted avg       0.40      0.39      0.39       220\n",
            "\n",
            "[[ 8  8  7  0  2  8]\n",
            " [ 2 20  4  4  4  5]\n",
            " [ 4  6 15  7  2 10]\n",
            " [ 2  1  7  9  2 12]\n",
            " [ 1  3  4  6 15  8]\n",
            " [ 3  2  4  2  4 19]]\n",
            "Accuracy: 0.39090909090909093\n",
            "Total f1 over classes: 2.306617837939372\n",
            "Doing test analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.39      0.33        33\n",
            "           1       0.38      0.49      0.43        39\n",
            "           2       0.17      0.16      0.17        44\n",
            "           3       0.25      0.27      0.26        33\n",
            "           4       0.50      0.32      0.39        37\n",
            "           5       0.42      0.29      0.34        34\n",
            "\n",
            "    accuracy                           0.32       220\n",
            "   macro avg       0.33      0.32      0.32       220\n",
            "weighted avg       0.33      0.32      0.32       220\n",
            "\n",
            "[[13  4  6  3  1  6]\n",
            " [ 8 19  3  5  2  2]\n",
            " [11  8  7 11  4  3]\n",
            " [ 7  4  7  9  3  3]\n",
            " [ 3 11  8  3 12  0]\n",
            " [ 4  4  9  5  2 10]]\n",
            "Accuracy: 0.3181818181818182\n",
            "Total f1 over classes: 1.9218866572272386\n",
            "Doing test analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.39      0.39        33\n",
            "           1       0.47      0.49      0.48        39\n",
            "           2       0.32      0.18      0.23        44\n",
            "           3       0.22      0.24      0.23        33\n",
            "           4       0.45      0.46      0.45        37\n",
            "           5       0.33      0.47      0.39        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.37      0.37      0.36       220\n",
            "weighted avg       0.37      0.37      0.36       220\n",
            "\n",
            "[[13  8  2  1  3  6]\n",
            " [ 6 19  1  5  5  3]\n",
            " [ 5  5  8 15  3  8]\n",
            " [ 4  1  7  8  5  8]\n",
            " [ 2  3  3  5 17  7]\n",
            " [ 3  4  4  2  5 16]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.1822974038816287\n",
            "Doing test analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.39      0.37        33\n",
            "           1       0.41      0.46      0.43        39\n",
            "           2       0.43      0.30      0.35        44\n",
            "           3       0.30      0.09      0.14        33\n",
            "           4       0.42      0.46      0.44        37\n",
            "           5       0.27      0.47      0.34        34\n",
            "\n",
            "    accuracy                           0.36       220\n",
            "   macro avg       0.36      0.36      0.35       220\n",
            "weighted avg       0.37      0.36      0.35       220\n",
            "\n",
            "[[13  6  2  0  2 10]\n",
            " [ 3 18  4  4  5  5]\n",
            " [ 7  5 13  3  5 11]\n",
            " [ 3  7  5  3  6  9]\n",
            " [ 4  3  5  0 17  8]\n",
            " [ 7  5  1  0  5 16]]\n",
            "Accuracy: 0.36363636363636365\n",
            "Total f1 over classes: 2.081694209323707\n",
            "Doing test analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.36      0.31        33\n",
            "           1       0.55      0.44      0.49        39\n",
            "           2       0.25      0.20      0.23        44\n",
            "           3       0.32      0.33      0.33        33\n",
            "           4       0.52      0.43      0.47        37\n",
            "           5       0.36      0.47      0.41        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.38      0.37      0.37       220\n",
            "weighted avg       0.38      0.37      0.37       220\n",
            "\n",
            "[[12  8  3  2  2  6]\n",
            " [ 7 17  5  5  2  3]\n",
            " [11  5  9 10  3  6]\n",
            " [ 4  1  6 11  3  8]\n",
            " [ 4  0  9  3 16  5]\n",
            " [ 6  0  4  3  5 16]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.231605451908349\n",
            "Doing test analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.36      0.38        33\n",
            "           1       0.53      0.44      0.48        39\n",
            "           2       0.35      0.14      0.20        44\n",
            "           3       0.31      0.12      0.17        33\n",
            "           4       0.32      0.70      0.44        37\n",
            "           5       0.28      0.38      0.32        34\n",
            "\n",
            "    accuracy                           0.35       220\n",
            "   macro avg       0.36      0.36      0.33       220\n",
            "weighted avg       0.37      0.35      0.33       220\n",
            "\n",
            "[[12  4  0  0  9  8]\n",
            " [ 5 17  2  2 11  2]\n",
            " [ 5  6  6  5 12 10]\n",
            " [ 2  1  7  4  9 10]\n",
            " [ 2  2  1  2 26  4]\n",
            " [ 4  2  1  0 14 13]]\n",
            "Accuracy: 0.35454545454545455\n",
            "Total f1 over classes: 1.9921255957653536\n",
            "No. of training examples: 1979\n",
            "No. of testing examples: 220\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.23      0.30        96\n",
            "           1       0.53      0.42      0.47        85\n",
            "           2       0.21      0.05      0.08        84\n",
            "           3       0.19      0.39      0.26        79\n",
            "           4       0.52      0.32      0.40        72\n",
            "           5       0.31      0.59      0.41        79\n",
            "\n",
            "    accuracy                           0.33       495\n",
            "   macro avg       0.37      0.33      0.32       495\n",
            "weighted avg       0.37      0.33      0.32       495\n",
            "\n",
            "[[22  9  1 22  2 40]\n",
            " [ 9 36  1 19  2 18]\n",
            " [ 1  8  4 44  4 23]\n",
            " [ 6  8  7 31 11 16]\n",
            " [ 7  3  3 27 23  9]\n",
            " [ 5  4  3 18  2 47]]\n",
            "Accuracy: 0.3292929292929293\n",
            "Total f1 over classes: 1.9096854724848054\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.38        96\n",
            "           1       0.29      0.49      0.37        85\n",
            "           2       0.25      0.17      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.39      0.28      0.33        72\n",
            "           5       0.31      0.29      0.30        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.32      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[37 19 13 10  4 13]\n",
            " [16 42  8  9  3  7]\n",
            " [11 16 14 18  6 19]\n",
            " [10 23  9 22 10  5]\n",
            " [ 5 29  4  6 20  8]\n",
            " [18 15  8  7  8 23]]\n",
            "Accuracy: 0.3191919191919192\n",
            "Total f1 over classes: 1.8655271954040469\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.29      0.31        96\n",
            "           1       0.41      0.47      0.44        85\n",
            "           2       0.35      0.23      0.28        84\n",
            "           3       0.24      0.16      0.20        79\n",
            "           4       0.44      0.54      0.49        72\n",
            "           5       0.29      0.44      0.35        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.35      0.36      0.34       495\n",
            "weighted avg       0.35      0.35      0.34       495\n",
            "\n",
            "[[28 20  8  6  9 25]\n",
            " [19 40  2  8  6 10]\n",
            " [ 6  8 19 14 10 27]\n",
            " [12 10 15 13 17 12]\n",
            " [ 3 10  2  8 39 10]\n",
            " [15  9  8  5  7 35]]\n",
            "Accuracy: 0.3515151515151515\n",
            "Total f1 over classes: 2.0642959957520572\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.42      0.39        96\n",
            "           1       0.37      0.51      0.43        85\n",
            "           2       0.35      0.14      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.38      0.47      0.42        72\n",
            "           5       0.38      0.35      0.37        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.36      0.36      0.35       495\n",
            "weighted avg       0.36      0.36      0.35       495\n",
            "\n",
            "[[40 25  5  8  5 13]\n",
            " [19 43  3  5  4 11]\n",
            " [13 13 12 19 14 13]\n",
            " [12 10  9 22 20  6]\n",
            " [13 15  1  6 34  3]\n",
            " [14 10  4 11 12 28]]\n",
            "Accuracy: 0.3616161616161616\n",
            "Total f1 over classes: 2.0994306106535414\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.38      0.38        96\n",
            "           1       0.52      0.55      0.54        85\n",
            "           2       0.28      0.18      0.22        84\n",
            "           3       0.30      0.33      0.31        79\n",
            "           4       0.47      0.53      0.50        72\n",
            "           5       0.34      0.39      0.36        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.39       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[36 15 12 15  4 14]\n",
            " [14 47  8  4  3  9]\n",
            " [10  6 15 22 11 20]\n",
            " [12  9  9 26 15  8]\n",
            " [ 5  8  1 11 38  9]\n",
            " [16  5  8  9 10 31]]\n",
            "Accuracy: 0.3898989898989899\n",
            "Total f1 over classes: 2.3117642608299436\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.19      0.20        96\n",
            "           1       0.43      0.42      0.43        85\n",
            "           2       0.44      0.14      0.22        84\n",
            "           3       0.27      0.08      0.12        79\n",
            "           4       0.32      0.65      0.43        72\n",
            "           5       0.27      0.44      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.32      0.29       495\n",
            "weighted avg       0.32      0.31      0.28       495\n",
            "\n",
            "[[18 12  3  2 23 38]\n",
            " [19 36  2  1 18  9]\n",
            " [12  8 12  4 22 26]\n",
            " [12 12  8  6 25 16]\n",
            " [ 6  7  1  5 47  6]\n",
            " [17  8  1  4 14 35]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.7238671221566724\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.19      0.24        97\n",
            "           1       0.44      0.56      0.50        85\n",
            "           2       0.38      0.10      0.15        83\n",
            "           3       0.33      0.47      0.39        79\n",
            "           4       0.55      0.50      0.53        72\n",
            "           5       0.31      0.53      0.39        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.39      0.39      0.37       495\n",
            "weighted avg       0.39      0.38      0.36       495\n",
            "\n",
            "[[18 23  6 13  5 32]\n",
            " [12 48  0  5  4 16]\n",
            " [ 9 11  8 33  6 16]\n",
            " [ 6 11  2 37 10 13]\n",
            " [ 2  7  2 10 36 15]\n",
            " [ 8  8  3 14  4 42]]\n",
            "Accuracy: 0.38181818181818183\n",
            "Total f1 over classes: 2.195445782946577\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.28      0.51      0.36        85\n",
            "           2       0.21      0.16      0.18        83\n",
            "           3       0.30      0.24      0.27        79\n",
            "           4       0.42      0.31      0.35        72\n",
            "           5       0.38      0.29      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.31      0.30       495\n",
            "weighted avg       0.32      0.31      0.31       495\n",
            "\n",
            "[[34 23 17  8  6  9]\n",
            " [17 43  5  3  4 13]\n",
            " [15 27 13 17  8  3]\n",
            " [13 26  7 19  8  6]\n",
            " [ 6 22  8  7 22  7]\n",
            " [16 15 11  9  5 23]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.82901343439319\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.31      0.33        97\n",
            "           1       0.44      0.61      0.51        85\n",
            "           2       0.31      0.19      0.24        83\n",
            "           3       0.30      0.20      0.24        79\n",
            "           4       0.46      0.51      0.48        72\n",
            "           5       0.38      0.52      0.44        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.37      0.39      0.37       495\n",
            "weighted avg       0.37      0.39      0.37       495\n",
            "\n",
            "[[30 26  9  5  8 19]\n",
            " [12 52  2  4  4 11]\n",
            " [16 11 16 17 11 12]\n",
            " [12 15 13 16 14  9]\n",
            " [ 5  7  2  6 37 15]\n",
            " [ 9  8  9  5  7 41]]\n",
            "Accuracy: 0.3878787878787879\n",
            "Total f1 over classes: 2.2470461926220233\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.45      0.36        97\n",
            "           1       0.39      0.46      0.42        85\n",
            "           2       0.00      0.00      0.00        83\n",
            "           3       0.27      0.19      0.22        79\n",
            "           4       0.43      0.40      0.42        72\n",
            "           5       0.31      0.49      0.38        79\n",
            "\n",
            "    accuracy                           0.34       495\n",
            "   macro avg       0.28      0.33      0.30       495\n",
            "weighted avg       0.28      0.34      0.30       495\n",
            "\n",
            "[[44 20  0  7  2 24]\n",
            " [21 39  0  4  7 14]\n",
            " [22 14  0 20 11 16]\n",
            " [19 16  1 15 10 18]\n",
            " [17  5  0  6 29 15]\n",
            " [23  5  0  4  8 39]]\n",
            "Accuracy: 0.33535353535353535\n",
            "Total f1 over classes: 1.8060291753243651\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.41      0.38        97\n",
            "           1       0.46      0.55      0.50        85\n",
            "           2       0.23      0.16      0.19        83\n",
            "           3       0.37      0.25      0.30        79\n",
            "           4       0.51      0.54      0.52        72\n",
            "           5       0.42      0.48      0.45        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.39      0.40      0.39       495\n",
            "weighted avg       0.39      0.40      0.39       495\n",
            "\n",
            "[[40 13 12  6  6 20]\n",
            " [18 47  3  5  4  8]\n",
            " [18 17 13 17 10  8]\n",
            " [18 13 12 20  9  7]\n",
            " [ 6  9  5  3 39 10]\n",
            " [14  3 12  3  9 38]]\n",
            "Accuracy: 0.397979797979798\n",
            "Total f1 over classes: 2.338835638051575\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.03      0.06        97\n",
            "           1       0.43      0.31      0.36        85\n",
            "           2       0.47      0.08      0.14        83\n",
            "           3       0.45      0.06      0.11        79\n",
            "           4       0.18      0.93      0.31        72\n",
            "           5       0.37      0.14      0.20        79\n",
            "\n",
            "    accuracy                           0.24       495\n",
            "   macro avg       0.36      0.26      0.20       495\n",
            "weighted avg       0.36      0.24      0.19       495\n",
            "\n",
            "[[ 3 12  2  2 74  4]\n",
            " [ 0 26  2  0 55  2]\n",
            " [ 2  9  7  4 56  5]\n",
            " [ 4  7  2  5 55  6]\n",
            " [ 0  3  0  0 67  2]\n",
            " [ 3  4  2  0 59 11]]\n",
            "Accuracy: 0.2404040404040404\n",
            "Total f1 over classes: 1.1729494445342126\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.34      0.35        97\n",
            "           1       0.50      0.60      0.55        86\n",
            "           2       0.75      0.07      0.13        83\n",
            "           3       0.42      0.40      0.41        78\n",
            "           4       0.60      0.60      0.60        72\n",
            "           5       0.33      0.61      0.43        79\n",
            "\n",
            "    accuracy                           0.43       495\n",
            "   macro avg       0.49      0.44      0.41       495\n",
            "weighted avg       0.49      0.43      0.41       495\n",
            "\n",
            "[[33 17  0 11  6 30]\n",
            " [17 52  0  3  5  9]\n",
            " [15 11  6 16  7 28]\n",
            " [11  6  1 31  4 25]\n",
            " [ 8  8  0  7 43  6]\n",
            " [ 8  9  1  6  7 48]]\n",
            "Accuracy: 0.4303030303030303\n",
            "Total f1 over classes: 2.4631226570700258\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.31      0.33        97\n",
            "           1       0.34      0.63      0.44        86\n",
            "           2       0.20      0.14      0.17        83\n",
            "           3       0.19      0.19      0.19        78\n",
            "           4       0.44      0.33      0.38        72\n",
            "           5       0.39      0.28      0.32        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.31      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[30 22  8 23  5  9]\n",
            " [ 9 54  5  9  6  3]\n",
            " [16 25 12 14  7  9]\n",
            " [11 22 15 15  5 10]\n",
            " [12 20  4  8 24  4]\n",
            " [ 7 16 16 10  8 22]]\n",
            "Accuracy: 0.31717171717171716\n",
            "Total f1 over classes: 1.830883794251098\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.37      0.40        97\n",
            "           1       0.52      0.59      0.55        86\n",
            "           2       0.30      0.23      0.26        83\n",
            "           3       0.37      0.26      0.30        78\n",
            "           4       0.47      0.65      0.54        72\n",
            "           5       0.38      0.46      0.41        79\n",
            "\n",
            "    accuracy                           0.42       495\n",
            "   macro avg       0.41      0.43      0.41       495\n",
            "weighted avg       0.41      0.42      0.41       495\n",
            "\n",
            "[[36 16 11 11  9 14]\n",
            " [11 51  3  2 12  7]\n",
            " [11 11 19 15  9 18]\n",
            " [ 5  5 19 20 14 15]\n",
            " [ 6  7  4  2 47  6]\n",
            " [13  8  8  4 10 36]]\n",
            "Accuracy: 0.4222222222222222\n",
            "Total f1 over classes: 2.472897339933953\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37        97\n",
            "           1       0.45      0.56      0.50        86\n",
            "           2       0.27      0.11      0.16        83\n",
            "           3       0.34      0.23      0.27        78\n",
            "           4       0.36      0.44      0.40        72\n",
            "           5       0.42      0.38      0.40        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.36      0.36      0.35       495\n",
            "weighted avg       0.36      0.37      0.35       495\n",
            "\n",
            "[[44 21  2  8  7 15]\n",
            " [15 48  3  1 15  4]\n",
            " [24 10  9 13 13 14]\n",
            " [19 11 13 18 10  7]\n",
            " [23  6  2  8 32  1]\n",
            " [17 10  4  5 13 30]]\n",
            "Accuracy: 0.3656565656565657\n",
            "Total f1 over classes: 2.0932441393135925\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.35        97\n",
            "           1       0.55      0.62      0.58        86\n",
            "           2       0.21      0.18      0.19        83\n",
            "           3       0.26      0.21      0.23        78\n",
            "           4       0.52      0.64      0.57        72\n",
            "           5       0.48      0.47      0.47        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.39      0.41      0.40       495\n",
            "weighted avg       0.39      0.41      0.40       495\n",
            "\n",
            "[[34 18 12 17  8  8]\n",
            " [15 53  2  2  8  6]\n",
            " [19 11 15 14 10 14]\n",
            " [10  7 29 16  9  7]\n",
            " [ 8  4  5  4 46  5]\n",
            " [14  3  9  9  7 37]]\n",
            "Accuracy: 0.40606060606060607\n",
            "Total f1 over classes: 2.399074037419379\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.43      0.37        97\n",
            "           1       0.49      0.47      0.48        86\n",
            "           2       0.26      0.10      0.14        83\n",
            "           3       0.26      0.12      0.16        78\n",
            "           4       0.35      0.72      0.47        72\n",
            "           5       0.41      0.35      0.38        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.35      0.36      0.33       495\n",
            "weighted avg       0.35      0.36      0.33       495\n",
            "\n",
            "[[42 12  3  8 21 11]\n",
            " [19 40  1  3 21  2]\n",
            " [21  9  8 12 17 16]\n",
            " [18 10 14  9 19  8]\n",
            " [ 7  6  1  2 52  4]\n",
            " [22  5  4  0 20 28]]\n",
            "Accuracy: 0.3616161616161616\n",
            "Total f1 over classes: 1.9957839018737946\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.18      0.25        97\n",
            "           1       0.39      0.67      0.49        85\n",
            "           2       0.29      0.43      0.35        83\n",
            "           3       0.37      0.24      0.29        79\n",
            "           4       0.55      0.65      0.59        72\n",
            "           5       0.43      0.26      0.32        78\n",
            "\n",
            "    accuracy                           0.40       494\n",
            "   macro avg       0.41      0.40      0.38       494\n",
            "weighted avg       0.41      0.40      0.38       494\n",
            "\n",
            "[[17 35 28  8  5  4]\n",
            " [ 6 57  6  5  9  2]\n",
            " [ 4 13 36 11  8 11]\n",
            " [ 7 12 28 19  8  5]\n",
            " [ 1  9  5  5 47  5]\n",
            " [ 2 22 21  4  9 20]]\n",
            "Accuracy: 0.3967611336032389\n",
            "Total f1 over classes: 2.29584086124482\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.26      0.28        97\n",
            "           1       0.32      0.55      0.40        85\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.23      0.24      0.23        79\n",
            "           4       0.48      0.32      0.38        72\n",
            "           5       0.34      0.26      0.29        78\n",
            "\n",
            "    accuracy                           0.31       494\n",
            "   macro avg       0.32      0.31      0.31       494\n",
            "weighted avg       0.32      0.31      0.30       494\n",
            "\n",
            "[[25 25 15 14  4 14]\n",
            " [ 8 47  5 14  6  5]\n",
            " [20  9 19 19  3 13]\n",
            " [16 21 12 19  6  5]\n",
            " [ 5 26  9  7 23  2]\n",
            " [ 7 20 15 10  6 20]]\n",
            "Accuracy: 0.3097165991902834\n",
            "Total f1 over classes: 1.8347107194008774\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.30      0.32        97\n",
            "           1       0.44      0.58      0.50        85\n",
            "           2       0.30      0.17      0.22        83\n",
            "           3       0.31      0.32      0.31        79\n",
            "           4       0.49      0.50      0.50        72\n",
            "           5       0.38      0.49      0.43        78\n",
            "\n",
            "    accuracy                           0.39       494\n",
            "   macro avg       0.38      0.39      0.38       494\n",
            "weighted avg       0.38      0.39      0.38       494\n",
            "\n",
            "[[29 23 12 12  5 16]\n",
            " [14 49  2  8  7  5]\n",
            " [14  8 14 19  7 21]\n",
            " [12 10 13 25  7 12]\n",
            " [ 6  8  1 14 36  7]\n",
            " [ 8 13  5  3 11 38]]\n",
            "Accuracy: 0.3866396761133603\n",
            "Total f1 over classes: 2.276037092818215\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.26      0.28        97\n",
            "           1       0.36      0.51      0.42        85\n",
            "           2       0.25      0.22      0.23        83\n",
            "           3       0.25      0.04      0.07        79\n",
            "           4       0.46      0.46      0.46        72\n",
            "           5       0.31      0.54      0.39        78\n",
            "\n",
            "    accuracy                           0.33       494\n",
            "   macro avg       0.32      0.34      0.31       494\n",
            "weighted avg       0.32      0.33      0.31       494\n",
            "\n",
            "[[25 23 11  2  3 33]\n",
            " [13 43  7  3  7 12]\n",
            " [17  7 18  3 11 27]\n",
            " [14 18 25  3  6 13]\n",
            " [ 7 15  7  1 33  9]\n",
            " [ 8 12  4  0 12 42]]\n",
            "Accuracy: 0.3319838056680162\n",
            "Total f1 over classes: 1.8489372423892056\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.36      0.33        97\n",
            "           1       0.46      0.48      0.47        85\n",
            "           2       0.22      0.16      0.18        83\n",
            "           3       0.26      0.22      0.24        79\n",
            "           4       0.55      0.57      0.56        72\n",
            "           5       0.41      0.49      0.45        78\n",
            "\n",
            "    accuracy                           0.37       494\n",
            "   macro avg       0.37      0.38      0.37       494\n",
            "weighted avg       0.36      0.37      0.37       494\n",
            "\n",
            "[[35 16 17  9  5 15]\n",
            " [14 41  6 12  6  6]\n",
            " [17  8 13 18  7 20]\n",
            " [20 11 15 17  7  9]\n",
            " [13  5  4  5 41  4]\n",
            " [15  8  5  4  8 38]]\n",
            "Accuracy: 0.37449392712550605\n",
            "Total f1 over classes: 2.229649874393605\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.31      0.34        97\n",
            "           1       0.40      0.44      0.42        85\n",
            "           2       0.29      0.12      0.17        83\n",
            "           3       0.34      0.15      0.21        79\n",
            "           4       0.28      0.61      0.39        72\n",
            "           5       0.29      0.36      0.32        78\n",
            "\n",
            "    accuracy                           0.33       494\n",
            "   macro avg       0.33      0.33      0.31       494\n",
            "weighted avg       0.33      0.33      0.31       494\n",
            "\n",
            "[[30 17  6  8 20 16]\n",
            " [11 37  4  3 22  8]\n",
            " [10 11 10  7 24 21]\n",
            " [13 10  8 12 21 15]\n",
            " [ 8  6  3  4 44  7]\n",
            " [ 9 12  3  1 25 28]]\n",
            "Accuracy: 0.3259109311740891\n",
            "Total f1 over classes: 1.843939809739709\n",
            "Doing test analysis with ANN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.24      0.31        33\n",
            "           1       0.47      0.62      0.53        39\n",
            "           2       0.35      0.48      0.40        44\n",
            "           3       0.30      0.24      0.27        33\n",
            "           4       0.46      0.57      0.51        37\n",
            "           5       0.29      0.15      0.20        34\n",
            "\n",
            "    accuracy                           0.40       220\n",
            "   macro avg       0.38      0.38      0.37       220\n",
            "weighted avg       0.38      0.40      0.38       220\n",
            "\n",
            "[[ 8 10  8  2  3  2]\n",
            " [ 3 24  2  3  6  1]\n",
            " [ 2  6 21  8  6  1]\n",
            " [ 3  2 13  8  4  3]\n",
            " [ 1  2  5  3 21  5]\n",
            " [ 2  7 11  3  6  5]]\n",
            "Accuracy: 0.39545454545454545\n",
            "Total f1 over classes: 2.213640989296553\n",
            "Doing test analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.39      0.33        33\n",
            "           1       0.38      0.49      0.43        39\n",
            "           2       0.17      0.16      0.17        44\n",
            "           3       0.25      0.27      0.26        33\n",
            "           4       0.50      0.32      0.39        37\n",
            "           5       0.42      0.29      0.34        34\n",
            "\n",
            "    accuracy                           0.32       220\n",
            "   macro avg       0.33      0.32      0.32       220\n",
            "weighted avg       0.33      0.32      0.32       220\n",
            "\n",
            "[[13  4  6  3  1  6]\n",
            " [ 8 19  3  5  2  2]\n",
            " [11  8  7 11  4  3]\n",
            " [ 7  4  7  9  3  3]\n",
            " [ 3 11  8  3 12  0]\n",
            " [ 4  4  9  5  2 10]]\n",
            "Accuracy: 0.3181818181818182\n",
            "Total f1 over classes: 1.9218866572272386\n",
            "Doing test analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.39      0.39        33\n",
            "           1       0.47      0.49      0.48        39\n",
            "           2       0.32      0.18      0.23        44\n",
            "           3       0.22      0.24      0.23        33\n",
            "           4       0.45      0.46      0.45        37\n",
            "           5       0.33      0.47      0.39        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.37      0.37      0.36       220\n",
            "weighted avg       0.37      0.37      0.36       220\n",
            "\n",
            "[[13  8  2  1  3  6]\n",
            " [ 6 19  1  5  5  3]\n",
            " [ 5  5  8 15  3  8]\n",
            " [ 4  1  7  8  5  8]\n",
            " [ 2  3  3  5 17  7]\n",
            " [ 3  4  4  2  5 16]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.1822974038816287\n",
            "Doing test analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.39      0.37        33\n",
            "           1       0.41      0.46      0.43        39\n",
            "           2       0.43      0.30      0.35        44\n",
            "           3       0.30      0.09      0.14        33\n",
            "           4       0.42      0.46      0.44        37\n",
            "           5       0.27      0.47      0.34        34\n",
            "\n",
            "    accuracy                           0.36       220\n",
            "   macro avg       0.36      0.36      0.35       220\n",
            "weighted avg       0.37      0.36      0.35       220\n",
            "\n",
            "[[13  6  2  0  2 10]\n",
            " [ 3 18  4  4  5  5]\n",
            " [ 7  5 13  3  5 11]\n",
            " [ 3  7  5  3  6  9]\n",
            " [ 4  3  5  0 17  8]\n",
            " [ 7  5  1  0  5 16]]\n",
            "Accuracy: 0.36363636363636365\n",
            "Total f1 over classes: 2.081694209323707\n",
            "Doing test analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.48      0.40        33\n",
            "           1       0.55      0.46      0.50        39\n",
            "           2       0.29      0.25      0.27        44\n",
            "           3       0.31      0.24      0.27        33\n",
            "           4       0.45      0.41      0.43        37\n",
            "           5       0.33      0.41      0.37        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.38      0.38      0.37       220\n",
            "weighted avg       0.38      0.37      0.37       220\n",
            "\n",
            "[[16  5  3  0  2  7]\n",
            " [ 9 18  2  5  3  2]\n",
            " [10  5 11  6  4  8]\n",
            " [ 3  4  7  8  4  7]\n",
            " [ 4  0  9  5 15  4]\n",
            " [ 6  1  6  2  5 14]]\n",
            "Accuracy: 0.37272727272727274\n",
            "Total f1 over classes: 2.231533333202864\n",
            "Doing test analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.36      0.38        33\n",
            "           1       0.53      0.44      0.48        39\n",
            "           2       0.35      0.14      0.20        44\n",
            "           3       0.31      0.12      0.17        33\n",
            "           4       0.32      0.70      0.44        37\n",
            "           5       0.28      0.38      0.32        34\n",
            "\n",
            "    accuracy                           0.35       220\n",
            "   macro avg       0.36      0.36      0.33       220\n",
            "weighted avg       0.37      0.35      0.33       220\n",
            "\n",
            "[[12  4  0  0  9  8]\n",
            " [ 5 17  2  2 11  2]\n",
            " [ 5  6  6  5 12 10]\n",
            " [ 2  1  7  4  9 10]\n",
            " [ 2  2  1  2 26  4]\n",
            " [ 4  2  1  0 14 13]]\n",
            "Accuracy: 0.35454545454545455\n",
            "Total f1 over classes: 1.9921255957653536\n",
            "No. of training examples: 1979\n",
            "No. of testing examples: 220\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.02      0.04        96\n",
            "           1       0.36      0.34      0.35        85\n",
            "           2       0.26      0.12      0.16        84\n",
            "           3       0.33      0.25      0.29        79\n",
            "           4       0.28      0.69      0.40        72\n",
            "           5       0.31      0.52      0.39        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.30      0.32      0.27       495\n",
            "weighted avg       0.30      0.31      0.26       495\n",
            "\n",
            "[[ 2 19  8  7 25 35]\n",
            " [ 0 29  5  7 30 14]\n",
            " [ 1  7 10 18 26 22]\n",
            " [ 4  7  8 20 29 11]\n",
            " [ 0  9  2  3 50  8]\n",
            " [ 1  9  5  6 17 41]]\n",
            "Accuracy: 0.30707070707070705\n",
            "Total f1 over classes: 1.6317080180994856\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.38        96\n",
            "           1       0.29      0.49      0.37        85\n",
            "           2       0.25      0.17      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.39      0.28      0.33        72\n",
            "           5       0.31      0.29      0.30        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.32      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[37 19 13 10  4 13]\n",
            " [16 42  8  9  3  7]\n",
            " [11 16 14 18  6 19]\n",
            " [10 23  9 22 10  5]\n",
            " [ 5 29  4  6 20  8]\n",
            " [18 15  8  7  8 23]]\n",
            "Accuracy: 0.3191919191919192\n",
            "Total f1 over classes: 1.8655271954040469\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.29      0.31        96\n",
            "           1       0.41      0.47      0.44        85\n",
            "           2       0.35      0.23      0.28        84\n",
            "           3       0.24      0.16      0.20        79\n",
            "           4       0.44      0.54      0.49        72\n",
            "           5       0.29      0.44      0.35        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.35      0.36      0.34       495\n",
            "weighted avg       0.35      0.35      0.34       495\n",
            "\n",
            "[[28 20  8  6  9 25]\n",
            " [19 40  2  8  6 10]\n",
            " [ 6  8 19 14 10 27]\n",
            " [12 10 15 13 17 12]\n",
            " [ 3 10  2  8 39 10]\n",
            " [15  9  8  5  7 35]]\n",
            "Accuracy: 0.3515151515151515\n",
            "Total f1 over classes: 2.0642959957520572\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.42      0.39        96\n",
            "           1       0.37      0.51      0.43        85\n",
            "           2       0.35      0.14      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.38      0.47      0.42        72\n",
            "           5       0.38      0.35      0.37        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.36      0.36      0.35       495\n",
            "weighted avg       0.36      0.36      0.35       495\n",
            "\n",
            "[[40 25  5  8  5 13]\n",
            " [19 43  3  5  4 11]\n",
            " [13 13 12 19 14 13]\n",
            " [12 10  9 22 20  6]\n",
            " [13 15  1  6 34  3]\n",
            " [14 10  4 11 12 28]]\n",
            "Accuracy: 0.3616161616161616\n",
            "Total f1 over classes: 2.0994306106535414\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.44      0.44        96\n",
            "           1       0.51      0.55      0.53        85\n",
            "           2       0.35      0.21      0.26        84\n",
            "           3       0.28      0.25      0.27        79\n",
            "           4       0.46      0.53      0.49        72\n",
            "           5       0.38      0.48      0.42        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.40      0.41      0.40       495\n",
            "weighted avg       0.40      0.41      0.40       495\n",
            "\n",
            "[[42 18 13  9  1 13]\n",
            " [14 47  6  5  4  9]\n",
            " [ 9  5 18 19 14 19]\n",
            " [13  9  9 20 16 12]\n",
            " [ 4 10  2  9 38  9]\n",
            " [15  3  4  9 10 38]]\n",
            "Accuracy: 0.4101010101010101\n",
            "Total f1 over classes: 2.4125827422008066\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.19      0.20        96\n",
            "           1       0.43      0.42      0.43        85\n",
            "           2       0.44      0.14      0.22        84\n",
            "           3       0.27      0.08      0.12        79\n",
            "           4       0.32      0.65      0.43        72\n",
            "           5       0.27      0.44      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.32      0.29       495\n",
            "weighted avg       0.32      0.31      0.28       495\n",
            "\n",
            "[[18 12  3  2 23 38]\n",
            " [19 36  2  1 18  9]\n",
            " [12  8 12  4 22 26]\n",
            " [12 12  8  6 25 16]\n",
            " [ 6  7  1  5 47  6]\n",
            " [17  8  1  4 14 35]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.7238671221566724\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.06      0.10        97\n",
            "           1       0.45      0.55      0.50        85\n",
            "           2       0.31      0.48      0.38        83\n",
            "           3       0.42      0.23      0.30        79\n",
            "           4       0.36      0.57      0.44        72\n",
            "           5       0.37      0.39      0.38        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.37      0.38      0.35       495\n",
            "weighted avg       0.36      0.37      0.34       495\n",
            "\n",
            "[[ 6 20 31  4 14 22]\n",
            " [ 3 47  7  4 15  9]\n",
            " [ 3 10 40  9 13  8]\n",
            " [ 6  8 26 18 18  3]\n",
            " [ 0  8  7  5 41 11]\n",
            " [ 3 11 19  3 12 31]]\n",
            "Accuracy: 0.3696969696969697\n",
            "Total f1 over classes: 2.0933295756846957\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.28      0.51      0.36        85\n",
            "           2       0.21      0.16      0.18        83\n",
            "           3       0.30      0.24      0.27        79\n",
            "           4       0.42      0.31      0.35        72\n",
            "           5       0.38      0.29      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.31      0.30       495\n",
            "weighted avg       0.32      0.31      0.31       495\n",
            "\n",
            "[[34 23 17  8  6  9]\n",
            " [17 43  5  3  4 13]\n",
            " [15 27 13 17  8  3]\n",
            " [13 26  7 19  8  6]\n",
            " [ 6 22  8  7 22  7]\n",
            " [16 15 11  9  5 23]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.82901343439319\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.31      0.33        97\n",
            "           1       0.44      0.61      0.51        85\n",
            "           2       0.31      0.19      0.24        83\n",
            "           3       0.30      0.20      0.24        79\n",
            "           4       0.46      0.51      0.48        72\n",
            "           5       0.38      0.52      0.44        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.37      0.39      0.37       495\n",
            "weighted avg       0.37      0.39      0.37       495\n",
            "\n",
            "[[30 26  9  5  8 19]\n",
            " [12 52  2  4  4 11]\n",
            " [16 11 16 17 11 12]\n",
            " [12 15 13 16 14  9]\n",
            " [ 5  7  2  6 37 15]\n",
            " [ 9  8  9  5  7 41]]\n",
            "Accuracy: 0.3878787878787879\n",
            "Total f1 over classes: 2.2470461926220233\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.45      0.36        97\n",
            "           1       0.39      0.46      0.42        85\n",
            "           2       0.00      0.00      0.00        83\n",
            "           3       0.27      0.19      0.22        79\n",
            "           4       0.43      0.40      0.42        72\n",
            "           5       0.31      0.49      0.38        79\n",
            "\n",
            "    accuracy                           0.34       495\n",
            "   macro avg       0.28      0.33      0.30       495\n",
            "weighted avg       0.28      0.34      0.30       495\n",
            "\n",
            "[[44 20  0  7  2 24]\n",
            " [21 39  0  4  7 14]\n",
            " [22 14  0 20 11 16]\n",
            " [19 16  1 15 10 18]\n",
            " [17  5  0  6 29 15]\n",
            " [23  5  0  4  8 39]]\n",
            "Accuracy: 0.33535353535353535\n",
            "Total f1 over classes: 1.8060291753243651\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.47      0.43        97\n",
            "           1       0.46      0.53      0.49        85\n",
            "           2       0.24      0.18      0.21        83\n",
            "           3       0.39      0.25      0.31        79\n",
            "           4       0.46      0.50      0.48        72\n",
            "           5       0.46      0.53      0.49        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.40      0.41      0.40       495\n",
            "weighted avg       0.40      0.41      0.40       495\n",
            "\n",
            "[[46  9 13  4  9 16]\n",
            " [16 45  5  4  6  9]\n",
            " [19 16 15 14 10  9]\n",
            " [18 14 11 20  9  7]\n",
            " [ 4  9  6  8 36  9]\n",
            " [12  4 12  1  8 42]]\n",
            "Accuracy: 0.4121212121212121\n",
            "Total f1 over classes: 2.4142846882483218\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.03      0.06        97\n",
            "           1       0.43      0.31      0.36        85\n",
            "           2       0.47      0.08      0.14        83\n",
            "           3       0.45      0.06      0.11        79\n",
            "           4       0.18      0.93      0.31        72\n",
            "           5       0.37      0.14      0.20        79\n",
            "\n",
            "    accuracy                           0.24       495\n",
            "   macro avg       0.36      0.26      0.20       495\n",
            "weighted avg       0.36      0.24      0.19       495\n",
            "\n",
            "[[ 3 12  2  2 74  4]\n",
            " [ 0 26  2  0 55  2]\n",
            " [ 2  9  7  4 56  5]\n",
            " [ 4  7  2  5 55  6]\n",
            " [ 0  3  0  0 67  2]\n",
            " [ 3  4  2  0 59 11]]\n",
            "Accuracy: 0.2404040404040404\n",
            "Total f1 over classes: 1.1729494445342126\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.45      0.42        97\n",
            "           1       0.67      0.53      0.59        86\n",
            "           2       0.41      0.19      0.26        83\n",
            "           3       0.49      0.31      0.38        78\n",
            "           4       0.49      0.64      0.55        72\n",
            "           5       0.30      0.49      0.38        79\n",
            "\n",
            "    accuracy                           0.43       495\n",
            "   macro avg       0.46      0.44      0.43       495\n",
            "weighted avg       0.46      0.43      0.43       495\n",
            "\n",
            "[[44 11  3  6  9 24]\n",
            " [14 46  2  1 10 13]\n",
            " [17  3 16 14  9 24]\n",
            " [13  3  9 24  9 20]\n",
            " [12  2  2  1 46  9]\n",
            " [15  4  7  3 11 39]]\n",
            "Accuracy: 0.43434343434343436\n",
            "Total f1 over classes: 2.57810743206202\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.31      0.33        97\n",
            "           1       0.34      0.63      0.44        86\n",
            "           2       0.20      0.14      0.17        83\n",
            "           3       0.19      0.19      0.19        78\n",
            "           4       0.44      0.33      0.38        72\n",
            "           5       0.39      0.28      0.32        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.31      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[30 22  8 23  5  9]\n",
            " [ 9 54  5  9  6  3]\n",
            " [16 25 12 14  7  9]\n",
            " [11 22 15 15  5 10]\n",
            " [12 20  4  8 24  4]\n",
            " [ 7 16 16 10  8 22]]\n",
            "Accuracy: 0.31717171717171716\n",
            "Total f1 over classes: 1.830883794251098\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.37      0.40        97\n",
            "           1       0.52      0.59      0.55        86\n",
            "           2       0.30      0.23      0.26        83\n",
            "           3       0.37      0.26      0.30        78\n",
            "           4       0.47      0.65      0.54        72\n",
            "           5       0.38      0.46      0.41        79\n",
            "\n",
            "    accuracy                           0.42       495\n",
            "   macro avg       0.41      0.43      0.41       495\n",
            "weighted avg       0.41      0.42      0.41       495\n",
            "\n",
            "[[36 16 11 11  9 14]\n",
            " [11 51  3  2 12  7]\n",
            " [11 11 19 15  9 18]\n",
            " [ 5  5 19 20 14 15]\n",
            " [ 6  7  4  2 47  6]\n",
            " [13  8  8  4 10 36]]\n",
            "Accuracy: 0.4222222222222222\n",
            "Total f1 over classes: 2.472897339933953\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37        97\n",
            "           1       0.45      0.56      0.50        86\n",
            "           2       0.27      0.11      0.16        83\n",
            "           3       0.34      0.23      0.27        78\n",
            "           4       0.36      0.44      0.40        72\n",
            "           5       0.42      0.38      0.40        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.36      0.36      0.35       495\n",
            "weighted avg       0.36      0.37      0.35       495\n",
            "\n",
            "[[44 21  2  8  7 15]\n",
            " [15 48  3  1 15  4]\n",
            " [24 10  9 13 13 14]\n",
            " [19 11 13 18 10  7]\n",
            " [23  6  2  8 32  1]\n",
            " [17 10  4  5 13 30]]\n",
            "Accuracy: 0.3656565656565657\n",
            "Total f1 over classes: 2.0932441393135925\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.38      0.36        97\n",
            "           1       0.56      0.64      0.60        86\n",
            "           2       0.19      0.17      0.18        83\n",
            "           3       0.31      0.26      0.28        78\n",
            "           4       0.53      0.58      0.56        72\n",
            "           5       0.45      0.42      0.43        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.40      0.41      0.40       495\n",
            "weighted avg       0.40      0.41      0.40       495\n",
            "\n",
            "[[37 18 13 14  5 10]\n",
            " [13 55  4  1  8  5]\n",
            " [22 10 14 14  9 14]\n",
            " [13  5 25 20  7  8]\n",
            " [ 8  5  7  6 42  4]\n",
            " [13  5 11  9  8 33]]\n",
            "Accuracy: 0.40606060606060607\n",
            "Total f1 over classes: 2.4100561362986963\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.43      0.37        97\n",
            "           1       0.49      0.47      0.48        86\n",
            "           2       0.26      0.10      0.14        83\n",
            "           3       0.26      0.12      0.16        78\n",
            "           4       0.35      0.72      0.47        72\n",
            "           5       0.41      0.35      0.38        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.35      0.36      0.33       495\n",
            "weighted avg       0.35      0.36      0.33       495\n",
            "\n",
            "[[42 12  3  8 21 11]\n",
            " [19 40  1  3 21  2]\n",
            " [21  9  8 12 17 16]\n",
            " [18 10 14  9 19  8]\n",
            " [ 7  6  1  2 52  4]\n",
            " [22  5  4  0 20 28]]\n",
            "Accuracy: 0.3616161616161616\n",
            "Total f1 over classes: 1.9957839018737946\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.19      0.25        97\n",
            "           1       0.48      0.52      0.50        85\n",
            "           2       0.27      0.23      0.25        83\n",
            "           3       0.30      0.58      0.40        79\n",
            "           4       0.56      0.49      0.52        72\n",
            "           5       0.42      0.40      0.41        78\n",
            "\n",
            "    accuracy                           0.39       494\n",
            "   macro avg       0.41      0.40      0.39       494\n",
            "weighted avg       0.40      0.39      0.38       494\n",
            "\n",
            "[[18 15 21 27  5 11]\n",
            " [ 6 44  7 17  7  4]\n",
            " [ 6  9 19 28  6 15]\n",
            " [ 6  5 11 46  2  9]\n",
            " [ 4  7  2 20 35  4]\n",
            " [ 5 11 10 14  7 31]]\n",
            "Accuracy: 0.39068825910931176\n",
            "Total f1 over classes: 2.330438334644455\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.26      0.28        97\n",
            "           1       0.32      0.55      0.40        85\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.23      0.24      0.23        79\n",
            "           4       0.48      0.32      0.38        72\n",
            "           5       0.34      0.26      0.29        78\n",
            "\n",
            "    accuracy                           0.31       494\n",
            "   macro avg       0.32      0.31      0.31       494\n",
            "weighted avg       0.32      0.31      0.30       494\n",
            "\n",
            "[[25 25 15 14  4 14]\n",
            " [ 8 47  5 14  6  5]\n",
            " [20  9 19 19  3 13]\n",
            " [16 21 12 19  6  5]\n",
            " [ 5 26  9  7 23  2]\n",
            " [ 7 20 15 10  6 20]]\n",
            "Accuracy: 0.3097165991902834\n",
            "Total f1 over classes: 1.8347107194008774\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.30      0.32        97\n",
            "           1       0.44      0.58      0.50        85\n",
            "           2       0.30      0.17      0.22        83\n",
            "           3       0.31      0.32      0.31        79\n",
            "           4       0.49      0.50      0.50        72\n",
            "           5       0.38      0.49      0.43        78\n",
            "\n",
            "    accuracy                           0.39       494\n",
            "   macro avg       0.38      0.39      0.38       494\n",
            "weighted avg       0.38      0.39      0.38       494\n",
            "\n",
            "[[29 23 12 12  5 16]\n",
            " [14 49  2  8  7  5]\n",
            " [14  8 14 19  7 21]\n",
            " [12 10 13 25  7 12]\n",
            " [ 6  8  1 14 36  7]\n",
            " [ 8 13  5  3 11 38]]\n",
            "Accuracy: 0.3866396761133603\n",
            "Total f1 over classes: 2.276037092818215\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.26      0.28        97\n",
            "           1       0.36      0.51      0.42        85\n",
            "           2       0.25      0.22      0.23        83\n",
            "           3       0.25      0.04      0.07        79\n",
            "           4       0.46      0.46      0.46        72\n",
            "           5       0.31      0.54      0.39        78\n",
            "\n",
            "    accuracy                           0.33       494\n",
            "   macro avg       0.32      0.34      0.31       494\n",
            "weighted avg       0.32      0.33      0.31       494\n",
            "\n",
            "[[25 23 11  2  3 33]\n",
            " [13 43  7  3  7 12]\n",
            " [17  7 18  3 11 27]\n",
            " [14 18 25  3  6 13]\n",
            " [ 7 15  7  1 33  9]\n",
            " [ 8 12  4  0 12 42]]\n",
            "Accuracy: 0.3319838056680162\n",
            "Total f1 over classes: 1.8489372423892056\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.35        97\n",
            "           1       0.46      0.52      0.49        85\n",
            "           2       0.22      0.17      0.19        83\n",
            "           3       0.33      0.27      0.29        79\n",
            "           4       0.55      0.57      0.56        72\n",
            "           5       0.43      0.54      0.48        78\n",
            "\n",
            "    accuracy                           0.40       494\n",
            "   macro avg       0.39      0.40      0.39       494\n",
            "weighted avg       0.38      0.40      0.39       494\n",
            "\n",
            "[[34 18 15 12  3 15]\n",
            " [12 44  7  8  6  8]\n",
            " [18 10 14 16  8 17]\n",
            " [15 11 15 21  8  9]\n",
            " [ 9  5  6  4 41  7]\n",
            " [11  7  6  3  9 42]]\n",
            "Accuracy: 0.3967611336032389\n",
            "Total f1 over classes: 2.356410636547623\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.31      0.34        97\n",
            "           1       0.40      0.44      0.42        85\n",
            "           2       0.29      0.12      0.17        83\n",
            "           3       0.34      0.15      0.21        79\n",
            "           4       0.28      0.61      0.39        72\n",
            "           5       0.29      0.36      0.32        78\n",
            "\n",
            "    accuracy                           0.33       494\n",
            "   macro avg       0.33      0.33      0.31       494\n",
            "weighted avg       0.33      0.33      0.31       494\n",
            "\n",
            "[[30 17  6  8 20 16]\n",
            " [11 37  4  3 22  8]\n",
            " [10 11 10  7 24 21]\n",
            " [13 10  8 12 21 15]\n",
            " [ 8  6  3  4 44  7]\n",
            " [ 9 12  3  1 25 28]]\n",
            "Accuracy: 0.3259109311740891\n",
            "Total f1 over classes: 1.843939809739709\n",
            "Doing test analysis with ANN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.33      0.40        33\n",
            "           1       0.57      0.51      0.54        39\n",
            "           2       0.27      0.23      0.25        44\n",
            "           3       0.24      0.48      0.32        33\n",
            "           4       0.59      0.43      0.50        37\n",
            "           5       0.42      0.41      0.42        34\n",
            "\n",
            "    accuracy                           0.40       220\n",
            "   macro avg       0.43      0.40      0.40       220\n",
            "weighted avg       0.43      0.40      0.40       220\n",
            "\n",
            "[[11  4  5  5  3  5]\n",
            " [ 2 20  3 12  0  2]\n",
            " [ 4  4 10 20  3  3]\n",
            " [ 2  1 10 16  1  3]\n",
            " [ 2  2  1 10 16  6]\n",
            " [ 1  4  8  3  4 14]]\n",
            "Accuracy: 0.39545454545454545\n",
            "Total f1 over classes: 2.428596891780971\n",
            "Doing test analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.39      0.33        33\n",
            "           1       0.38      0.49      0.43        39\n",
            "           2       0.17      0.16      0.17        44\n",
            "           3       0.25      0.27      0.26        33\n",
            "           4       0.50      0.32      0.39        37\n",
            "           5       0.42      0.29      0.34        34\n",
            "\n",
            "    accuracy                           0.32       220\n",
            "   macro avg       0.33      0.32      0.32       220\n",
            "weighted avg       0.33      0.32      0.32       220\n",
            "\n",
            "[[13  4  6  3  1  6]\n",
            " [ 8 19  3  5  2  2]\n",
            " [11  8  7 11  4  3]\n",
            " [ 7  4  7  9  3  3]\n",
            " [ 3 11  8  3 12  0]\n",
            " [ 4  4  9  5  2 10]]\n",
            "Accuracy: 0.3181818181818182\n",
            "Total f1 over classes: 1.9218866572272386\n",
            "Doing test analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.39      0.39        33\n",
            "           1       0.47      0.49      0.48        39\n",
            "           2       0.32      0.18      0.23        44\n",
            "           3       0.22      0.24      0.23        33\n",
            "           4       0.45      0.46      0.45        37\n",
            "           5       0.33      0.47      0.39        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.37      0.37      0.36       220\n",
            "weighted avg       0.37      0.37      0.36       220\n",
            "\n",
            "[[13  8  2  1  3  6]\n",
            " [ 6 19  1  5  5  3]\n",
            " [ 5  5  8 15  3  8]\n",
            " [ 4  1  7  8  5  8]\n",
            " [ 2  3  3  5 17  7]\n",
            " [ 3  4  4  2  5 16]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.1822974038816287\n",
            "Doing test analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.39      0.37        33\n",
            "           1       0.41      0.46      0.43        39\n",
            "           2       0.43      0.30      0.35        44\n",
            "           3       0.30      0.09      0.14        33\n",
            "           4       0.42      0.46      0.44        37\n",
            "           5       0.27      0.47      0.34        34\n",
            "\n",
            "    accuracy                           0.36       220\n",
            "   macro avg       0.36      0.36      0.35       220\n",
            "weighted avg       0.37      0.36      0.35       220\n",
            "\n",
            "[[13  6  2  0  2 10]\n",
            " [ 3 18  4  4  5  5]\n",
            " [ 7  5 13  3  5 11]\n",
            " [ 3  7  5  3  6  9]\n",
            " [ 4  3  5  0 17  8]\n",
            " [ 7  5  1  0  5 16]]\n",
            "Accuracy: 0.36363636363636365\n",
            "Total f1 over classes: 2.081694209323707\n",
            "Doing test analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.39      0.31        33\n",
            "           1       0.55      0.46      0.50        39\n",
            "           2       0.35      0.25      0.29        44\n",
            "           3       0.27      0.24      0.25        33\n",
            "           4       0.55      0.43      0.48        37\n",
            "           5       0.38      0.53      0.44        34\n",
            "\n",
            "    accuracy                           0.38       220\n",
            "   macro avg       0.39      0.38      0.38       220\n",
            "weighted avg       0.40      0.38      0.38       220\n",
            "\n",
            "[[13  8  2  1  1  8]\n",
            " [ 9 18  3  4  3  2]\n",
            " [12  3 11  9  2  7]\n",
            " [ 6  2  7  8  4  6]\n",
            " [ 5  1  4  5 16  6]\n",
            " [ 5  1  4  3  3 18]]\n",
            "Accuracy: 0.38181818181818183\n",
            "Total f1 over classes: 2.2898475286427096\n",
            "Doing test analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.36      0.38        33\n",
            "           1       0.53      0.44      0.48        39\n",
            "           2       0.35      0.14      0.20        44\n",
            "           3       0.31      0.12      0.17        33\n",
            "           4       0.32      0.70      0.44        37\n",
            "           5       0.28      0.38      0.32        34\n",
            "\n",
            "    accuracy                           0.35       220\n",
            "   macro avg       0.36      0.36      0.33       220\n",
            "weighted avg       0.37      0.35      0.33       220\n",
            "\n",
            "[[12  4  0  0  9  8]\n",
            " [ 5 17  2  2 11  2]\n",
            " [ 5  6  6  5 12 10]\n",
            " [ 2  1  7  4  9 10]\n",
            " [ 2  2  1  2 26  4]\n",
            " [ 4  2  1  0 14 13]]\n",
            "Accuracy: 0.35454545454545455\n",
            "Total f1 over classes: 1.9921255957653536\n",
            "No. of training examples: 1979\n",
            "No. of testing examples: 220\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.04      0.08        96\n",
            "           1       0.36      0.73      0.48        85\n",
            "           2       0.38      0.18      0.24        84\n",
            "           3       0.30      0.33      0.32        79\n",
            "           4       0.48      0.67      0.56        72\n",
            "           5       0.32      0.37      0.34        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.38      0.39      0.34       495\n",
            "weighted avg       0.38      0.37      0.33       495\n",
            "\n",
            "[[ 4 40  7 14  8 23]\n",
            " [ 1 62  3  7  6  6]\n",
            " [ 2 13 15 19 16 19]\n",
            " [ 0 19 10 26 15  9]\n",
            " [ 2 12  0  6 48  4]\n",
            " [ 0 25  5 14  6 29]]\n",
            "Accuracy: 0.3717171717171717\n",
            "Total f1 over classes: 2.0222512502570784\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.38        96\n",
            "           1       0.29      0.49      0.37        85\n",
            "           2       0.25      0.17      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.39      0.28      0.33        72\n",
            "           5       0.31      0.29      0.30        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.32      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[37 19 13 10  4 13]\n",
            " [16 42  8  9  3  7]\n",
            " [11 16 14 18  6 19]\n",
            " [10 23  9 22 10  5]\n",
            " [ 5 29  4  6 20  8]\n",
            " [18 15  8  7  8 23]]\n",
            "Accuracy: 0.3191919191919192\n",
            "Total f1 over classes: 1.8655271954040469\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.29      0.31        96\n",
            "           1       0.41      0.47      0.44        85\n",
            "           2       0.35      0.23      0.28        84\n",
            "           3       0.24      0.16      0.20        79\n",
            "           4       0.44      0.54      0.49        72\n",
            "           5       0.29      0.44      0.35        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.35      0.36      0.34       495\n",
            "weighted avg       0.35      0.35      0.34       495\n",
            "\n",
            "[[28 20  8  6  9 25]\n",
            " [19 40  2  8  6 10]\n",
            " [ 6  8 19 14 10 27]\n",
            " [12 10 15 13 17 12]\n",
            " [ 3 10  2  8 39 10]\n",
            " [15  9  8  5  7 35]]\n",
            "Accuracy: 0.3515151515151515\n",
            "Total f1 over classes: 2.0642959957520572\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.42      0.39        96\n",
            "           1       0.37      0.51      0.43        85\n",
            "           2       0.35      0.14      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.38      0.47      0.42        72\n",
            "           5       0.38      0.35      0.37        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.36      0.36      0.35       495\n",
            "weighted avg       0.36      0.36      0.35       495\n",
            "\n",
            "[[40 25  5  8  5 13]\n",
            " [19 43  3  5  4 11]\n",
            " [13 13 12 19 14 13]\n",
            " [12 10  9 22 20  6]\n",
            " [13 15  1  6 34  3]\n",
            " [14 10  4 11 12 28]]\n",
            "Accuracy: 0.3616161616161616\n",
            "Total f1 over classes: 2.0994306106535414\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.40      0.38        96\n",
            "           1       0.51      0.52      0.51        85\n",
            "           2       0.26      0.18      0.21        84\n",
            "           3       0.26      0.27      0.26        79\n",
            "           4       0.46      0.53      0.49        72\n",
            "           5       0.35      0.37      0.36        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.37      0.38      0.37       495\n",
            "weighted avg       0.37      0.37      0.37       495\n",
            "\n",
            "[[38 18 15 11  2 12]\n",
            " [19 44  5  7  3  7]\n",
            " [10  5 15 22 14 18]\n",
            " [13  8 12 21 16  9]\n",
            " [ 6  9  2 10 38  7]\n",
            " [19  2  8 11 10 29]]\n",
            "Accuracy: 0.37373737373737376\n",
            "Total f1 over classes: 2.216935886291584\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.19      0.20        96\n",
            "           1       0.43      0.42      0.43        85\n",
            "           2       0.44      0.14      0.22        84\n",
            "           3       0.27      0.08      0.12        79\n",
            "           4       0.32      0.65      0.43        72\n",
            "           5       0.27      0.44      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.32      0.29       495\n",
            "weighted avg       0.32      0.31      0.28       495\n",
            "\n",
            "[[18 12  3  2 23 38]\n",
            " [19 36  2  1 18  9]\n",
            " [12  8 12  4 22 26]\n",
            " [12 12  8  6 25 16]\n",
            " [ 6  7  1  5 47  6]\n",
            " [17  8  1  4 14 35]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.7238671221566724\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.08      0.14        97\n",
            "           1       0.39      0.74      0.51        85\n",
            "           2       0.38      0.13      0.20        83\n",
            "           3       0.36      0.34      0.35        79\n",
            "           4       0.35      0.62      0.45        72\n",
            "           5       0.38      0.38      0.38        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.37      0.38      0.34       495\n",
            "weighted avg       0.37      0.37      0.33       495\n",
            "\n",
            "[[ 8 40  6  7 16 20]\n",
            " [ 2 63  1  4 11  4]\n",
            " [ 6 16 11 21 19 10]\n",
            " [ 2 20  6 27 16  8]\n",
            " [ 1  9  0  9 45  8]\n",
            " [ 2 13  5  7 22 30]]\n",
            "Accuracy: 0.3717171717171717\n",
            "Total f1 over classes: 2.0199859489640133\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.28      0.51      0.36        85\n",
            "           2       0.21      0.16      0.18        83\n",
            "           3       0.30      0.24      0.27        79\n",
            "           4       0.42      0.31      0.35        72\n",
            "           5       0.38      0.29      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.31      0.30       495\n",
            "weighted avg       0.32      0.31      0.31       495\n",
            "\n",
            "[[34 23 17  8  6  9]\n",
            " [17 43  5  3  4 13]\n",
            " [15 27 13 17  8  3]\n",
            " [13 26  7 19  8  6]\n",
            " [ 6 22  8  7 22  7]\n",
            " [16 15 11  9  5 23]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.82901343439319\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.31      0.33        97\n",
            "           1       0.44      0.61      0.51        85\n",
            "           2       0.31      0.19      0.24        83\n",
            "           3       0.30      0.20      0.24        79\n",
            "           4       0.46      0.51      0.48        72\n",
            "           5       0.38      0.52      0.44        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.37      0.39      0.37       495\n",
            "weighted avg       0.37      0.39      0.37       495\n",
            "\n",
            "[[30 26  9  5  8 19]\n",
            " [12 52  2  4  4 11]\n",
            " [16 11 16 17 11 12]\n",
            " [12 15 13 16 14  9]\n",
            " [ 5  7  2  6 37 15]\n",
            " [ 9  8  9  5  7 41]]\n",
            "Accuracy: 0.3878787878787879\n",
            "Total f1 over classes: 2.2470461926220233\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.45      0.36        97\n",
            "           1       0.39      0.46      0.42        85\n",
            "           2       0.00      0.00      0.00        83\n",
            "           3       0.27      0.19      0.22        79\n",
            "           4       0.43      0.40      0.42        72\n",
            "           5       0.31      0.49      0.38        79\n",
            "\n",
            "    accuracy                           0.34       495\n",
            "   macro avg       0.28      0.33      0.30       495\n",
            "weighted avg       0.28      0.34      0.30       495\n",
            "\n",
            "[[44 20  0  7  2 24]\n",
            " [21 39  0  4  7 14]\n",
            " [22 14  0 20 11 16]\n",
            " [19 16  1 15 10 18]\n",
            " [17  5  0  6 29 15]\n",
            " [23  5  0  4  8 39]]\n",
            "Accuracy: 0.33535353535353535\n",
            "Total f1 over classes: 1.8060291753243651\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.45      0.41        97\n",
            "           1       0.45      0.56      0.50        85\n",
            "           2       0.28      0.18      0.22        83\n",
            "           3       0.31      0.19      0.23        79\n",
            "           4       0.47      0.50      0.48        72\n",
            "           5       0.39      0.46      0.42        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.38       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[44 14  6  5  8 20]\n",
            " [16 48  5  4  5  7]\n",
            " [21 17 15 14  9  7]\n",
            " [18 14 11 15  9 12]\n",
            " [ 6  8  5  7 36 10]\n",
            " [12  6 11  4 10 36]]\n",
            "Accuracy: 0.39191919191919194\n",
            "Total f1 over classes: 2.2704522966541605\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.03      0.06        97\n",
            "           1       0.43      0.31      0.36        85\n",
            "           2       0.47      0.08      0.14        83\n",
            "           3       0.45      0.06      0.11        79\n",
            "           4       0.18      0.93      0.31        72\n",
            "           5       0.37      0.14      0.20        79\n",
            "\n",
            "    accuracy                           0.24       495\n",
            "   macro avg       0.36      0.26      0.20       495\n",
            "weighted avg       0.36      0.24      0.19       495\n",
            "\n",
            "[[ 3 12  2  2 74  4]\n",
            " [ 0 26  2  0 55  2]\n",
            " [ 2  9  7  4 56  5]\n",
            " [ 4  7  2  5 55  6]\n",
            " [ 0  3  0  0 67  2]\n",
            " [ 3  4  2  0 59 11]]\n",
            "Accuracy: 0.2404040404040404\n",
            "Total f1 over classes: 1.1729494445342126\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.60      0.45        97\n",
            "           1       0.74      0.30      0.43        86\n",
            "           2       0.50      0.11      0.18        83\n",
            "           3       0.35      0.47      0.40        78\n",
            "           4       0.52      0.61      0.56        72\n",
            "           5       0.34      0.41      0.37        79\n",
            "\n",
            "    accuracy                           0.42       495\n",
            "   macro avg       0.47      0.42      0.40       495\n",
            "weighted avg       0.47      0.42      0.40       495\n",
            "\n",
            "[[58  2  2 15  8 12]\n",
            " [32 26  0 10  8 10]\n",
            " [21  4  9 21  9 19]\n",
            " [14  1  5 37  5 16]\n",
            " [11  1  1 11 44  4]\n",
            " [22  1  1 13 10 32]]\n",
            "Accuracy: 0.4161616161616162\n",
            "Total f1 over classes: 2.399067436040572\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.31      0.33        97\n",
            "           1       0.34      0.63      0.44        86\n",
            "           2       0.20      0.14      0.17        83\n",
            "           3       0.19      0.19      0.19        78\n",
            "           4       0.44      0.33      0.38        72\n",
            "           5       0.39      0.28      0.32        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.31      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[30 22  8 23  5  9]\n",
            " [ 9 54  5  9  6  3]\n",
            " [16 25 12 14  7  9]\n",
            " [11 22 15 15  5 10]\n",
            " [12 20  4  8 24  4]\n",
            " [ 7 16 16 10  8 22]]\n",
            "Accuracy: 0.31717171717171716\n",
            "Total f1 over classes: 1.830883794251098\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.37      0.40        97\n",
            "           1       0.52      0.59      0.55        86\n",
            "           2       0.30      0.23      0.26        83\n",
            "           3       0.37      0.26      0.30        78\n",
            "           4       0.47      0.65      0.54        72\n",
            "           5       0.38      0.46      0.41        79\n",
            "\n",
            "    accuracy                           0.42       495\n",
            "   macro avg       0.41      0.43      0.41       495\n",
            "weighted avg       0.41      0.42      0.41       495\n",
            "\n",
            "[[36 16 11 11  9 14]\n",
            " [11 51  3  2 12  7]\n",
            " [11 11 19 15  9 18]\n",
            " [ 5  5 19 20 14 15]\n",
            " [ 6  7  4  2 47  6]\n",
            " [13  8  8  4 10 36]]\n",
            "Accuracy: 0.4222222222222222\n",
            "Total f1 over classes: 2.472897339933953\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37        97\n",
            "           1       0.45      0.56      0.50        86\n",
            "           2       0.27      0.11      0.16        83\n",
            "           3       0.34      0.23      0.27        78\n",
            "           4       0.36      0.44      0.40        72\n",
            "           5       0.42      0.38      0.40        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.36      0.36      0.35       495\n",
            "weighted avg       0.36      0.37      0.35       495\n",
            "\n",
            "[[44 21  2  8  7 15]\n",
            " [15 48  3  1 15  4]\n",
            " [24 10  9 13 13 14]\n",
            " [19 11 13 18 10  7]\n",
            " [23  6  2  8 32  1]\n",
            " [17 10  4  5 13 30]]\n",
            "Accuracy: 0.3656565656565657\n",
            "Total f1 over classes: 2.0932441393135925\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.33      0.34        97\n",
            "           1       0.51      0.58      0.54        86\n",
            "           2       0.18      0.16      0.17        83\n",
            "           3       0.30      0.27      0.28        78\n",
            "           4       0.49      0.57      0.53        72\n",
            "           5       0.42      0.41      0.41        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.37      0.39      0.38       495\n",
            "weighted avg       0.37      0.38      0.38       495\n",
            "\n",
            "[[32 18 14 16  7 10]\n",
            " [13 50  3  1 11  8]\n",
            " [18 12 13 16 10 14]\n",
            " [10  7 24 21  7  9]\n",
            " [ 7  7  7  7 41  3]\n",
            " [13  5 13  9  7 32]]\n",
            "Accuracy: 0.38181818181818183\n",
            "Total f1 over classes: 2.268707008999851\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.43      0.37        97\n",
            "           1       0.49      0.47      0.48        86\n",
            "           2       0.26      0.10      0.14        83\n",
            "           3       0.26      0.12      0.16        78\n",
            "           4       0.35      0.72      0.47        72\n",
            "           5       0.41      0.35      0.38        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.35      0.36      0.33       495\n",
            "weighted avg       0.35      0.36      0.33       495\n",
            "\n",
            "[[42 12  3  8 21 11]\n",
            " [19 40  1  3 21  2]\n",
            " [21  9  8 12 17 16]\n",
            " [18 10 14  9 19  8]\n",
            " [ 7  6  1  2 52  4]\n",
            " [22  5  4  0 20 28]]\n",
            "Accuracy: 0.3616161616161616\n",
            "Total f1 over classes: 1.9957839018737946\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.53      0.46        97\n",
            "           1       0.44      0.61      0.51        85\n",
            "           2       0.56      0.06      0.11        83\n",
            "           3       0.38      0.29      0.33        79\n",
            "           4       0.63      0.50      0.56        72\n",
            "           5       0.36      0.56      0.44        78\n",
            "\n",
            "    accuracy                           0.43       494\n",
            "   macro avg       0.46      0.43      0.40       494\n",
            "weighted avg       0.46      0.43      0.40       494\n",
            "\n",
            "[[51 21  0  4  3 18]\n",
            " [11 52  0  5  4 13]\n",
            " [23 10  5 16  4 25]\n",
            " [25 10  3 23  3 15]\n",
            " [ 8 11  0 10 36  7]\n",
            " [ 9 14  1  3  7 44]]\n",
            "Accuracy: 0.4271255060728745\n",
            "Total f1 over classes: 2.403079029422166\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.26      0.28        97\n",
            "           1       0.32      0.55      0.40        85\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.23      0.24      0.23        79\n",
            "           4       0.48      0.32      0.38        72\n",
            "           5       0.34      0.26      0.29        78\n",
            "\n",
            "    accuracy                           0.31       494\n",
            "   macro avg       0.32      0.31      0.31       494\n",
            "weighted avg       0.32      0.31      0.30       494\n",
            "\n",
            "[[25 25 15 14  4 14]\n",
            " [ 8 47  5 14  6  5]\n",
            " [20  9 19 19  3 13]\n",
            " [16 21 12 19  6  5]\n",
            " [ 5 26  9  7 23  2]\n",
            " [ 7 20 15 10  6 20]]\n",
            "Accuracy: 0.3097165991902834\n",
            "Total f1 over classes: 1.8347107194008774\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.30      0.32        97\n",
            "           1       0.44      0.58      0.50        85\n",
            "           2       0.30      0.17      0.22        83\n",
            "           3       0.31      0.32      0.31        79\n",
            "           4       0.49      0.50      0.50        72\n",
            "           5       0.38      0.49      0.43        78\n",
            "\n",
            "    accuracy                           0.39       494\n",
            "   macro avg       0.38      0.39      0.38       494\n",
            "weighted avg       0.38      0.39      0.38       494\n",
            "\n",
            "[[29 23 12 12  5 16]\n",
            " [14 49  2  8  7  5]\n",
            " [14  8 14 19  7 21]\n",
            " [12 10 13 25  7 12]\n",
            " [ 6  8  1 14 36  7]\n",
            " [ 8 13  5  3 11 38]]\n",
            "Accuracy: 0.3866396761133603\n",
            "Total f1 over classes: 2.276037092818215\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.26      0.28        97\n",
            "           1       0.36      0.51      0.42        85\n",
            "           2       0.25      0.22      0.23        83\n",
            "           3       0.25      0.04      0.07        79\n",
            "           4       0.46      0.46      0.46        72\n",
            "           5       0.31      0.54      0.39        78\n",
            "\n",
            "    accuracy                           0.33       494\n",
            "   macro avg       0.32      0.34      0.31       494\n",
            "weighted avg       0.32      0.33      0.31       494\n",
            "\n",
            "[[25 23 11  2  3 33]\n",
            " [13 43  7  3  7 12]\n",
            " [17  7 18  3 11 27]\n",
            " [14 18 25  3  6 13]\n",
            " [ 7 15  7  1 33  9]\n",
            " [ 8 12  4  0 12 42]]\n",
            "Accuracy: 0.3319838056680162\n",
            "Total f1 over classes: 1.8489372423892056\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.32      0.30        97\n",
            "           1       0.50      0.49      0.50        85\n",
            "           2       0.21      0.18      0.20        83\n",
            "           3       0.28      0.23      0.25        79\n",
            "           4       0.56      0.56      0.56        72\n",
            "           5       0.44      0.53      0.48        78\n",
            "\n",
            "    accuracy                           0.38       494\n",
            "   macro avg       0.38      0.38      0.38       494\n",
            "weighted avg       0.37      0.38      0.37       494\n",
            "\n",
            "[[31 15 20 12  4 15]\n",
            " [12 42  7 11  4  9]\n",
            " [23  6 15 15  8 16]\n",
            " [19 11 16 18  8  7]\n",
            " [10  5  8  3 40  6]\n",
            " [15  5  4  5  8 41]]\n",
            "Accuracy: 0.3785425101214575\n",
            "Total f1 over classes: 2.2766847530537717\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.31      0.34        97\n",
            "           1       0.40      0.44      0.42        85\n",
            "           2       0.29      0.12      0.17        83\n",
            "           3       0.34      0.15      0.21        79\n",
            "           4       0.28      0.61      0.39        72\n",
            "           5       0.29      0.36      0.32        78\n",
            "\n",
            "    accuracy                           0.33       494\n",
            "   macro avg       0.33      0.33      0.31       494\n",
            "weighted avg       0.33      0.33      0.31       494\n",
            "\n",
            "[[30 17  6  8 20 16]\n",
            " [11 37  4  3 22  8]\n",
            " [10 11 10  7 24 21]\n",
            " [13 10  8 12 21 15]\n",
            " [ 8  6  3  4 44  7]\n",
            " [ 9 12  3  1 25 28]]\n",
            "Accuracy: 0.3259109311740891\n",
            "Total f1 over classes: 1.843939809739709\n",
            "Doing test analysis with ANN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.52      0.37        33\n",
            "           1       0.48      0.51      0.49        39\n",
            "           2       0.75      0.07      0.12        44\n",
            "           3       0.36      0.36      0.36        33\n",
            "           4       0.53      0.46      0.49        37\n",
            "           5       0.31      0.47      0.38        34\n",
            "\n",
            "    accuracy                           0.39       220\n",
            "   macro avg       0.45      0.40      0.37       220\n",
            "weighted avg       0.47      0.39      0.36       220\n",
            "\n",
            "[[17  7  0  1  2  6]\n",
            " [ 9 20  0  3  3  4]\n",
            " [17  5  3 10  2  7]\n",
            " [ 4  2  1 12  4 10]\n",
            " [ 4  4  0  4 17  8]\n",
            " [ 7  4  0  3  4 16]]\n",
            "Accuracy: 0.38636363636363635\n",
            "Total f1 over classes: 2.2253141091802644\n",
            "Doing test analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.39      0.33        33\n",
            "           1       0.38      0.49      0.43        39\n",
            "           2       0.17      0.16      0.17        44\n",
            "           3       0.25      0.27      0.26        33\n",
            "           4       0.50      0.32      0.39        37\n",
            "           5       0.42      0.29      0.34        34\n",
            "\n",
            "    accuracy                           0.32       220\n",
            "   macro avg       0.33      0.32      0.32       220\n",
            "weighted avg       0.33      0.32      0.32       220\n",
            "\n",
            "[[13  4  6  3  1  6]\n",
            " [ 8 19  3  5  2  2]\n",
            " [11  8  7 11  4  3]\n",
            " [ 7  4  7  9  3  3]\n",
            " [ 3 11  8  3 12  0]\n",
            " [ 4  4  9  5  2 10]]\n",
            "Accuracy: 0.3181818181818182\n",
            "Total f1 over classes: 1.9218866572272386\n",
            "Doing test analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.39      0.39        33\n",
            "           1       0.47      0.49      0.48        39\n",
            "           2       0.32      0.18      0.23        44\n",
            "           3       0.22      0.24      0.23        33\n",
            "           4       0.45      0.46      0.45        37\n",
            "           5       0.33      0.47      0.39        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.37      0.37      0.36       220\n",
            "weighted avg       0.37      0.37      0.36       220\n",
            "\n",
            "[[13  8  2  1  3  6]\n",
            " [ 6 19  1  5  5  3]\n",
            " [ 5  5  8 15  3  8]\n",
            " [ 4  1  7  8  5  8]\n",
            " [ 2  3  3  5 17  7]\n",
            " [ 3  4  4  2  5 16]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.1822974038816287\n",
            "Doing test analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.39      0.37        33\n",
            "           1       0.41      0.46      0.43        39\n",
            "           2       0.43      0.30      0.35        44\n",
            "           3       0.30      0.09      0.14        33\n",
            "           4       0.42      0.46      0.44        37\n",
            "           5       0.27      0.47      0.34        34\n",
            "\n",
            "    accuracy                           0.36       220\n",
            "   macro avg       0.36      0.36      0.35       220\n",
            "weighted avg       0.37      0.36      0.35       220\n",
            "\n",
            "[[13  6  2  0  2 10]\n",
            " [ 3 18  4  4  5  5]\n",
            " [ 7  5 13  3  5 11]\n",
            " [ 3  7  5  3  6  9]\n",
            " [ 4  3  5  0 17  8]\n",
            " [ 7  5  1  0  5 16]]\n",
            "Accuracy: 0.36363636363636365\n",
            "Total f1 over classes: 2.081694209323707\n",
            "Doing test analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37        33\n",
            "           1       0.50      0.44      0.47        39\n",
            "           2       0.35      0.30      0.32        44\n",
            "           3       0.32      0.30      0.31        33\n",
            "           4       0.52      0.38      0.44        37\n",
            "           5       0.40      0.50      0.44        34\n",
            "\n",
            "    accuracy                           0.39       220\n",
            "   macro avg       0.40      0.39      0.39       220\n",
            "weighted avg       0.40      0.39      0.39       220\n",
            "\n",
            "[[15  6  3  0  3  6]\n",
            " [ 9 17  3  6  1  3]\n",
            " [ 9  6 13  7  2  7]\n",
            " [ 4  3  6 10  4  6]\n",
            " [ 4  1  7  7 14  4]\n",
            " [ 7  1  5  1  3 17]]\n",
            "Accuracy: 0.39090909090909093\n",
            "Total f1 over classes: 2.348669890907334\n",
            "Doing test analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.36      0.38        33\n",
            "           1       0.53      0.44      0.48        39\n",
            "           2       0.35      0.14      0.20        44\n",
            "           3       0.31      0.12      0.17        33\n",
            "           4       0.32      0.70      0.44        37\n",
            "           5       0.28      0.38      0.32        34\n",
            "\n",
            "    accuracy                           0.35       220\n",
            "   macro avg       0.36      0.36      0.33       220\n",
            "weighted avg       0.37      0.35      0.33       220\n",
            "\n",
            "[[12  4  0  0  9  8]\n",
            " [ 5 17  2  2 11  2]\n",
            " [ 5  6  6  5 12 10]\n",
            " [ 2  1  7  4  9 10]\n",
            " [ 2  2  1  2 26  4]\n",
            " [ 4  2  1  0 14 13]]\n",
            "Accuracy: 0.35454545454545455\n",
            "Total f1 over classes: 1.9921255957653536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing pca"
      ],
      "metadata": {
        "id": "vCRmEEAJKrE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale data before applying PCA\n",
        "#scaling=StandardScaler()\n",
        " \n",
        "# Use fit and transform method\n",
        "#scaling.fit(data)\n",
        "#Scaled_data=scaling.transform(data)\n",
        "\n",
        "# Set the n_components=3\n",
        "principal=PCA(n_components=11)\n",
        "principal.fit(data)\n",
        "pca_data=principal.transform(data)\n",
        "\n",
        "# summarize components\n",
        "print(\"Explained Variance: %s\" % principal.explained_variance_ratio_)\n",
        "print(\"Shape of Transformed data: \",pca_data.shape)\n",
        "\n",
        "pca_df = pd.DataFrame(pca_data, columns = [\"pc\"+str(pci) for pci in range(11)], index = labels.index)\n",
        "print(\"PCA data frame has row,columns:\", pca_df.shape)\n",
        "perform_experiment('G-1', 1,pca_df, labels,filenames[1])\n",
        "perform_experiment('G-1', 2,pca_df, labels,filenames[1])\n",
        "perform_experiment('G-1', 3,pca_df, labels,filenames[1])\n",
        "perform_experiment('G-1', 4,pca_df, labels,filenames[1])\n",
        "perform_experiment('G-1', 5,pca_df, labels,filenames[1])"
      ],
      "metadata": {
        "id": "mr18AR9EKo2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed43d63-09ed-4eeb-db50-d2a2fb0537b0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance: [8.175e-01 7.990e-02 6.077e-02 1.751e-02 1.216e-02 8.528e-03 1.988e-03\n",
            " 1.345e-03 2.146e-04 2.429e-05 1.267e-05]\n",
            "Shape of Transformed data:  (2199, 11)\n",
            "PCA data frame has row,columns: (2199, 11)\n",
            "No. of training examples: 1979\n",
            "No. of testing examples: 220\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.09      0.15        96\n",
            "           1       0.44      0.56      0.49        85\n",
            "           2       0.22      0.29      0.25        84\n",
            "           3       0.31      0.18      0.23        79\n",
            "           4       0.43      0.61      0.50        72\n",
            "           5       0.41      0.52      0.46        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.36      0.38      0.35       495\n",
            "weighted avg       0.36      0.36      0.34       495\n",
            "\n",
            "[[ 9 21 32  6  8 20]\n",
            " [ 3 48 21  2  6  5]\n",
            " [ 6  9 24 11 16 18]\n",
            " [ 4 12 18 14 21 10]\n",
            " [ 0 10  3  8 44  7]\n",
            " [ 4 10 12  4  8 41]]\n",
            "Accuracy: 0.36363636363636365\n",
            "Total f1 over classes: 2.0714905063522226\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.38        96\n",
            "           1       0.29      0.49      0.37        85\n",
            "           2       0.25      0.17      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.39      0.28      0.33        72\n",
            "           5       0.31      0.29      0.30        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.32      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[37 19 13 10  4 13]\n",
            " [16 42  8  9  3  7]\n",
            " [11 16 14 18  6 19]\n",
            " [10 23  9 22 10  5]\n",
            " [ 5 29  4  6 20  8]\n",
            " [18 15  8  7  8 23]]\n",
            "Accuracy: 0.3191919191919192\n",
            "Total f1 over classes: 1.8655271954040469\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.35      0.37        96\n",
            "           1       0.45      0.49      0.47        85\n",
            "           2       0.37      0.23      0.28        84\n",
            "           3       0.28      0.22      0.24        79\n",
            "           4       0.45      0.57      0.50        72\n",
            "           5       0.31      0.43      0.36        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.37      0.38      0.37       495\n",
            "weighted avg       0.37      0.38      0.37       495\n",
            "\n",
            "[[34 18  4  7 11 22]\n",
            " [15 42  5  6  7 10]\n",
            " [ 7  7 19 16  9 26]\n",
            " [14  9 13 17 15 11]\n",
            " [ 2  9  2 11 41  7]\n",
            " [14  9  9  4  9 34]]\n",
            "Accuracy: 0.37777777777777777\n",
            "Total f1 over classes: 2.2249573839945183\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.27      0.33        96\n",
            "           1       0.38      0.36      0.37        85\n",
            "           2       0.24      0.37      0.29        84\n",
            "           3       0.30      0.28      0.29        79\n",
            "           4       0.44      0.56      0.49        72\n",
            "           5       0.42      0.29      0.34        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.36      0.35      0.35       495\n",
            "weighted avg       0.36      0.35      0.35       495\n",
            "\n",
            "[[26 18 30 10  5  7]\n",
            " [14 31 17  8 11  4]\n",
            " [ 5  9 31 19 10 10]\n",
            " [ 6  7 22 22 18  4]\n",
            " [ 8  7  5  5 40  7]\n",
            " [ 5  9 26 10  6 23]]\n",
            "Accuracy: 0.34949494949494947\n",
            "Total f1 over classes: 2.111558510856655\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.44      0.43        96\n",
            "           1       0.43      0.47      0.45        85\n",
            "           2       0.32      0.24      0.27        84\n",
            "           3       0.36      0.30      0.33        79\n",
            "           4       0.52      0.58      0.55        72\n",
            "           5       0.37      0.43      0.40        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.40      0.41      0.40       495\n",
            "weighted avg       0.40      0.41      0.40       495\n",
            "\n",
            "[[42 17 12  8  4 13]\n",
            " [19 40  6  9  3  8]\n",
            " [ 8 11 20 15 11 19]\n",
            " [11 11 13 24 12  8]\n",
            " [ 7  6  0  7 42 10]\n",
            " [13  7 12  4  9 34]]\n",
            "Accuracy: 0.4080808080808081\n",
            "Total f1 over classes: 2.428105223083046\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.74      0.45        96\n",
            "           1       0.49      0.44      0.46        85\n",
            "           2       0.36      0.15      0.22        84\n",
            "           3       0.20      0.05      0.08        79\n",
            "           4       0.57      0.57      0.57        72\n",
            "           5       0.45      0.42      0.43        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.40      0.39      0.37       495\n",
            "weighted avg       0.40      0.40      0.37       495\n",
            "\n",
            "[[71 12  2  1  1  9]\n",
            " [33 37  4  2  3  6]\n",
            " [37  7 13  5  7 15]\n",
            " [29 11 14  4 15  6]\n",
            " [14  6  2  4 41  5]\n",
            " [34  2  1  4  5 33]]\n",
            "Accuracy: 0.402020202020202\n",
            "Total f1 over classes: 2.2130210403018573\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.42      0.37        97\n",
            "           1       0.45      0.53      0.49        85\n",
            "           2       0.39      0.14      0.21        83\n",
            "           3       0.37      0.28      0.32        79\n",
            "           4       0.44      0.49      0.46        72\n",
            "           5       0.41      0.52      0.46        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.40      0.40      0.38       495\n",
            "weighted avg       0.40      0.40      0.38       495\n",
            "\n",
            "[[41 18  5  6  7 20]\n",
            " [19 45  1  4  6 10]\n",
            " [27 10 12 14 11  9]\n",
            " [15 14  6 22 12 10]\n",
            " [ 8  9  4  7 35  9]\n",
            " [17  3  3  7  8 41]]\n",
            "Accuracy: 0.39595959595959596\n",
            "Total f1 over classes: 2.3065252579772073\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.28      0.51      0.36        85\n",
            "           2       0.21      0.16      0.18        83\n",
            "           3       0.30      0.24      0.27        79\n",
            "           4       0.42      0.31      0.35        72\n",
            "           5       0.38      0.29      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.31      0.30       495\n",
            "weighted avg       0.32      0.31      0.31       495\n",
            "\n",
            "[[34 23 17  8  6  9]\n",
            " [17 43  5  3  4 13]\n",
            " [15 27 13 17  8  3]\n",
            " [13 26  7 19  8  6]\n",
            " [ 6 22  8  7 22  7]\n",
            " [16 15 11  9  5 23]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.82901343439319\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.43      0.40        97\n",
            "           1       0.47      0.59      0.52        85\n",
            "           2       0.33      0.17      0.22        83\n",
            "           3       0.31      0.22      0.26        79\n",
            "           4       0.39      0.53      0.45        72\n",
            "           5       0.41      0.42      0.41        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.38       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[42 20  5  5 11 14]\n",
            " [13 50  1  5  8  8]\n",
            " [17 10 14 18 15  9]\n",
            " [16 13 11 17 16  6]\n",
            " [ 8  7  3  5 38 11]\n",
            " [17  6  9  4 10 33]]\n",
            "Accuracy: 0.39191919191919194\n",
            "Total f1 over classes: 2.260980352920079\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.29      0.31        97\n",
            "           1       0.41      0.22      0.29        85\n",
            "           2       0.24      0.08      0.12        83\n",
            "           3       0.28      0.13      0.17        79\n",
            "           4       0.30      0.60      0.40        72\n",
            "           5       0.34      0.67      0.45        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.33      0.29       495\n",
            "weighted avg       0.32      0.32      0.29       495\n",
            "\n",
            "[[28  7  5  3 17 37]\n",
            " [15 19  0  4 30 17]\n",
            " [18  6  7 10 20 22]\n",
            " [ 8  8 12 10 25 16]\n",
            " [10  3  2  5 43  9]\n",
            " [ 7  3  3  4  9 53]]\n",
            "Accuracy: 0.32323232323232326\n",
            "Total f1 over classes: 1.7480840787836167\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.44      0.41        97\n",
            "           1       0.43      0.52      0.47        85\n",
            "           2       0.28      0.18      0.22        83\n",
            "           3       0.36      0.25      0.30        79\n",
            "           4       0.48      0.50      0.49        72\n",
            "           5       0.43      0.52      0.47        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.39      0.40      0.39       495\n",
            "weighted avg       0.39      0.40      0.39       495\n",
            "\n",
            "[[43 16  8  6  7 17]\n",
            " [16 44  4  4  3 14]\n",
            " [19 15 15 16 12  6]\n",
            " [16 16 11 20  9  7]\n",
            " [ 7  8  4  6 36 11]\n",
            " [13  3 11  3  8 41]]\n",
            "Accuracy: 0.402020202020202\n",
            "Total f1 over classes: 2.3556342186022037\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.70      0.45        97\n",
            "           1       0.61      0.41      0.49        85\n",
            "           2       0.37      0.08      0.14        83\n",
            "           3       0.42      0.18      0.25        79\n",
            "           4       0.44      0.62      0.51        72\n",
            "           5       0.42      0.42      0.42        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.43      0.40      0.38       495\n",
            "weighted avg       0.43      0.41      0.38       495\n",
            "\n",
            "[[68  6  2  1  8 12]\n",
            " [28 35  1  2 11  8]\n",
            " [35  6  7 12 14  9]\n",
            " [33  8  5 14 12  7]\n",
            " [14  1  0  3 45  9]\n",
            " [27  1  4  1 13 33]]\n",
            "Accuracy: 0.4080808080808081\n",
            "Total f1 over classes: 2.265211654158282\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.31      0.35        97\n",
            "           1       0.49      0.63      0.55        86\n",
            "           2       0.29      0.13      0.18        83\n",
            "           3       0.36      0.37      0.37        78\n",
            "           4       0.44      0.69      0.54        72\n",
            "           5       0.46      0.46      0.46        79\n",
            "\n",
            "    accuracy                           0.42       495\n",
            "   macro avg       0.41      0.43      0.41       495\n",
            "weighted avg       0.41      0.42      0.40       495\n",
            "\n",
            "[[30 19  8 15 12 13]\n",
            " [16 54  0  3 11  2]\n",
            " [ 9 12 11 21 14 16]\n",
            " [ 5  8 12 29 15  9]\n",
            " [ 4  8  3  4 50  3]\n",
            " [ 9 10  4  8 12 36]]\n",
            "Accuracy: 0.42424242424242425\n",
            "Total f1 over classes: 2.4434019272713097\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.31      0.33        97\n",
            "           1       0.34      0.63      0.44        86\n",
            "           2       0.20      0.14      0.17        83\n",
            "           3       0.19      0.19      0.19        78\n",
            "           4       0.44      0.33      0.38        72\n",
            "           5       0.39      0.28      0.32        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.31      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[30 22  8 23  5  9]\n",
            " [ 9 54  5  9  6  3]\n",
            " [16 25 12 14  7  9]\n",
            " [11 22 15 15  5 10]\n",
            " [12 20  4  8 24  4]\n",
            " [ 7 16 16 10  8 22]]\n",
            "Accuracy: 0.31717171717171716\n",
            "Total f1 over classes: 1.830883794251098\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.40      0.42        97\n",
            "           1       0.57      0.65      0.61        86\n",
            "           2       0.27      0.20      0.23        83\n",
            "           3       0.41      0.26      0.31        78\n",
            "           4       0.47      0.67      0.55        72\n",
            "           5       0.38      0.44      0.41        79\n",
            "\n",
            "    accuracy                           0.43       495\n",
            "   macro avg       0.42      0.44      0.42       495\n",
            "weighted avg       0.42      0.43      0.42       495\n",
            "\n",
            "[[39 15 12  8 10 13]\n",
            " [12 56  3  1 10  4]\n",
            " [12  8 17 14 11 21]\n",
            " [ 7  4 19 20 13 15]\n",
            " [ 6  8  3  3 48  4]\n",
            " [14  7 10  3 10 35]]\n",
            "Accuracy: 0.43434343434343436\n",
            "Total f1 over classes: 2.53314196164445\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.23      0.26        97\n",
            "           1       0.34      0.60      0.43        86\n",
            "           2       0.22      0.34      0.27        83\n",
            "           3       0.26      0.06      0.10        78\n",
            "           4       0.54      0.46      0.50        72\n",
            "           5       0.47      0.35      0.41        79\n",
            "\n",
            "    accuracy                           0.34       495\n",
            "   macro avg       0.35      0.34      0.33       495\n",
            "weighted avg       0.35      0.34      0.32       495\n",
            "\n",
            "[[22 28 30  5  3  9]\n",
            " [11 52  7  6  8  2]\n",
            " [20 21 28  1  4  9]\n",
            " [10 12 34  5  7 10]\n",
            " [ 1 25 11  1 33  1]\n",
            " [11 17 16  1  6 28]]\n",
            "Accuracy: 0.3393939393939394\n",
            "Total f1 over classes: 1.9604222933881625\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.31      0.31        97\n",
            "           1       0.50      0.58      0.54        86\n",
            "           2       0.21      0.19      0.20        83\n",
            "           3       0.33      0.24      0.28        78\n",
            "           4       0.54      0.62      0.58        72\n",
            "           5       0.41      0.41      0.41        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.38       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[30 19 19 13  4 12]\n",
            " [14 50  5  3 11  3]\n",
            " [21  8 16 14  9 15]\n",
            " [14  8 19 19  6 12]\n",
            " [ 6  6  6  4 45  5]\n",
            " [14  9 10  5  9 32]]\n",
            "Accuracy: 0.3878787878787879\n",
            "Total f1 over classes: 2.3076866359195627\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.67      0.43        97\n",
            "           1       0.57      0.49      0.53        86\n",
            "           2       0.29      0.13      0.18        83\n",
            "           3       0.33      0.17      0.22        78\n",
            "           4       0.49      0.42      0.45        72\n",
            "           5       0.38      0.37      0.37        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.39      0.37      0.36       495\n",
            "weighted avg       0.39      0.38      0.37       495\n",
            "\n",
            "[[65 11  2  6  4  9]\n",
            " [29 42  3  2  6  4]\n",
            " [34  5 11  8  7 18]\n",
            " [26  4 15 13  9 11]\n",
            " [19  9  2  6 30  6]\n",
            " [32  3  5  5  5 29]]\n",
            "Accuracy: 0.3838383838383838\n",
            "Total f1 over classes: 2.180543432371714\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.42      0.41        97\n",
            "           1       0.41      0.55      0.47        85\n",
            "           2       0.33      0.18      0.23        83\n",
            "           3       0.34      0.28      0.31        79\n",
            "           4       0.49      0.57      0.53        72\n",
            "           5       0.45      0.47      0.46        78\n",
            "\n",
            "    accuracy                           0.41       494\n",
            "   macro avg       0.40      0.41      0.40       494\n",
            "weighted avg       0.40      0.41      0.40       494\n",
            "\n",
            "[[41 26 12  3  8  7]\n",
            " [14 47  2  7  7  8]\n",
            " [19 12 15 16 10 11]\n",
            " [17  9 11 22  8 12]\n",
            " [ 5  7  2 10 41  7]\n",
            " [ 8 14  3  6 10 37]]\n",
            "Accuracy: 0.4109311740890688\n",
            "Total f1 over classes: 2.4081685323383084\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.26      0.28        97\n",
            "           1       0.32      0.55      0.40        85\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.23      0.24      0.23        79\n",
            "           4       0.48      0.32      0.38        72\n",
            "           5       0.34      0.26      0.29        78\n",
            "\n",
            "    accuracy                           0.31       494\n",
            "   macro avg       0.32      0.31      0.31       494\n",
            "weighted avg       0.32      0.31      0.30       494\n",
            "\n",
            "[[25 25 15 14  4 14]\n",
            " [ 8 47  5 14  6  5]\n",
            " [20  9 19 19  3 13]\n",
            " [16 21 12 19  6  5]\n",
            " [ 5 26  9  7 23  2]\n",
            " [ 7 20 15 10  6 20]]\n",
            "Accuracy: 0.3097165991902834\n",
            "Total f1 over classes: 1.8347107194008774\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.37      0.38        97\n",
            "           1       0.45      0.59      0.51        85\n",
            "           2       0.35      0.22      0.27        83\n",
            "           3       0.36      0.30      0.33        79\n",
            "           4       0.48      0.56      0.51        72\n",
            "           5       0.43      0.49      0.46        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.41      0.42      0.41       494\n",
            "weighted avg       0.41      0.42      0.41       494\n",
            "\n",
            "[[36 23 11  9  6 12]\n",
            " [12 50  2  8  7  6]\n",
            " [15  8 18 16 11 15]\n",
            " [13 10 12 24  8 12]\n",
            " [ 5  8  5  9 40  5]\n",
            " [10 13  4  1 12 38]]\n",
            "Accuracy: 0.41700404858299595\n",
            "Total f1 over classes: 2.4566785646782803\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.28      0.30        97\n",
            "           1       0.42      0.41      0.41        85\n",
            "           2       0.12      0.04      0.06        83\n",
            "           3       0.33      0.48      0.39        79\n",
            "           4       0.41      0.49      0.45        72\n",
            "           5       0.36      0.47      0.41        78\n",
            "\n",
            "    accuracy                           0.35       494\n",
            "   macro avg       0.33      0.36      0.34       494\n",
            "weighted avg       0.33      0.35      0.33       494\n",
            "\n",
            "[[27 17  9 15 13 16]\n",
            " [16 35  2 11 12  9]\n",
            " [ 9  9  3 30  6 26]\n",
            " [15  5  2 38 10  9]\n",
            " [ 4 11  6 10 35  6]\n",
            " [10  7  4 11  9 37]]\n",
            "Accuracy: 0.354251012145749\n",
            "Total f1 over classes: 2.019070070445017\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.35      0.36        97\n",
            "           1       0.40      0.44      0.42        85\n",
            "           2       0.35      0.29      0.32        83\n",
            "           3       0.31      0.29      0.30        79\n",
            "           4       0.58      0.62      0.60        72\n",
            "           5       0.45      0.51      0.48        78\n",
            "\n",
            "    accuracy                           0.41       494\n",
            "   macro avg       0.41      0.42      0.41       494\n",
            "weighted avg       0.41      0.41      0.41       494\n",
            "\n",
            "[[34 22 14 13  2 12]\n",
            " [11 37 10  8  6 13]\n",
            " [15  8 24 17  6 13]\n",
            " [17 10 12 23 10  7]\n",
            " [ 6  7  3  7 45  4]\n",
            " [10  9  5  6  8 40]]\n",
            "Accuracy: 0.4109311740890688\n",
            "Total f1 over classes: 2.475228225199234\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.64      0.44        97\n",
            "           1       0.49      0.48      0.49        85\n",
            "           2       0.42      0.17      0.24        83\n",
            "           3       0.35      0.22      0.27        79\n",
            "           4       0.53      0.51      0.52        72\n",
            "           5       0.50      0.46      0.48        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.44      0.41      0.41       494\n",
            "weighted avg       0.43      0.42      0.40       494\n",
            "\n",
            "[[62 16  5  6  3  5]\n",
            " [25 41  0  7  7  5]\n",
            " [34  8 14  8  4 15]\n",
            " [26  7  9 17 11  9]\n",
            " [16  7  3  7 37  2]\n",
            " [25  4  2  3  8 36]]\n",
            "Accuracy: 0.4190283400809717\n",
            "Total f1 over classes: 2.4334055637347625\n",
            "Doing test analysis with ANN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.45      0.38        33\n",
            "           1       0.59      0.56      0.58        39\n",
            "           2       0.45      0.20      0.28        44\n",
            "           3       0.26      0.27      0.27        33\n",
            "           4       0.46      0.57      0.51        37\n",
            "           5       0.34      0.38      0.36        34\n",
            "\n",
            "    accuracy                           0.40       220\n",
            "   macro avg       0.41      0.41      0.40       220\n",
            "weighted avg       0.41      0.40      0.40       220\n",
            "\n",
            "[[15  7  0  1  6  4]\n",
            " [ 6 22  0  4  5  2]\n",
            " [ 7  5  9 11  5  7]\n",
            " [ 5  1  6  9  5  7]\n",
            " [ 4  0  3  4 21  5]\n",
            " [ 8  2  2  5  4 13]]\n",
            "Accuracy: 0.40454545454545454\n",
            "Total f1 over classes: 2.3806046769510014\n",
            "Doing test analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.39      0.33        33\n",
            "           1       0.38      0.49      0.43        39\n",
            "           2       0.17      0.16      0.17        44\n",
            "           3       0.25      0.27      0.26        33\n",
            "           4       0.50      0.32      0.39        37\n",
            "           5       0.42      0.29      0.34        34\n",
            "\n",
            "    accuracy                           0.32       220\n",
            "   macro avg       0.33      0.32      0.32       220\n",
            "weighted avg       0.33      0.32      0.32       220\n",
            "\n",
            "[[13  4  6  3  1  6]\n",
            " [ 8 19  3  5  2  2]\n",
            " [11  8  7 11  4  3]\n",
            " [ 7  4  7  9  3  3]\n",
            " [ 3 11  8  3 12  0]\n",
            " [ 4  4  9  5  2 10]]\n",
            "Accuracy: 0.3181818181818182\n",
            "Total f1 over classes: 1.9218866572272386\n",
            "Doing test analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.48      0.43        33\n",
            "           1       0.57      0.59      0.58        39\n",
            "           2       0.42      0.23      0.29        44\n",
            "           3       0.28      0.27      0.28        33\n",
            "           4       0.44      0.51      0.48        37\n",
            "           5       0.36      0.41      0.38        34\n",
            "\n",
            "    accuracy                           0.41       220\n",
            "   macro avg       0.41      0.42      0.41       220\n",
            "weighted avg       0.41      0.41      0.41       220\n",
            "\n",
            "[[16  6  2  0  4  5]\n",
            " [ 5 23  0  6  4  1]\n",
            " [ 6  5 10 12  6  5]\n",
            " [ 5  1  7  9  4  7]\n",
            " [ 3  1  3  4 19  7]\n",
            " [ 7  4  2  1  6 14]]\n",
            "Accuracy: 0.41363636363636364\n",
            "Total f1 over classes: 2.438547515496842\n",
            "Doing test analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.30      0.28        33\n",
            "           1       0.33      0.23      0.27        39\n",
            "           2       0.17      0.05      0.07        44\n",
            "           3       0.24      0.39      0.30        33\n",
            "           4       0.41      0.38      0.39        37\n",
            "           5       0.26      0.41      0.32        34\n",
            "\n",
            "    accuracy                           0.28       220\n",
            "   macro avg       0.28      0.29      0.27       220\n",
            "weighted avg       0.28      0.28      0.26       220\n",
            "\n",
            "[[10  6  0  4  3 10]\n",
            " [10  9  5  9  5  1]\n",
            " [ 4  4  2 15  7 12]\n",
            " [ 4  3  1 13  2 10]\n",
            " [ 2  3  2  9 14  7]\n",
            " [ 9  2  2  4  3 14]]\n",
            "Accuracy: 0.2818181818181818\n",
            "Total f1 over classes: 1.6333322120111824\n",
            "Doing test analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.45      0.39        33\n",
            "           1       0.56      0.49      0.52        39\n",
            "           2       0.43      0.27      0.33        44\n",
            "           3       0.24      0.27      0.25        33\n",
            "           4       0.47      0.43      0.45        37\n",
            "           5       0.31      0.38      0.34        34\n",
            "\n",
            "    accuracy                           0.38       220\n",
            "   macro avg       0.39      0.38      0.38       220\n",
            "weighted avg       0.40      0.38      0.38       220\n",
            "\n",
            "[[15  5  2  2  3  6]\n",
            " [ 6 19  2  7  2  3]\n",
            " [ 8  5 12 11  2  6]\n",
            " [ 5  1  5  9  6  7]\n",
            " [ 4  2  5  3 16  7]\n",
            " [ 6  2  2  6  5 13]]\n",
            "Accuracy: 0.38181818181818183\n",
            "Total f1 over classes: 2.2898222834197735\n",
            "Doing test analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.82      0.43        33\n",
            "           1       0.57      0.44      0.49        39\n",
            "           2       0.33      0.14      0.19        44\n",
            "           3       0.26      0.18      0.21        33\n",
            "           4       0.59      0.43      0.50        37\n",
            "           5       0.32      0.26      0.29        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.39      0.38      0.35       220\n",
            "weighted avg       0.40      0.37      0.35       220\n",
            "\n",
            "[[27  3  0  0  0  3]\n",
            " [ 9 17  1  6  4  2]\n",
            " [20  3  6  7  2  6]\n",
            " [14  2  7  6  2  2]\n",
            " [ 8  3  1  3 16  6]\n",
            " [16  2  3  1  3  9]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.1161071556097566\n",
            "No. of training examples: 1979\n",
            "No. of testing examples: 220\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.21      0.31        96\n",
            "           1       0.37      0.55      0.44        85\n",
            "           2       0.20      0.20      0.20        84\n",
            "           3       0.38      0.27      0.31        79\n",
            "           4       0.39      0.61      0.48        72\n",
            "           5       0.40      0.42      0.41        79\n",
            "\n",
            "    accuracy                           0.37       495\n",
            "   macro avg       0.39      0.38      0.36       495\n",
            "weighted avg       0.39      0.37      0.35       495\n",
            "\n",
            "[[20 25 19 10  9 13]\n",
            " [ 7 47 12  4  9  6]\n",
            " [ 0 13 17 14 23 17]\n",
            " [ 1 16 16 21 16  9]\n",
            " [ 1 15  4  4 44  4]\n",
            " [ 5 12 15  2 12 33]]\n",
            "Accuracy: 0.36767676767676766\n",
            "Total f1 over classes: 2.1516460757495044\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.38        96\n",
            "           1       0.29      0.49      0.37        85\n",
            "           2       0.25      0.17      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.39      0.28      0.33        72\n",
            "           5       0.31      0.29      0.30        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.32      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[37 19 13 10  4 13]\n",
            " [16 42  8  9  3  7]\n",
            " [11 16 14 18  6 19]\n",
            " [10 23  9 22 10  5]\n",
            " [ 5 29  4  6 20  8]\n",
            " [18 15  8  7  8 23]]\n",
            "Accuracy: 0.3191919191919192\n",
            "Total f1 over classes: 1.8655271954040469\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.35      0.37        96\n",
            "           1       0.45      0.49      0.47        85\n",
            "           2       0.37      0.23      0.28        84\n",
            "           3       0.28      0.22      0.24        79\n",
            "           4       0.45      0.57      0.50        72\n",
            "           5       0.31      0.43      0.36        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.37      0.38      0.37       495\n",
            "weighted avg       0.37      0.38      0.37       495\n",
            "\n",
            "[[34 18  4  7 11 22]\n",
            " [15 42  5  6  7 10]\n",
            " [ 7  7 19 16  9 26]\n",
            " [14  9 13 17 15 11]\n",
            " [ 2  9  2 11 41  7]\n",
            " [14  9  9  4  9 34]]\n",
            "Accuracy: 0.37777777777777777\n",
            "Total f1 over classes: 2.2249573839945183\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.27      0.33        96\n",
            "           1       0.38      0.36      0.37        85\n",
            "           2       0.24      0.37      0.29        84\n",
            "           3       0.30      0.28      0.29        79\n",
            "           4       0.44      0.56      0.49        72\n",
            "           5       0.42      0.29      0.34        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.36      0.35      0.35       495\n",
            "weighted avg       0.36      0.35      0.35       495\n",
            "\n",
            "[[26 18 30 10  5  7]\n",
            " [14 31 17  8 11  4]\n",
            " [ 5  9 31 19 10 10]\n",
            " [ 6  7 22 22 18  4]\n",
            " [ 8  7  5  5 40  7]\n",
            " [ 5  9 26 10  6 23]]\n",
            "Accuracy: 0.34949494949494947\n",
            "Total f1 over classes: 2.111558510856655\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.42      0.43        96\n",
            "           1       0.44      0.51      0.47        85\n",
            "           2       0.30      0.23      0.26        84\n",
            "           3       0.33      0.32      0.32        79\n",
            "           4       0.53      0.53      0.53        72\n",
            "           5       0.40      0.48      0.44        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.41      0.41      0.41       495\n",
            "weighted avg       0.40      0.41      0.41       495\n",
            "\n",
            "[[40 19 15  9  2 11]\n",
            " [16 43  5 10  3  8]\n",
            " [ 8  9 19 18 11 19]\n",
            " [10 11 13 25 11  9]\n",
            " [ 8  8  1  7 38 10]\n",
            " [10  8 10  6  7 38]]\n",
            "Accuracy: 0.4101010101010101\n",
            "Total f1 over classes: 2.443215383093923\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.74      0.45        96\n",
            "           1       0.49      0.44      0.46        85\n",
            "           2       0.36      0.15      0.22        84\n",
            "           3       0.20      0.05      0.08        79\n",
            "           4       0.57      0.57      0.57        72\n",
            "           5       0.45      0.42      0.43        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.40      0.39      0.37       495\n",
            "weighted avg       0.40      0.40      0.37       495\n",
            "\n",
            "[[71 12  2  1  1  9]\n",
            " [33 37  4  2  3  6]\n",
            " [37  7 13  5  7 15]\n",
            " [29 11 14  4 15  6]\n",
            " [14  6  2  4 41  5]\n",
            " [34  2  1  4  5 33]]\n",
            "Accuracy: 0.402020202020202\n",
            "Total f1 over classes: 2.2130210403018573\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.41      0.35        97\n",
            "           1       0.45      0.58      0.50        85\n",
            "           2       0.33      0.11      0.16        83\n",
            "           3       0.36      0.20      0.26        79\n",
            "           4       0.40      0.58      0.47        72\n",
            "           5       0.43      0.42      0.42        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.38      0.38      0.36       495\n",
            "weighted avg       0.38      0.38      0.36       495\n",
            "\n",
            "[[40 25  5  3 10 14]\n",
            " [18 49  1  2  7  8]\n",
            " [27  7  9 16 15  9]\n",
            " [20 12  6 16 17  8]\n",
            " [10 10  2  3 42  5]\n",
            " [17  7  4  4 14 33]]\n",
            "Accuracy: 0.38181818181818183\n",
            "Total f1 over classes: 2.173361240255785\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.28      0.51      0.36        85\n",
            "           2       0.21      0.16      0.18        83\n",
            "           3       0.30      0.24      0.27        79\n",
            "           4       0.42      0.31      0.35        72\n",
            "           5       0.38      0.29      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.31      0.30       495\n",
            "weighted avg       0.32      0.31      0.31       495\n",
            "\n",
            "[[34 23 17  8  6  9]\n",
            " [17 43  5  3  4 13]\n",
            " [15 27 13 17  8  3]\n",
            " [13 26  7 19  8  6]\n",
            " [ 6 22  8  7 22  7]\n",
            " [16 15 11  9  5 23]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.82901343439319\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.43      0.40        97\n",
            "           1       0.47      0.59      0.52        85\n",
            "           2       0.33      0.17      0.22        83\n",
            "           3       0.31      0.22      0.26        79\n",
            "           4       0.39      0.53      0.45        72\n",
            "           5       0.41      0.42      0.41        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.38       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[42 20  5  5 11 14]\n",
            " [13 50  1  5  8  8]\n",
            " [17 10 14 18 15  9]\n",
            " [16 13 11 17 16  6]\n",
            " [ 8  7  3  5 38 11]\n",
            " [17  6  9  4 10 33]]\n",
            "Accuracy: 0.39191919191919194\n",
            "Total f1 over classes: 2.260980352920079\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.29      0.31        97\n",
            "           1       0.41      0.22      0.29        85\n",
            "           2       0.24      0.08      0.12        83\n",
            "           3       0.28      0.13      0.17        79\n",
            "           4       0.30      0.60      0.40        72\n",
            "           5       0.34      0.67      0.45        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.33      0.29       495\n",
            "weighted avg       0.32      0.32      0.29       495\n",
            "\n",
            "[[28  7  5  3 17 37]\n",
            " [15 19  0  4 30 17]\n",
            " [18  6  7 10 20 22]\n",
            " [ 8  8 12 10 25 16]\n",
            " [10  3  2  5 43  9]\n",
            " [ 7  3  3  4  9 53]]\n",
            "Accuracy: 0.32323232323232326\n",
            "Total f1 over classes: 1.7480840787836167\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.45      0.41        97\n",
            "           1       0.42      0.49      0.46        85\n",
            "           2       0.28      0.17      0.21        83\n",
            "           3       0.33      0.23      0.27        79\n",
            "           4       0.52      0.51      0.52        72\n",
            "           5       0.40      0.52      0.45        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.39      0.40      0.39       495\n",
            "weighted avg       0.39      0.40      0.38       495\n",
            "\n",
            "[[44 15  5  9  6 18]\n",
            " [18 42  3  5  2 15]\n",
            " [20 14 14 15 10 10]\n",
            " [17 16 12 18  9  7]\n",
            " [ 7  8  4  5 37 11]\n",
            " [12  4 12  3  7 41]]\n",
            "Accuracy: 0.39595959595959596\n",
            "Total f1 over classes: 2.315528288434881\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.70      0.45        97\n",
            "           1       0.61      0.41      0.49        85\n",
            "           2       0.37      0.08      0.14        83\n",
            "           3       0.42      0.18      0.25        79\n",
            "           4       0.44      0.62      0.51        72\n",
            "           5       0.42      0.42      0.42        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.43      0.40      0.38       495\n",
            "weighted avg       0.43      0.41      0.38       495\n",
            "\n",
            "[[68  6  2  1  8 12]\n",
            " [28 35  1  2 11  8]\n",
            " [35  6  7 12 14  9]\n",
            " [33  8  5 14 12  7]\n",
            " [14  1  0  3 45  9]\n",
            " [27  1  4  1 13 33]]\n",
            "Accuracy: 0.4080808080808081\n",
            "Total f1 over classes: 2.265211654158282\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.38      0.40        97\n",
            "           1       0.52      0.64      0.57        86\n",
            "           2       0.26      0.08      0.13        83\n",
            "           3       0.32      0.32      0.32        78\n",
            "           4       0.46      0.76      0.58        72\n",
            "           5       0.42      0.42      0.42        79\n",
            "\n",
            "    accuracy                           0.43       495\n",
            "   macro avg       0.40      0.43      0.40       495\n",
            "weighted avg       0.40      0.43      0.40       495\n",
            "\n",
            "[[37 16  2 15 14 13]\n",
            " [13 55  2  3 10  3]\n",
            " [11 12  7 23 14 16]\n",
            " [ 9  9 10 25 12 13]\n",
            " [ 2  8  3  3 55  1]\n",
            " [15  6  3  8 14 33]]\n",
            "Accuracy: 0.42828282828282827\n",
            "Total f1 over classes: 2.4185817014979962\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.31      0.33        97\n",
            "           1       0.34      0.63      0.44        86\n",
            "           2       0.20      0.14      0.17        83\n",
            "           3       0.19      0.19      0.19        78\n",
            "           4       0.44      0.33      0.38        72\n",
            "           5       0.39      0.28      0.32        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.31      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[30 22  8 23  5  9]\n",
            " [ 9 54  5  9  6  3]\n",
            " [16 25 12 14  7  9]\n",
            " [11 22 15 15  5 10]\n",
            " [12 20  4  8 24  4]\n",
            " [ 7 16 16 10  8 22]]\n",
            "Accuracy: 0.31717171717171716\n",
            "Total f1 over classes: 1.830883794251098\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.40      0.42        97\n",
            "           1       0.57      0.65      0.61        86\n",
            "           2       0.27      0.20      0.23        83\n",
            "           3       0.41      0.26      0.31        78\n",
            "           4       0.47      0.67      0.55        72\n",
            "           5       0.38      0.44      0.41        79\n",
            "\n",
            "    accuracy                           0.43       495\n",
            "   macro avg       0.42      0.44      0.42       495\n",
            "weighted avg       0.42      0.43      0.42       495\n",
            "\n",
            "[[39 15 12  8 10 13]\n",
            " [12 56  3  1 10  4]\n",
            " [12  8 17 14 11 21]\n",
            " [ 7  4 19 20 13 15]\n",
            " [ 6  8  3  3 48  4]\n",
            " [14  7 10  3 10 35]]\n",
            "Accuracy: 0.43434343434343436\n",
            "Total f1 over classes: 2.53314196164445\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.23      0.26        97\n",
            "           1       0.34      0.60      0.43        86\n",
            "           2       0.22      0.34      0.27        83\n",
            "           3       0.26      0.06      0.10        78\n",
            "           4       0.54      0.46      0.50        72\n",
            "           5       0.47      0.35      0.41        79\n",
            "\n",
            "    accuracy                           0.34       495\n",
            "   macro avg       0.35      0.34      0.33       495\n",
            "weighted avg       0.35      0.34      0.32       495\n",
            "\n",
            "[[22 28 30  5  3  9]\n",
            " [11 52  7  6  8  2]\n",
            " [20 21 28  1  4  9]\n",
            " [10 12 34  5  7 10]\n",
            " [ 1 25 11  1 33  1]\n",
            " [11 17 16  1  6 28]]\n",
            "Accuracy: 0.3393939393939394\n",
            "Total f1 over classes: 1.9604222933881625\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.36      0.36        97\n",
            "           1       0.51      0.58      0.54        86\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.34      0.27      0.30        78\n",
            "           4       0.56      0.65      0.60        72\n",
            "           5       0.38      0.37      0.37        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.40      0.41      0.40       495\n",
            "weighted avg       0.40      0.41      0.40       495\n",
            "\n",
            "[[35 22 13 13  2 12]\n",
            " [12 50  6  5 10  3]\n",
            " [16  8 19 15  9 16]\n",
            " [12  6 20 21  7 12]\n",
            " [ 7  6  6  2 47  4]\n",
            " [16  7 13  5  9 29]]\n",
            "Accuracy: 0.40606060606060607\n",
            "Total f1 over classes: 2.4159308238473938\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.67      0.43        97\n",
            "           1       0.57      0.49      0.53        86\n",
            "           2       0.29      0.13      0.18        83\n",
            "           3       0.33      0.17      0.22        78\n",
            "           4       0.49      0.42      0.45        72\n",
            "           5       0.38      0.37      0.37        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.39      0.37      0.36       495\n",
            "weighted avg       0.39      0.38      0.37       495\n",
            "\n",
            "[[65 11  2  6  4  9]\n",
            " [29 42  3  2  6  4]\n",
            " [34  5 11  8  7 18]\n",
            " [26  4 15 13  9 11]\n",
            " [19  9  2  6 30  6]\n",
            " [32  3  5  5  5 29]]\n",
            "Accuracy: 0.3838383838383838\n",
            "Total f1 over classes: 2.180543432371714\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.38      0.40        97\n",
            "           1       0.42      0.64      0.50        85\n",
            "           2       0.33      0.06      0.10        83\n",
            "           3       0.41      0.38      0.39        79\n",
            "           4       0.54      0.65      0.59        72\n",
            "           5       0.47      0.60      0.53        78\n",
            "\n",
            "    accuracy                           0.45       494\n",
            "   macro avg       0.43      0.45      0.42       494\n",
            "weighted avg       0.43      0.45      0.41       494\n",
            "\n",
            "[[37 32  3  8  8  9]\n",
            " [ 8 54  1  8  5  9]\n",
            " [19 10  5 19 12 18]\n",
            " [13 12  4 30  8 12]\n",
            " [ 5  8  2  5 47  5]\n",
            " [ 6 14  0  4  7 47]]\n",
            "Accuracy: 0.44534412955465585\n",
            "Total f1 over classes: 2.515808116660886\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.26      0.28        97\n",
            "           1       0.32      0.55      0.40        85\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.23      0.24      0.23        79\n",
            "           4       0.48      0.32      0.38        72\n",
            "           5       0.34      0.26      0.29        78\n",
            "\n",
            "    accuracy                           0.31       494\n",
            "   macro avg       0.32      0.31      0.31       494\n",
            "weighted avg       0.32      0.31      0.30       494\n",
            "\n",
            "[[25 25 15 14  4 14]\n",
            " [ 8 47  5 14  6  5]\n",
            " [20  9 19 19  3 13]\n",
            " [16 21 12 19  6  5]\n",
            " [ 5 26  9  7 23  2]\n",
            " [ 7 20 15 10  6 20]]\n",
            "Accuracy: 0.3097165991902834\n",
            "Total f1 over classes: 1.8347107194008774\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.37      0.38        97\n",
            "           1       0.45      0.59      0.51        85\n",
            "           2       0.35      0.22      0.27        83\n",
            "           3       0.36      0.30      0.33        79\n",
            "           4       0.48      0.56      0.51        72\n",
            "           5       0.43      0.49      0.46        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.41      0.42      0.41       494\n",
            "weighted avg       0.41      0.42      0.41       494\n",
            "\n",
            "[[36 23 11  9  6 12]\n",
            " [12 50  2  8  7  6]\n",
            " [15  8 18 16 11 15]\n",
            " [13 10 12 24  8 12]\n",
            " [ 5  8  5  9 40  5]\n",
            " [10 13  4  1 12 38]]\n",
            "Accuracy: 0.41700404858299595\n",
            "Total f1 over classes: 2.4566785646782803\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.28      0.30        97\n",
            "           1       0.42      0.41      0.41        85\n",
            "           2       0.12      0.04      0.06        83\n",
            "           3       0.33      0.48      0.39        79\n",
            "           4       0.41      0.49      0.45        72\n",
            "           5       0.36      0.47      0.41        78\n",
            "\n",
            "    accuracy                           0.35       494\n",
            "   macro avg       0.33      0.36      0.34       494\n",
            "weighted avg       0.33      0.35      0.33       494\n",
            "\n",
            "[[27 17  9 15 13 16]\n",
            " [16 35  2 11 12  9]\n",
            " [ 9  9  3 30  6 26]\n",
            " [15  5  2 38 10  9]\n",
            " [ 4 11  6 10 35  6]\n",
            " [10  7  4 11  9 37]]\n",
            "Accuracy: 0.354251012145749\n",
            "Total f1 over classes: 2.019070070445017\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.39        97\n",
            "           1       0.39      0.46      0.42        85\n",
            "           2       0.29      0.22      0.25        83\n",
            "           3       0.29      0.24      0.26        79\n",
            "           4       0.61      0.60      0.60        72\n",
            "           5       0.42      0.53      0.47        78\n",
            "\n",
            "    accuracy                           0.40       494\n",
            "   macro avg       0.40      0.41      0.40       494\n",
            "weighted avg       0.39      0.40      0.39       494\n",
            "\n",
            "[[38 23 15  8  2 11]\n",
            " [13 39  7  9  6 11]\n",
            " [13 10 18 19  6 17]\n",
            " [17 10 14 19  8 11]\n",
            " [ 7  7  3  6 43  6]\n",
            " [11 10  5  5  6 41]]\n",
            "Accuracy: 0.4008097165991903\n",
            "Total f1 over classes: 2.391983003075314\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.64      0.44        97\n",
            "           1       0.49      0.48      0.49        85\n",
            "           2       0.42      0.17      0.24        83\n",
            "           3       0.35      0.22      0.27        79\n",
            "           4       0.53      0.51      0.52        72\n",
            "           5       0.50      0.46      0.48        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.44      0.41      0.41       494\n",
            "weighted avg       0.43      0.42      0.40       494\n",
            "\n",
            "[[62 16  5  6  3  5]\n",
            " [25 41  0  7  7  5]\n",
            " [34  8 14  8  4 15]\n",
            " [26  7  9 17 11  9]\n",
            " [16  7  3  7 37  2]\n",
            " [25  4  2  3  8 36]]\n",
            "Accuracy: 0.4190283400809717\n",
            "Total f1 over classes: 2.4334055637347625\n",
            "Doing test analysis with ANN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.39      0.36        33\n",
            "           1       0.56      0.51      0.53        39\n",
            "           2       0.60      0.14      0.22        44\n",
            "           3       0.35      0.36      0.36        33\n",
            "           4       0.41      0.57      0.48        37\n",
            "           5       0.37      0.53      0.43        34\n",
            "\n",
            "    accuracy                           0.41       220\n",
            "   macro avg       0.44      0.42      0.40       220\n",
            "weighted avg       0.45      0.41      0.39       220\n",
            "\n",
            "[[13  7  0  1  5  7]\n",
            " [ 7 20  0  4  6  2]\n",
            " [ 6  4  6 11  9  8]\n",
            " [ 6  1  0 12  6  8]\n",
            " [ 3  1  4  2 21  6]\n",
            " [ 5  3  0  4  4 18]]\n",
            "Accuracy: 0.4090909090909091\n",
            "Total f1 over classes: 2.380936561372843\n",
            "Doing test analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.39      0.33        33\n",
            "           1       0.38      0.49      0.43        39\n",
            "           2       0.17      0.16      0.17        44\n",
            "           3       0.25      0.27      0.26        33\n",
            "           4       0.50      0.32      0.39        37\n",
            "           5       0.42      0.29      0.34        34\n",
            "\n",
            "    accuracy                           0.32       220\n",
            "   macro avg       0.33      0.32      0.32       220\n",
            "weighted avg       0.33      0.32      0.32       220\n",
            "\n",
            "[[13  4  6  3  1  6]\n",
            " [ 8 19  3  5  2  2]\n",
            " [11  8  7 11  4  3]\n",
            " [ 7  4  7  9  3  3]\n",
            " [ 3 11  8  3 12  0]\n",
            " [ 4  4  9  5  2 10]]\n",
            "Accuracy: 0.3181818181818182\n",
            "Total f1 over classes: 1.9218866572272386\n",
            "Doing test analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.48      0.43        33\n",
            "           1       0.57      0.59      0.58        39\n",
            "           2       0.42      0.23      0.29        44\n",
            "           3       0.28      0.27      0.28        33\n",
            "           4       0.44      0.51      0.48        37\n",
            "           5       0.36      0.41      0.38        34\n",
            "\n",
            "    accuracy                           0.41       220\n",
            "   macro avg       0.41      0.42      0.41       220\n",
            "weighted avg       0.41      0.41      0.41       220\n",
            "\n",
            "[[16  6  2  0  4  5]\n",
            " [ 5 23  0  6  4  1]\n",
            " [ 6  5 10 12  6  5]\n",
            " [ 5  1  7  9  4  7]\n",
            " [ 3  1  3  4 19  7]\n",
            " [ 7  4  2  1  6 14]]\n",
            "Accuracy: 0.41363636363636364\n",
            "Total f1 over classes: 2.438547515496842\n",
            "Doing test analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.30      0.28        33\n",
            "           1       0.33      0.23      0.27        39\n",
            "           2       0.17      0.05      0.07        44\n",
            "           3       0.24      0.39      0.30        33\n",
            "           4       0.41      0.38      0.39        37\n",
            "           5       0.26      0.41      0.32        34\n",
            "\n",
            "    accuracy                           0.28       220\n",
            "   macro avg       0.28      0.29      0.27       220\n",
            "weighted avg       0.28      0.28      0.26       220\n",
            "\n",
            "[[10  6  0  4  3 10]\n",
            " [10  9  5  9  5  1]\n",
            " [ 4  4  2 15  7 12]\n",
            " [ 4  3  1 13  2 10]\n",
            " [ 2  3  2  9 14  7]\n",
            " [ 9  2  2  4  3 14]]\n",
            "Accuracy: 0.2818181818181818\n",
            "Total f1 over classes: 1.6333322120111824\n",
            "Doing test analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.52      0.43        33\n",
            "           1       0.62      0.51      0.56        39\n",
            "           2       0.45      0.34      0.39        44\n",
            "           3       0.34      0.33      0.34        33\n",
            "           4       0.57      0.57      0.57        37\n",
            "           5       0.33      0.38      0.36        34\n",
            "\n",
            "    accuracy                           0.44       220\n",
            "   macro avg       0.45      0.44      0.44       220\n",
            "weighted avg       0.45      0.44      0.44       220\n",
            "\n",
            "[[17  4  3  1  2  6]\n",
            " [ 5 20  3  3  5  3]\n",
            " [10  5 15  7  1  6]\n",
            " [ 6  1  6 11  3  6]\n",
            " [ 3  1  4  3 21  5]\n",
            " [ 6  1  2  7  5 13]]\n",
            "Accuracy: 0.4409090909090909\n",
            "Total f1 over classes: 2.6401841608912804\n",
            "Doing test analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.82      0.43        33\n",
            "           1       0.57      0.44      0.49        39\n",
            "           2       0.33      0.14      0.19        44\n",
            "           3       0.26      0.18      0.21        33\n",
            "           4       0.59      0.43      0.50        37\n",
            "           5       0.32      0.26      0.29        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.39      0.38      0.35       220\n",
            "weighted avg       0.40      0.37      0.35       220\n",
            "\n",
            "[[27  3  0  0  0  3]\n",
            " [ 9 17  1  6  4  2]\n",
            " [20  3  6  7  2  6]\n",
            " [14  2  7  6  2  2]\n",
            " [ 8  3  1  3 16  6]\n",
            " [16  2  3  1  3  9]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.1161071556097566\n",
            "No. of training examples: 1979\n",
            "No. of testing examples: 220\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.20      0.22        96\n",
            "           1       0.42      0.41      0.42        85\n",
            "           2       0.25      0.21      0.23        84\n",
            "           3       0.33      0.19      0.24        79\n",
            "           4       0.38      0.57      0.45        72\n",
            "           5       0.39      0.56      0.46        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.34      0.36      0.34       495\n",
            "weighted avg       0.33      0.35      0.33       495\n",
            "\n",
            "[[19 19 23  5 13 17]\n",
            " [19 35  4  6 15  6]\n",
            " [ 6  7 18 13 17 23]\n",
            " [12 10 16 15 13 13]\n",
            " [ 4 10  5  3 41  9]\n",
            " [13  2  6  4 10 44]]\n",
            "Accuracy: 0.3474747474747475\n",
            "Total f1 over classes: 2.0260596267681574\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.38        96\n",
            "           1       0.29      0.49      0.37        85\n",
            "           2       0.25      0.17      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.39      0.28      0.33        72\n",
            "           5       0.31      0.29      0.30        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.32      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[37 19 13 10  4 13]\n",
            " [16 42  8  9  3  7]\n",
            " [11 16 14 18  6 19]\n",
            " [10 23  9 22 10  5]\n",
            " [ 5 29  4  6 20  8]\n",
            " [18 15  8  7  8 23]]\n",
            "Accuracy: 0.3191919191919192\n",
            "Total f1 over classes: 1.8655271954040469\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.35      0.37        96\n",
            "           1       0.45      0.49      0.47        85\n",
            "           2       0.37      0.23      0.28        84\n",
            "           3       0.28      0.22      0.24        79\n",
            "           4       0.45      0.57      0.50        72\n",
            "           5       0.31      0.43      0.36        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.37      0.38      0.37       495\n",
            "weighted avg       0.37      0.38      0.37       495\n",
            "\n",
            "[[34 18  4  7 11 22]\n",
            " [15 42  5  6  7 10]\n",
            " [ 7  7 19 16  9 26]\n",
            " [14  9 13 17 15 11]\n",
            " [ 2  9  2 11 41  7]\n",
            " [14  9  9  4  9 34]]\n",
            "Accuracy: 0.37777777777777777\n",
            "Total f1 over classes: 2.2249573839945183\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.27      0.33        96\n",
            "           1       0.38      0.36      0.37        85\n",
            "           2       0.24      0.37      0.29        84\n",
            "           3       0.30      0.28      0.29        79\n",
            "           4       0.44      0.56      0.49        72\n",
            "           5       0.42      0.29      0.34        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.36      0.35      0.35       495\n",
            "weighted avg       0.36      0.35      0.35       495\n",
            "\n",
            "[[26 18 30 10  5  7]\n",
            " [14 31 17  8 11  4]\n",
            " [ 5  9 31 19 10 10]\n",
            " [ 6  7 22 22 18  4]\n",
            " [ 8  7  5  5 40  7]\n",
            " [ 5  9 26 10  6 23]]\n",
            "Accuracy: 0.34949494949494947\n",
            "Total f1 over classes: 2.111558510856655\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.42      0.42        96\n",
            "           1       0.46      0.53      0.49        85\n",
            "           2       0.35      0.21      0.26        84\n",
            "           3       0.33      0.30      0.32        79\n",
            "           4       0.48      0.51      0.50        72\n",
            "           5       0.35      0.44      0.39        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.40      0.40      0.40       495\n",
            "weighted avg       0.40      0.40      0.40       495\n",
            "\n",
            "[[40 20  8 10  3 15]\n",
            " [17 45  4  6  4  9]\n",
            " [ 8 10 18 16 11 21]\n",
            " [12  9  9 24 14 11]\n",
            " [ 4  8  4  9 37 10]\n",
            " [14  6  9  7  8 35]]\n",
            "Accuracy: 0.402020202020202\n",
            "Total f1 over classes: 2.3787713074736216\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.74      0.45        96\n",
            "           1       0.49      0.44      0.46        85\n",
            "           2       0.36      0.15      0.22        84\n",
            "           3       0.20      0.05      0.08        79\n",
            "           4       0.57      0.57      0.57        72\n",
            "           5       0.45      0.42      0.43        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.40      0.39      0.37       495\n",
            "weighted avg       0.40      0.40      0.37       495\n",
            "\n",
            "[[71 12  2  1  1  9]\n",
            " [33 37  4  2  3  6]\n",
            " [37  7 13  5  7 15]\n",
            " [29 11 14  4 15  6]\n",
            " [14  6  2  4 41  5]\n",
            " [34  2  1  4  5 33]]\n",
            "Accuracy: 0.402020202020202\n",
            "Total f1 over classes: 2.2130210403018573\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.34      0.36        97\n",
            "           1       0.45      0.66      0.54        85\n",
            "           2       0.36      0.24      0.29        83\n",
            "           3       0.42      0.22      0.29        79\n",
            "           4       0.46      0.54      0.50        72\n",
            "           5       0.37      0.48      0.42        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.41      0.41      0.40       495\n",
            "weighted avg       0.41      0.41      0.40       495\n",
            "\n",
            "[[33 21  7  6  9 21]\n",
            " [ 9 56  3  1  5 11]\n",
            " [16 14 20 11 13  9]\n",
            " [10 19  8 17 10 15]\n",
            " [10  8  1  4 39 10]\n",
            " [10  6 16  1  8 38]]\n",
            "Accuracy: 0.4101010101010101\n",
            "Total f1 over classes: 2.3835118288470127\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.28      0.51      0.36        85\n",
            "           2       0.21      0.16      0.18        83\n",
            "           3       0.30      0.24      0.27        79\n",
            "           4       0.42      0.31      0.35        72\n",
            "           5       0.38      0.29      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.31      0.30       495\n",
            "weighted avg       0.32      0.31      0.31       495\n",
            "\n",
            "[[34 23 17  8  6  9]\n",
            " [17 43  5  3  4 13]\n",
            " [15 27 13 17  8  3]\n",
            " [13 26  7 19  8  6]\n",
            " [ 6 22  8  7 22  7]\n",
            " [16 15 11  9  5 23]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.82901343439319\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.43      0.40        97\n",
            "           1       0.47      0.59      0.52        85\n",
            "           2       0.33      0.17      0.22        83\n",
            "           3       0.31      0.22      0.26        79\n",
            "           4       0.39      0.53      0.45        72\n",
            "           5       0.41      0.42      0.41        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.38       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[42 20  5  5 11 14]\n",
            " [13 50  1  5  8  8]\n",
            " [17 10 14 18 15  9]\n",
            " [16 13 11 17 16  6]\n",
            " [ 8  7  3  5 38 11]\n",
            " [17  6  9  4 10 33]]\n",
            "Accuracy: 0.39191919191919194\n",
            "Total f1 over classes: 2.260980352920079\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.29      0.31        97\n",
            "           1       0.41      0.22      0.29        85\n",
            "           2       0.24      0.08      0.12        83\n",
            "           3       0.28      0.13      0.17        79\n",
            "           4       0.30      0.60      0.40        72\n",
            "           5       0.34      0.67      0.45        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.33      0.29       495\n",
            "weighted avg       0.32      0.32      0.29       495\n",
            "\n",
            "[[28  7  5  3 17 37]\n",
            " [15 19  0  4 30 17]\n",
            " [18  6  7 10 20 22]\n",
            " [ 8  8 12 10 25 16]\n",
            " [10  3  2  5 43  9]\n",
            " [ 7  3  3  4  9 53]]\n",
            "Accuracy: 0.32323232323232326\n",
            "Total f1 over classes: 1.7480840787836167\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.39      0.38        97\n",
            "           1       0.43      0.54      0.48        85\n",
            "           2       0.27      0.17      0.21        83\n",
            "           3       0.30      0.23      0.26        79\n",
            "           4       0.50      0.53      0.51        72\n",
            "           5       0.42      0.52      0.47        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.40      0.38       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[38 18  8  8  9 16]\n",
            " [13 46  3  5  3 15]\n",
            " [18 16 14 15 11  9]\n",
            " [17 16 13 18  9  6]\n",
            " [ 6  7  4  7 38 10]\n",
            " [11  4 10  7  6 41]]\n",
            "Accuracy: 0.3939393939393939\n",
            "Total f1 over classes: 2.3049894842520744\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.70      0.45        97\n",
            "           1       0.61      0.41      0.49        85\n",
            "           2       0.37      0.08      0.14        83\n",
            "           3       0.42      0.18      0.25        79\n",
            "           4       0.44      0.62      0.51        72\n",
            "           5       0.42      0.42      0.42        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.43      0.40      0.38       495\n",
            "weighted avg       0.43      0.41      0.38       495\n",
            "\n",
            "[[68  6  2  1  8 12]\n",
            " [28 35  1  2 11  8]\n",
            " [35  6  7 12 14  9]\n",
            " [33  8  5 14 12  7]\n",
            " [14  1  0  3 45  9]\n",
            " [27  1  4  1 13 33]]\n",
            "Accuracy: 0.4080808080808081\n",
            "Total f1 over classes: 2.265211654158282\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.31      0.34        97\n",
            "           1       0.51      0.67      0.58        86\n",
            "           2       0.37      0.31      0.34        83\n",
            "           3       0.38      0.29      0.33        78\n",
            "           4       0.45      0.61      0.52        72\n",
            "           5       0.47      0.46      0.46        79\n",
            "\n",
            "    accuracy                           0.44       495\n",
            "   macro avg       0.43      0.44      0.43       495\n",
            "weighted avg       0.43      0.44      0.43       495\n",
            "\n",
            "[[30 22 12  9 10 14]\n",
            " [11 58  2  3  9  3]\n",
            " [10  9 26 14 13 11]\n",
            " [12  8 16 23 10  9]\n",
            " [ 4  9  7  4 44  4]\n",
            " [10  7  8  7 11 36]]\n",
            "Accuracy: 0.4383838383838384\n",
            "Total f1 over classes: 2.5809863507769486\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.31      0.33        97\n",
            "           1       0.34      0.63      0.44        86\n",
            "           2       0.20      0.14      0.17        83\n",
            "           3       0.19      0.19      0.19        78\n",
            "           4       0.44      0.33      0.38        72\n",
            "           5       0.39      0.28      0.32        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.31      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[30 22  8 23  5  9]\n",
            " [ 9 54  5  9  6  3]\n",
            " [16 25 12 14  7  9]\n",
            " [11 22 15 15  5 10]\n",
            " [12 20  4  8 24  4]\n",
            " [ 7 16 16 10  8 22]]\n",
            "Accuracy: 0.31717171717171716\n",
            "Total f1 over classes: 1.830883794251098\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.40      0.42        97\n",
            "           1       0.57      0.65      0.61        86\n",
            "           2       0.27      0.20      0.23        83\n",
            "           3       0.41      0.26      0.31        78\n",
            "           4       0.47      0.67      0.55        72\n",
            "           5       0.38      0.44      0.41        79\n",
            "\n",
            "    accuracy                           0.43       495\n",
            "   macro avg       0.42      0.44      0.42       495\n",
            "weighted avg       0.42      0.43      0.42       495\n",
            "\n",
            "[[39 15 12  8 10 13]\n",
            " [12 56  3  1 10  4]\n",
            " [12  8 17 14 11 21]\n",
            " [ 7  4 19 20 13 15]\n",
            " [ 6  8  3  3 48  4]\n",
            " [14  7 10  3 10 35]]\n",
            "Accuracy: 0.43434343434343436\n",
            "Total f1 over classes: 2.53314196164445\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.23      0.26        97\n",
            "           1       0.34      0.60      0.43        86\n",
            "           2       0.22      0.34      0.27        83\n",
            "           3       0.26      0.06      0.10        78\n",
            "           4       0.54      0.46      0.50        72\n",
            "           5       0.47      0.35      0.41        79\n",
            "\n",
            "    accuracy                           0.34       495\n",
            "   macro avg       0.35      0.34      0.33       495\n",
            "weighted avg       0.35      0.34      0.32       495\n",
            "\n",
            "[[22 28 30  5  3  9]\n",
            " [11 52  7  6  8  2]\n",
            " [20 21 28  1  4  9]\n",
            " [10 12 34  5  7 10]\n",
            " [ 1 25 11  1 33  1]\n",
            " [11 17 16  1  6 28]]\n",
            "Accuracy: 0.3393939393939394\n",
            "Total f1 over classes: 1.9604222933881625\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.55      0.59      0.57        86\n",
            "           2       0.21      0.19      0.20        83\n",
            "           3       0.31      0.23      0.26        78\n",
            "           4       0.52      0.60      0.56        72\n",
            "           5       0.38      0.42      0.40        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.39      0.40      0.39       495\n",
            "weighted avg       0.38      0.39      0.39       495\n",
            "\n",
            "[[34 16 17 13  2 15]\n",
            " [13 51  5  3  9  5]\n",
            " [19  7 16 13 11 17]\n",
            " [13  8 21 18  7 11]\n",
            " [ 8  6  7  3 43  5]\n",
            " [14  5  9  8 10 33]]\n",
            "Accuracy: 0.3939393939393939\n",
            "Total f1 over classes: 2.3389458320331\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.67      0.43        97\n",
            "           1       0.57      0.49      0.53        86\n",
            "           2       0.29      0.13      0.18        83\n",
            "           3       0.33      0.17      0.22        78\n",
            "           4       0.49      0.42      0.45        72\n",
            "           5       0.38      0.37      0.37        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.39      0.37      0.36       495\n",
            "weighted avg       0.39      0.38      0.37       495\n",
            "\n",
            "[[65 11  2  6  4  9]\n",
            " [29 42  3  2  6  4]\n",
            " [34  5 11  8  7 18]\n",
            " [26  4 15 13  9 11]\n",
            " [19  9  2  6 30  6]\n",
            " [32  3  5  5  5 29]]\n",
            "Accuracy: 0.3838383838383838\n",
            "Total f1 over classes: 2.180543432371714\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.32      0.38        97\n",
            "           1       0.43      0.61      0.50        85\n",
            "           2       0.45      0.24      0.31        83\n",
            "           3       0.38      0.42      0.40        79\n",
            "           4       0.51      0.60      0.55        72\n",
            "           5       0.46      0.53      0.49        78\n",
            "\n",
            "    accuracy                           0.45       494\n",
            "   macro avg       0.45      0.45      0.44       494\n",
            "weighted avg       0.45      0.45      0.43       494\n",
            "\n",
            "[[31 25 10 15  8  8]\n",
            " [ 6 52  2  6  7 12]\n",
            " [10 12 20 20  9 12]\n",
            " [11  9  6 33  9 11]\n",
            " [ 5  9  1  9 43  5]\n",
            " [ 4 14  5  5  9 41]]\n",
            "Accuracy: 0.44534412955465585\n",
            "Total f1 over classes: 2.6318620248882256\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.26      0.28        97\n",
            "           1       0.32      0.55      0.40        85\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.23      0.24      0.23        79\n",
            "           4       0.48      0.32      0.38        72\n",
            "           5       0.34      0.26      0.29        78\n",
            "\n",
            "    accuracy                           0.31       494\n",
            "   macro avg       0.32      0.31      0.31       494\n",
            "weighted avg       0.32      0.31      0.30       494\n",
            "\n",
            "[[25 25 15 14  4 14]\n",
            " [ 8 47  5 14  6  5]\n",
            " [20  9 19 19  3 13]\n",
            " [16 21 12 19  6  5]\n",
            " [ 5 26  9  7 23  2]\n",
            " [ 7 20 15 10  6 20]]\n",
            "Accuracy: 0.3097165991902834\n",
            "Total f1 over classes: 1.8347107194008774\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.37      0.38        97\n",
            "           1       0.45      0.59      0.51        85\n",
            "           2       0.35      0.22      0.27        83\n",
            "           3       0.36      0.30      0.33        79\n",
            "           4       0.48      0.56      0.51        72\n",
            "           5       0.43      0.49      0.46        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.41      0.42      0.41       494\n",
            "weighted avg       0.41      0.42      0.41       494\n",
            "\n",
            "[[36 23 11  9  6 12]\n",
            " [12 50  2  8  7  6]\n",
            " [15  8 18 16 11 15]\n",
            " [13 10 12 24  8 12]\n",
            " [ 5  8  5  9 40  5]\n",
            " [10 13  4  1 12 38]]\n",
            "Accuracy: 0.41700404858299595\n",
            "Total f1 over classes: 2.4566785646782803\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.28      0.30        97\n",
            "           1       0.42      0.41      0.41        85\n",
            "           2       0.12      0.04      0.06        83\n",
            "           3       0.33      0.48      0.39        79\n",
            "           4       0.41      0.49      0.45        72\n",
            "           5       0.36      0.47      0.41        78\n",
            "\n",
            "    accuracy                           0.35       494\n",
            "   macro avg       0.33      0.36      0.34       494\n",
            "weighted avg       0.33      0.35      0.33       494\n",
            "\n",
            "[[27 17  9 15 13 16]\n",
            " [16 35  2 11 12  9]\n",
            " [ 9  9  3 30  6 26]\n",
            " [15  5  2 38 10  9]\n",
            " [ 4 11  6 10 35  6]\n",
            " [10  7  4 11  9 37]]\n",
            "Accuracy: 0.354251012145749\n",
            "Total f1 over classes: 2.019070070445017\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.33      0.34        97\n",
            "           1       0.39      0.47      0.43        85\n",
            "           2       0.31      0.27      0.29        83\n",
            "           3       0.30      0.27      0.28        79\n",
            "           4       0.58      0.58      0.58        72\n",
            "           5       0.43      0.49      0.46        78\n",
            "\n",
            "    accuracy                           0.39       494\n",
            "   macro avg       0.39      0.40      0.40       494\n",
            "weighted avg       0.39      0.39      0.39       494\n",
            "\n",
            "[[32 22 19  8  3 13]\n",
            " [10 40  8  9  7 11]\n",
            " [14 12 22 16  6 13]\n",
            " [15 12 14 21  6 11]\n",
            " [ 7  8  2 10 42  3]\n",
            " [12  9  5  6  8 38]]\n",
            "Accuracy: 0.39473684210526316\n",
            "Total f1 over classes: 2.3756619518683224\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.64      0.44        97\n",
            "           1       0.49      0.48      0.49        85\n",
            "           2       0.42      0.17      0.24        83\n",
            "           3       0.35      0.22      0.27        79\n",
            "           4       0.53      0.51      0.52        72\n",
            "           5       0.50      0.46      0.48        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.44      0.41      0.41       494\n",
            "weighted avg       0.43      0.42      0.40       494\n",
            "\n",
            "[[62 16  5  6  3  5]\n",
            " [25 41  0  7  7  5]\n",
            " [34  8 14  8  4 15]\n",
            " [26  7  9 17 11  9]\n",
            " [16  7  3  7 37  2]\n",
            " [25  4  2  3  8 36]]\n",
            "Accuracy: 0.4190283400809717\n",
            "Total f1 over classes: 2.4334055637347625\n",
            "Doing test analysis with ANN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.33      0.35        33\n",
            "           1       0.62      0.64      0.63        39\n",
            "           2       0.42      0.25      0.31        44\n",
            "           3       0.34      0.39      0.37        33\n",
            "           4       0.53      0.62      0.57        37\n",
            "           5       0.37      0.47      0.42        34\n",
            "\n",
            "    accuracy                           0.45       220\n",
            "   macro avg       0.44      0.45      0.44       220\n",
            "weighted avg       0.45      0.45      0.44       220\n",
            "\n",
            "[[11  5  2  4  5  6]\n",
            " [ 4 25  0  4  4  2]\n",
            " [ 5  4 11 11  5  8]\n",
            " [ 4  2  5 13  4  5]\n",
            " [ 0  2  4  2 23  6]\n",
            " [ 6  2  4  4  2 16]]\n",
            "Accuracy: 0.45\n",
            "Total f1 over classes: 2.653185054580134\n",
            "Doing test analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.39      0.33        33\n",
            "           1       0.38      0.49      0.43        39\n",
            "           2       0.17      0.16      0.17        44\n",
            "           3       0.25      0.27      0.26        33\n",
            "           4       0.50      0.32      0.39        37\n",
            "           5       0.42      0.29      0.34        34\n",
            "\n",
            "    accuracy                           0.32       220\n",
            "   macro avg       0.33      0.32      0.32       220\n",
            "weighted avg       0.33      0.32      0.32       220\n",
            "\n",
            "[[13  4  6  3  1  6]\n",
            " [ 8 19  3  5  2  2]\n",
            " [11  8  7 11  4  3]\n",
            " [ 7  4  7  9  3  3]\n",
            " [ 3 11  8  3 12  0]\n",
            " [ 4  4  9  5  2 10]]\n",
            "Accuracy: 0.3181818181818182\n",
            "Total f1 over classes: 1.9218866572272386\n",
            "Doing test analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.48      0.43        33\n",
            "           1       0.57      0.59      0.58        39\n",
            "           2       0.42      0.23      0.29        44\n",
            "           3       0.28      0.27      0.28        33\n",
            "           4       0.44      0.51      0.48        37\n",
            "           5       0.36      0.41      0.38        34\n",
            "\n",
            "    accuracy                           0.41       220\n",
            "   macro avg       0.41      0.42      0.41       220\n",
            "weighted avg       0.41      0.41      0.41       220\n",
            "\n",
            "[[16  6  2  0  4  5]\n",
            " [ 5 23  0  6  4  1]\n",
            " [ 6  5 10 12  6  5]\n",
            " [ 5  1  7  9  4  7]\n",
            " [ 3  1  3  4 19  7]\n",
            " [ 7  4  2  1  6 14]]\n",
            "Accuracy: 0.41363636363636364\n",
            "Total f1 over classes: 2.438547515496842\n",
            "Doing test analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.30      0.28        33\n",
            "           1       0.33      0.23      0.27        39\n",
            "           2       0.17      0.05      0.07        44\n",
            "           3       0.24      0.39      0.30        33\n",
            "           4       0.41      0.38      0.39        37\n",
            "           5       0.26      0.41      0.32        34\n",
            "\n",
            "    accuracy                           0.28       220\n",
            "   macro avg       0.28      0.29      0.27       220\n",
            "weighted avg       0.28      0.28      0.26       220\n",
            "\n",
            "[[10  6  0  4  3 10]\n",
            " [10  9  5  9  5  1]\n",
            " [ 4  4  2 15  7 12]\n",
            " [ 4  3  1 13  2 10]\n",
            " [ 2  3  2  9 14  7]\n",
            " [ 9  2  2  4  3 14]]\n",
            "Accuracy: 0.2818181818181818\n",
            "Total f1 over classes: 1.6333322120111824\n",
            "Doing test analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.45      0.39        33\n",
            "           1       0.54      0.51      0.53        39\n",
            "           2       0.41      0.25      0.31        44\n",
            "           3       0.34      0.33      0.34        33\n",
            "           4       0.53      0.51      0.52        37\n",
            "           5       0.34      0.44      0.38        34\n",
            "\n",
            "    accuracy                           0.41       220\n",
            "   macro avg       0.42      0.42      0.41       220\n",
            "weighted avg       0.42      0.41      0.41       220\n",
            "\n",
            "[[15  6  3  0  3  6]\n",
            " [ 5 20  2  6  3  3]\n",
            " [12  4 11  9  1  7]\n",
            " [ 4  2  4 11  4  8]\n",
            " [ 2  3  5  3 19  5]\n",
            " [ 6  2  2  3  6 15]]\n",
            "Accuracy: 0.41363636363636364\n",
            "Total f1 over classes: 2.4694102022960536\n",
            "Doing test analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.82      0.43        33\n",
            "           1       0.57      0.44      0.49        39\n",
            "           2       0.33      0.14      0.19        44\n",
            "           3       0.26      0.18      0.21        33\n",
            "           4       0.59      0.43      0.50        37\n",
            "           5       0.32      0.26      0.29        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.39      0.38      0.35       220\n",
            "weighted avg       0.40      0.37      0.35       220\n",
            "\n",
            "[[27  3  0  0  0  3]\n",
            " [ 9 17  1  6  4  2]\n",
            " [20  3  6  7  2  6]\n",
            " [14  2  7  6  2  2]\n",
            " [ 8  3  1  3 16  6]\n",
            " [16  2  3  1  3  9]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.1161071556097566\n",
            "No. of training examples: 1979\n",
            "No. of testing examples: 220\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.30      0.33        96\n",
            "           1       0.36      0.52      0.43        85\n",
            "           2       0.38      0.24      0.29        84\n",
            "           3       0.28      0.18      0.22        79\n",
            "           4       0.52      0.54      0.53        72\n",
            "           5       0.40      0.58      0.47        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.38       495\n",
            "weighted avg       0.38      0.39      0.37       495\n",
            "\n",
            "[[29 27  5  9  5 21]\n",
            " [18 44  3  7  5  8]\n",
            " [12  8 20 15  8 21]\n",
            " [ 9 17 17 14 11 11]\n",
            " [ 6 13  3  3 39  8]\n",
            " [ 6 13  5  2  7 46]]\n",
            "Accuracy: 0.3878787878787879\n",
            "Total f1 over classes: 2.2685303429995844\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.38        96\n",
            "           1       0.29      0.49      0.37        85\n",
            "           2       0.25      0.17      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.39      0.28      0.33        72\n",
            "           5       0.31      0.29      0.30        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.32      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[37 19 13 10  4 13]\n",
            " [16 42  8  9  3  7]\n",
            " [11 16 14 18  6 19]\n",
            " [10 23  9 22 10  5]\n",
            " [ 5 29  4  6 20  8]\n",
            " [18 15  8  7  8 23]]\n",
            "Accuracy: 0.3191919191919192\n",
            "Total f1 over classes: 1.8655271954040469\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.35      0.37        96\n",
            "           1       0.45      0.49      0.47        85\n",
            "           2       0.37      0.23      0.28        84\n",
            "           3       0.28      0.22      0.24        79\n",
            "           4       0.45      0.57      0.50        72\n",
            "           5       0.31      0.43      0.36        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.37      0.38      0.37       495\n",
            "weighted avg       0.37      0.38      0.37       495\n",
            "\n",
            "[[34 18  4  7 11 22]\n",
            " [15 42  5  6  7 10]\n",
            " [ 7  7 19 16  9 26]\n",
            " [14  9 13 17 15 11]\n",
            " [ 2  9  2 11 41  7]\n",
            " [14  9  9  4  9 34]]\n",
            "Accuracy: 0.37777777777777777\n",
            "Total f1 over classes: 2.2249573839945183\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.27      0.33        96\n",
            "           1       0.38      0.36      0.37        85\n",
            "           2       0.24      0.37      0.29        84\n",
            "           3       0.30      0.28      0.29        79\n",
            "           4       0.44      0.56      0.49        72\n",
            "           5       0.42      0.29      0.34        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.36      0.35      0.35       495\n",
            "weighted avg       0.36      0.35      0.35       495\n",
            "\n",
            "[[26 18 30 10  5  7]\n",
            " [14 31 17  8 11  4]\n",
            " [ 5  9 31 19 10 10]\n",
            " [ 6  7 22 22 18  4]\n",
            " [ 8  7  5  5 40  7]\n",
            " [ 5  9 26 10  6 23]]\n",
            "Accuracy: 0.34949494949494947\n",
            "Total f1 over classes: 2.111558510856655\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.47      0.46        96\n",
            "           1       0.46      0.53      0.49        85\n",
            "           2       0.30      0.20      0.24        84\n",
            "           3       0.35      0.32      0.33        79\n",
            "           4       0.55      0.64      0.59        72\n",
            "           5       0.38      0.43      0.40        79\n",
            "\n",
            "    accuracy                           0.43       495\n",
            "   macro avg       0.42      0.43      0.42       495\n",
            "weighted avg       0.42      0.43      0.42       495\n",
            "\n",
            "[[45 14 13  9  3 12]\n",
            " [19 45  5  7  3  6]\n",
            " [ 8 12 17 17 11 19]\n",
            " [11 11 10 25 12 10]\n",
            " [ 3  7  1  7 46  8]\n",
            " [13  8 10  6  8 34]]\n",
            "Accuracy: 0.42828282828282827\n",
            "Total f1 over classes: 2.530544724093111\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.74      0.45        96\n",
            "           1       0.49      0.44      0.46        85\n",
            "           2       0.36      0.15      0.22        84\n",
            "           3       0.20      0.05      0.08        79\n",
            "           4       0.57      0.57      0.57        72\n",
            "           5       0.45      0.42      0.43        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.40      0.39      0.37       495\n",
            "weighted avg       0.40      0.40      0.37       495\n",
            "\n",
            "[[71 12  2  1  1  9]\n",
            " [33 37  4  2  3  6]\n",
            " [37  7 13  5  7 15]\n",
            " [29 11 14  4 15  6]\n",
            " [14  6  2  4 41  5]\n",
            " [34  2  1  4  5 33]]\n",
            "Accuracy: 0.402020202020202\n",
            "Total f1 over classes: 2.2130210403018573\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.25      0.33        97\n",
            "           1       0.42      0.67      0.52        85\n",
            "           2       0.34      0.35      0.35        83\n",
            "           3       0.32      0.22      0.26        79\n",
            "           4       0.43      0.54      0.48        72\n",
            "           5       0.41      0.42      0.41        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.40      0.41      0.39       495\n",
            "weighted avg       0.40      0.40      0.39       495\n",
            "\n",
            "[[24 24 18  5 11 15]\n",
            " [ 6 57  2  4  6 10]\n",
            " [ 8 14 29 15 11  6]\n",
            " [ 3 18 17 17 16  8]\n",
            " [ 3 11  6  4 39  9]\n",
            " [ 6 11 13  8  8 33]]\n",
            "Accuracy: 0.402020202020202\n",
            "Total f1 over classes: 2.3385538906025323\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.28      0.51      0.36        85\n",
            "           2       0.21      0.16      0.18        83\n",
            "           3       0.30      0.24      0.27        79\n",
            "           4       0.42      0.31      0.35        72\n",
            "           5       0.38      0.29      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.31      0.30       495\n",
            "weighted avg       0.32      0.31      0.31       495\n",
            "\n",
            "[[34 23 17  8  6  9]\n",
            " [17 43  5  3  4 13]\n",
            " [15 27 13 17  8  3]\n",
            " [13 26  7 19  8  6]\n",
            " [ 6 22  8  7 22  7]\n",
            " [16 15 11  9  5 23]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.82901343439319\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.43      0.40        97\n",
            "           1       0.47      0.59      0.52        85\n",
            "           2       0.33      0.17      0.22        83\n",
            "           3       0.31      0.22      0.26        79\n",
            "           4       0.39      0.53      0.45        72\n",
            "           5       0.41      0.42      0.41        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.38       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[42 20  5  5 11 14]\n",
            " [13 50  1  5  8  8]\n",
            " [17 10 14 18 15  9]\n",
            " [16 13 11 17 16  6]\n",
            " [ 8  7  3  5 38 11]\n",
            " [17  6  9  4 10 33]]\n",
            "Accuracy: 0.39191919191919194\n",
            "Total f1 over classes: 2.260980352920079\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.29      0.31        97\n",
            "           1       0.41      0.22      0.29        85\n",
            "           2       0.24      0.08      0.12        83\n",
            "           3       0.28      0.13      0.17        79\n",
            "           4       0.30      0.60      0.40        72\n",
            "           5       0.34      0.67      0.45        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.33      0.29       495\n",
            "weighted avg       0.32      0.32      0.29       495\n",
            "\n",
            "[[28  7  5  3 17 37]\n",
            " [15 19  0  4 30 17]\n",
            " [18  6  7 10 20 22]\n",
            " [ 8  8 12 10 25 16]\n",
            " [10  3  2  5 43  9]\n",
            " [ 7  3  3  4  9 53]]\n",
            "Accuracy: 0.32323232323232326\n",
            "Total f1 over classes: 1.7480840787836167\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.45      0.42        97\n",
            "           1       0.45      0.53      0.49        85\n",
            "           2       0.31      0.18      0.23        83\n",
            "           3       0.31      0.22      0.25        79\n",
            "           4       0.50      0.54      0.52        72\n",
            "           5       0.39      0.51      0.44        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.39      0.40      0.39       495\n",
            "weighted avg       0.39      0.40      0.39       495\n",
            "\n",
            "[[44 13  7  7  8 18]\n",
            " [14 45  2  4  5 15]\n",
            " [18 14 15 16 11  9]\n",
            " [16 17 12 17  8  9]\n",
            " [ 7  6  2  7 39 11]\n",
            " [13  4 11  4  7 40]]\n",
            "Accuracy: 0.40404040404040403\n",
            "Total f1 over classes: 2.3531760871941088\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.70      0.45        97\n",
            "           1       0.61      0.41      0.49        85\n",
            "           2       0.37      0.08      0.14        83\n",
            "           3       0.42      0.18      0.25        79\n",
            "           4       0.44      0.62      0.51        72\n",
            "           5       0.42      0.42      0.42        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.43      0.40      0.38       495\n",
            "weighted avg       0.43      0.41      0.38       495\n",
            "\n",
            "[[68  6  2  1  8 12]\n",
            " [28 35  1  2 11  8]\n",
            " [35  6  7 12 14  9]\n",
            " [33  8  5 14 12  7]\n",
            " [14  1  0  3 45  9]\n",
            " [27  1  4  1 13 33]]\n",
            "Accuracy: 0.4080808080808081\n",
            "Total f1 over classes: 2.265211654158282\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.13      0.21        97\n",
            "           1       0.45      0.69      0.54        86\n",
            "           2       0.29      0.33      0.31        83\n",
            "           3       0.35      0.26      0.30        78\n",
            "           4       0.48      0.61      0.54        72\n",
            "           5       0.40      0.51      0.45        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.42      0.42      0.39       495\n",
            "weighted avg       0.42      0.41      0.38       495\n",
            "\n",
            "[[13 26 21 11  9 17]\n",
            " [ 2 59  7  3 11  4]\n",
            " [ 3 16 27 11  6 20]\n",
            " [ 3  7 25 20 11 12]\n",
            " [ 2 11  5  4 44  6]\n",
            " [ 1 12  8  8 10 40]]\n",
            "Accuracy: 0.4101010101010101\n",
            "Total f1 over classes: 2.3510848158763356\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.31      0.33        97\n",
            "           1       0.34      0.63      0.44        86\n",
            "           2       0.20      0.14      0.17        83\n",
            "           3       0.19      0.19      0.19        78\n",
            "           4       0.44      0.33      0.38        72\n",
            "           5       0.39      0.28      0.32        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.31      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[30 22  8 23  5  9]\n",
            " [ 9 54  5  9  6  3]\n",
            " [16 25 12 14  7  9]\n",
            " [11 22 15 15  5 10]\n",
            " [12 20  4  8 24  4]\n",
            " [ 7 16 16 10  8 22]]\n",
            "Accuracy: 0.31717171717171716\n",
            "Total f1 over classes: 1.830883794251098\n",
            "Doing analysis with Logistic Regression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.40      0.42        97\n",
            "           1       0.57      0.65      0.61        86\n",
            "           2       0.27      0.20      0.23        83\n",
            "           3       0.41      0.26      0.31        78\n",
            "           4       0.47      0.67      0.55        72\n",
            "           5       0.38      0.44      0.41        79\n",
            "\n",
            "    accuracy                           0.43       495\n",
            "   macro avg       0.42      0.44      0.42       495\n",
            "weighted avg       0.42      0.43      0.42       495\n",
            "\n",
            "[[39 15 12  8 10 13]\n",
            " [12 56  3  1 10  4]\n",
            " [12  8 17 14 11 21]\n",
            " [ 7  4 19 20 13 15]\n",
            " [ 6  8  3  3 48  4]\n",
            " [14  7 10  3 10 35]]\n",
            "Accuracy: 0.43434343434343436\n",
            "Total f1 over classes: 2.53314196164445\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.23      0.26        97\n",
            "           1       0.34      0.60      0.43        86\n",
            "           2       0.22      0.34      0.27        83\n",
            "           3       0.26      0.06      0.10        78\n",
            "           4       0.54      0.46      0.50        72\n",
            "           5       0.47      0.35      0.41        79\n",
            "\n",
            "    accuracy                           0.34       495\n",
            "   macro avg       0.35      0.34      0.33       495\n",
            "weighted avg       0.35      0.34      0.32       495\n",
            "\n",
            "[[22 28 30  5  3  9]\n",
            " [11 52  7  6  8  2]\n",
            " [20 21 28  1  4  9]\n",
            " [10 12 34  5  7 10]\n",
            " [ 1 25 11  1 33  1]\n",
            " [11 17 16  1  6 28]]\n",
            "Accuracy: 0.3393939393939394\n",
            "Total f1 over classes: 1.9604222933881625\n",
            "Doing analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.34      0.34        97\n",
            "           1       0.48      0.58      0.52        86\n",
            "           2       0.21      0.19      0.20        83\n",
            "           3       0.36      0.26      0.30        78\n",
            "           4       0.51      0.58      0.55        72\n",
            "           5       0.39      0.39      0.39        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.38       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[33 18 15 12  5 14]\n",
            " [15 50  5  3  9  4]\n",
            " [16 13 16 12 10 16]\n",
            " [11  9 20 20  7 11]\n",
            " [ 7  7  8  4 42  4]\n",
            " [15  8 11  5  9 31]]\n",
            "Accuracy: 0.3878787878787879\n",
            "Total f1 over classes: 2.300197155620066\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.67      0.43        97\n",
            "           1       0.57      0.49      0.53        86\n",
            "           2       0.29      0.13      0.18        83\n",
            "           3       0.33      0.17      0.22        78\n",
            "           4       0.49      0.42      0.45        72\n",
            "           5       0.38      0.37      0.37        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.39      0.37      0.36       495\n",
            "weighted avg       0.39      0.38      0.37       495\n",
            "\n",
            "[[65 11  2  6  4  9]\n",
            " [29 42  3  2  6  4]\n",
            " [34  5 11  8  7 18]\n",
            " [26  4 15 13  9 11]\n",
            " [19  9  2  6 30  6]\n",
            " [32  3  5  5  5 29]]\n",
            "Accuracy: 0.3838383838383838\n",
            "Total f1 over classes: 2.180543432371714\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.22      0.29        97\n",
            "           1       0.41      0.64      0.50        85\n",
            "           2       0.25      0.16      0.19        83\n",
            "           3       0.38      0.44      0.41        79\n",
            "           4       0.54      0.49      0.51        72\n",
            "           5       0.41      0.55      0.47        78\n",
            "\n",
            "    accuracy                           0.41       494\n",
            "   macro avg       0.40      0.41      0.40       494\n",
            "weighted avg       0.40      0.41      0.39       494\n",
            "\n",
            "[[21 29 19  9  3 16]\n",
            " [ 6 54  2  7  6 10]\n",
            " [ 9 13 13 19 11 18]\n",
            " [ 5 11 11 35  5 12]\n",
            " [ 3  9  3 15 35  7]\n",
            " [ 4 15  5  6  5 43]]\n",
            "Accuracy: 0.4068825910931174\n",
            "Total f1 over classes: 2.3709365583416964\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.26      0.28        97\n",
            "           1       0.32      0.55      0.40        85\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.23      0.24      0.23        79\n",
            "           4       0.48      0.32      0.38        72\n",
            "           5       0.34      0.26      0.29        78\n",
            "\n",
            "    accuracy                           0.31       494\n",
            "   macro avg       0.32      0.31      0.31       494\n",
            "weighted avg       0.32      0.31      0.30       494\n",
            "\n",
            "[[25 25 15 14  4 14]\n",
            " [ 8 47  5 14  6  5]\n",
            " [20  9 19 19  3 13]\n",
            " [16 21 12 19  6  5]\n",
            " [ 5 26  9  7 23  2]\n",
            " [ 7 20 15 10  6 20]]\n",
            "Accuracy: 0.3097165991902834\n",
            "Total f1 over classes: 1.8347107194008774\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.37      0.38        97\n",
            "           1       0.45      0.59      0.51        85\n",
            "           2       0.35      0.22      0.27        83\n",
            "           3       0.36      0.30      0.33        79\n",
            "           4       0.48      0.56      0.51        72\n",
            "           5       0.43      0.49      0.46        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.41      0.42      0.41       494\n",
            "weighted avg       0.41      0.42      0.41       494\n",
            "\n",
            "[[36 23 11  9  6 12]\n",
            " [12 50  2  8  7  6]\n",
            " [15  8 18 16 11 15]\n",
            " [13 10 12 24  8 12]\n",
            " [ 5  8  5  9 40  5]\n",
            " [10 13  4  1 12 38]]\n",
            "Accuracy: 0.41700404858299595\n",
            "Total f1 over classes: 2.4566785646782803\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.28      0.30        97\n",
            "           1       0.42      0.41      0.41        85\n",
            "           2       0.12      0.04      0.06        83\n",
            "           3       0.33      0.48      0.39        79\n",
            "           4       0.41      0.49      0.45        72\n",
            "           5       0.36      0.47      0.41        78\n",
            "\n",
            "    accuracy                           0.35       494\n",
            "   macro avg       0.33      0.36      0.34       494\n",
            "weighted avg       0.33      0.35      0.33       494\n",
            "\n",
            "[[27 17  9 15 13 16]\n",
            " [16 35  2 11 12  9]\n",
            " [ 9  9  3 30  6 26]\n",
            " [15  5  2 38 10  9]\n",
            " [ 4 11  6 10 35  6]\n",
            " [10  7  4 11  9 37]]\n",
            "Accuracy: 0.354251012145749\n",
            "Total f1 over classes: 2.019070070445017\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.36      0.37        97\n",
            "           1       0.43      0.51      0.46        85\n",
            "           2       0.31      0.24      0.27        83\n",
            "           3       0.26      0.25      0.26        79\n",
            "           4       0.58      0.57      0.57        72\n",
            "           5       0.42      0.50      0.46        78\n",
            "\n",
            "    accuracy                           0.40       494\n",
            "   macro avg       0.40      0.41      0.40       494\n",
            "weighted avg       0.40      0.40      0.40       494\n",
            "\n",
            "[[35 20 19  8  2 13]\n",
            " [ 8 43  6 12  5 11]\n",
            " [11 10 20 20  8 14]\n",
            " [17 12 13 20  7 10]\n",
            " [ 9  6  3  8 41  5]\n",
            " [11  9  3  8  8 39]]\n",
            "Accuracy: 0.4008097165991903\n",
            "Total f1 over classes: 2.3996287529015654\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.64      0.44        97\n",
            "           1       0.49      0.48      0.49        85\n",
            "           2       0.42      0.17      0.24        83\n",
            "           3       0.35      0.22      0.27        79\n",
            "           4       0.53      0.51      0.52        72\n",
            "           5       0.50      0.46      0.48        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.44      0.41      0.41       494\n",
            "weighted avg       0.43      0.42      0.40       494\n",
            "\n",
            "[[62 16  5  6  3  5]\n",
            " [25 41  0  7  7  5]\n",
            " [34  8 14  8  4 15]\n",
            " [26  7  9 17 11  9]\n",
            " [16  7  3  7 37  2]\n",
            " [25  4  2  3  8 36]]\n",
            "Accuracy: 0.4190283400809717\n",
            "Total f1 over classes: 2.4334055637347625\n",
            "Doing test analysis with ANN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.18      0.22        33\n",
            "           1       0.51      0.62      0.56        39\n",
            "           2       0.54      0.34      0.42        44\n",
            "           3       0.35      0.36      0.36        33\n",
            "           4       0.50      0.49      0.49        37\n",
            "           5       0.32      0.50      0.39        34\n",
            "\n",
            "    accuracy                           0.42       220\n",
            "   macro avg       0.42      0.41      0.41       220\n",
            "weighted avg       0.43      0.42      0.41       220\n",
            "\n",
            "[[ 6 11  5  1  3  7]\n",
            " [ 4 24  0  6  3  2]\n",
            " [ 4  5 15  7  4  9]\n",
            " [ 2  1  3 12  4 11]\n",
            " [ 2  3  3  4 18  7]\n",
            " [ 4  3  2  4  4 17]]\n",
            "Accuracy: 0.41818181818181815\n",
            "Total f1 over classes: 2.435152257588743\n",
            "Doing test analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.39      0.33        33\n",
            "           1       0.38      0.49      0.43        39\n",
            "           2       0.17      0.16      0.17        44\n",
            "           3       0.25      0.27      0.26        33\n",
            "           4       0.50      0.32      0.39        37\n",
            "           5       0.42      0.29      0.34        34\n",
            "\n",
            "    accuracy                           0.32       220\n",
            "   macro avg       0.33      0.32      0.32       220\n",
            "weighted avg       0.33      0.32      0.32       220\n",
            "\n",
            "[[13  4  6  3  1  6]\n",
            " [ 8 19  3  5  2  2]\n",
            " [11  8  7 11  4  3]\n",
            " [ 7  4  7  9  3  3]\n",
            " [ 3 11  8  3 12  0]\n",
            " [ 4  4  9  5  2 10]]\n",
            "Accuracy: 0.3181818181818182\n",
            "Total f1 over classes: 1.9218866572272386\n",
            "Doing test analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.48      0.43        33\n",
            "           1       0.57      0.59      0.58        39\n",
            "           2       0.42      0.23      0.29        44\n",
            "           3       0.28      0.27      0.28        33\n",
            "           4       0.44      0.51      0.48        37\n",
            "           5       0.36      0.41      0.38        34\n",
            "\n",
            "    accuracy                           0.41       220\n",
            "   macro avg       0.41      0.42      0.41       220\n",
            "weighted avg       0.41      0.41      0.41       220\n",
            "\n",
            "[[16  6  2  0  4  5]\n",
            " [ 5 23  0  6  4  1]\n",
            " [ 6  5 10 12  6  5]\n",
            " [ 5  1  7  9  4  7]\n",
            " [ 3  1  3  4 19  7]\n",
            " [ 7  4  2  1  6 14]]\n",
            "Accuracy: 0.41363636363636364\n",
            "Total f1 over classes: 2.438547515496842\n",
            "Doing test analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.30      0.28        33\n",
            "           1       0.33      0.23      0.27        39\n",
            "           2       0.17      0.05      0.07        44\n",
            "           3       0.24      0.39      0.30        33\n",
            "           4       0.41      0.38      0.39        37\n",
            "           5       0.26      0.41      0.32        34\n",
            "\n",
            "    accuracy                           0.28       220\n",
            "   macro avg       0.28      0.29      0.27       220\n",
            "weighted avg       0.28      0.28      0.26       220\n",
            "\n",
            "[[10  6  0  4  3 10]\n",
            " [10  9  5  9  5  1]\n",
            " [ 4  4  2 15  7 12]\n",
            " [ 4  3  1 13  2 10]\n",
            " [ 2  3  2  9 14  7]\n",
            " [ 9  2  2  4  3 14]]\n",
            "Accuracy: 0.2818181818181818\n",
            "Total f1 over classes: 1.6333322120111824\n",
            "Doing test analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.48      0.42        33\n",
            "           1       0.53      0.46      0.49        39\n",
            "           2       0.41      0.34      0.37        44\n",
            "           3       0.32      0.36      0.34        33\n",
            "           4       0.56      0.49      0.52        37\n",
            "           5       0.35      0.38      0.37        34\n",
            "\n",
            "    accuracy                           0.42       220\n",
            "   macro avg       0.42      0.42      0.42       220\n",
            "weighted avg       0.43      0.42      0.42       220\n",
            "\n",
            "[[16  7  5  0  2  3]\n",
            " [ 6 18  4  5  3  3]\n",
            " [ 8  5 15  9  2  5]\n",
            " [ 4  1  5 12  3  8]\n",
            " [ 4  2  4  4 18  5]\n",
            " [ 5  1  4  7  4 13]]\n",
            "Accuracy: 0.41818181818181815\n",
            "Total f1 over classes: 2.515367143271342\n",
            "Doing test analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.82      0.43        33\n",
            "           1       0.57      0.44      0.49        39\n",
            "           2       0.33      0.14      0.19        44\n",
            "           3       0.26      0.18      0.21        33\n",
            "           4       0.59      0.43      0.50        37\n",
            "           5       0.32      0.26      0.29        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.39      0.38      0.35       220\n",
            "weighted avg       0.40      0.37      0.35       220\n",
            "\n",
            "[[27  3  0  0  0  3]\n",
            " [ 9 17  1  6  4  2]\n",
            " [20  3  6  7  2  6]\n",
            " [14  2  7  6  2  2]\n",
            " [ 8  3  1  3 16  6]\n",
            " [16  2  3  1  3  9]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.1161071556097566\n",
            "No. of training examples: 1979\n",
            "No. of testing examples: 220\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.29      0.33        96\n",
            "           1       0.42      0.55      0.47        85\n",
            "           2       0.36      0.24      0.29        84\n",
            "           3       0.19      0.08      0.11        79\n",
            "           4       0.37      0.60      0.46        72\n",
            "           5       0.34      0.46      0.39        79\n",
            "\n",
            "    accuracy                           0.36       495\n",
            "   macro avg       0.34      0.37      0.34       495\n",
            "weighted avg       0.34      0.36      0.34       495\n",
            "\n",
            "[[28 21 11  3 13 20]\n",
            " [15 47  3  3 14  3]\n",
            " [10  7 20 10 13 24]\n",
            " [11  9 15  6 26 12]\n",
            " [ 0 13  3  2 43 11]\n",
            " [ 9 16  4  8  6 36]]\n",
            "Accuracy: 0.36363636363636365\n",
            "Total f1 over classes: 2.049013052632962\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.39      0.38        96\n",
            "           1       0.29      0.49      0.37        85\n",
            "           2       0.25      0.17      0.20        84\n",
            "           3       0.31      0.28      0.29        79\n",
            "           4       0.39      0.28      0.33        72\n",
            "           5       0.31      0.29      0.30        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.32      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[37 19 13 10  4 13]\n",
            " [16 42  8  9  3  7]\n",
            " [11 16 14 18  6 19]\n",
            " [10 23  9 22 10  5]\n",
            " [ 5 29  4  6 20  8]\n",
            " [18 15  8  7  8 23]]\n",
            "Accuracy: 0.3191919191919192\n",
            "Total f1 over classes: 1.8655271954040469\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.35      0.37        96\n",
            "           1       0.45      0.49      0.47        85\n",
            "           2       0.37      0.23      0.28        84\n",
            "           3       0.28      0.22      0.24        79\n",
            "           4       0.45      0.57      0.50        72\n",
            "           5       0.31      0.43      0.36        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.37      0.38      0.37       495\n",
            "weighted avg       0.37      0.38      0.37       495\n",
            "\n",
            "[[34 18  4  7 11 22]\n",
            " [15 42  5  6  7 10]\n",
            " [ 7  7 19 16  9 26]\n",
            " [14  9 13 17 15 11]\n",
            " [ 2  9  2 11 41  7]\n",
            " [14  9  9  4  9 34]]\n",
            "Accuracy: 0.37777777777777777\n",
            "Total f1 over classes: 2.2249573839945183\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.27      0.33        96\n",
            "           1       0.38      0.36      0.37        85\n",
            "           2       0.24      0.37      0.29        84\n",
            "           3       0.30      0.28      0.29        79\n",
            "           4       0.44      0.56      0.49        72\n",
            "           5       0.42      0.29      0.34        79\n",
            "\n",
            "    accuracy                           0.35       495\n",
            "   macro avg       0.36      0.35      0.35       495\n",
            "weighted avg       0.36      0.35      0.35       495\n",
            "\n",
            "[[26 18 30 10  5  7]\n",
            " [14 31 17  8 11  4]\n",
            " [ 5  9 31 19 10 10]\n",
            " [ 6  7 22 22 18  4]\n",
            " [ 8  7  5  5 40  7]\n",
            " [ 5  9 26 10  6 23]]\n",
            "Accuracy: 0.34949494949494947\n",
            "Total f1 over classes: 2.111558510856655\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.40      0.42        96\n",
            "           1       0.43      0.52      0.47        85\n",
            "           2       0.29      0.21      0.24        84\n",
            "           3       0.33      0.33      0.33        79\n",
            "           4       0.49      0.51      0.50        72\n",
            "           5       0.39      0.44      0.42        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.40      0.40      0.40       495\n",
            "weighted avg       0.39      0.40      0.40       495\n",
            "\n",
            "[[38 20 13 11  3 11]\n",
            " [16 44  5  8  3  9]\n",
            " [ 9 11 18 17 12 17]\n",
            " [ 9 10 15 26 11  8]\n",
            " [ 3  8  3 12 37  9]\n",
            " [11 10  9  5  9 35]]\n",
            "Accuracy: 0.4\n",
            "Total f1 over classes: 2.379747434410587\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.74      0.45        96\n",
            "           1       0.49      0.44      0.46        85\n",
            "           2       0.36      0.15      0.22        84\n",
            "           3       0.20      0.05      0.08        79\n",
            "           4       0.57      0.57      0.57        72\n",
            "           5       0.45      0.42      0.43        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.40      0.39      0.37       495\n",
            "weighted avg       0.40      0.40      0.37       495\n",
            "\n",
            "[[71 12  2  1  1  9]\n",
            " [33 37  4  2  3  6]\n",
            " [37  7 13  5  7 15]\n",
            " [29 11 14  4 15  6]\n",
            " [14  6  2  4 41  5]\n",
            " [34  2  1  4  5 33]]\n",
            "Accuracy: 0.402020202020202\n",
            "Total f1 over classes: 2.2130210403018573\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.31      0.34        97\n",
            "           1       0.41      0.73      0.53        85\n",
            "           2       0.33      0.17      0.22        83\n",
            "           3       0.42      0.24      0.31        79\n",
            "           4       0.43      0.60      0.50        72\n",
            "           5       0.39      0.39      0.39        79\n",
            "\n",
            "    accuracy                           0.40       495\n",
            "   macro avg       0.40      0.41      0.38       495\n",
            "weighted avg       0.39      0.40      0.38       495\n",
            "\n",
            "[[30 27  9  2 13 16]\n",
            " [ 8 62  1  2  5  7]\n",
            " [17 16 14 16 14  6]\n",
            " [ 8 17  8 19 17 10]\n",
            " [ 5 11  2  2 43  9]\n",
            " [11 17  8  4  8 31]]\n",
            "Accuracy: 0.402020202020202\n",
            "Total f1 over classes: 2.291425341571541\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.35      0.34        97\n",
            "           1       0.28      0.51      0.36        85\n",
            "           2       0.21      0.16      0.18        83\n",
            "           3       0.30      0.24      0.27        79\n",
            "           4       0.42      0.31      0.35        72\n",
            "           5       0.38      0.29      0.33        79\n",
            "\n",
            "    accuracy                           0.31       495\n",
            "   macro avg       0.32      0.31      0.30       495\n",
            "weighted avg       0.32      0.31      0.31       495\n",
            "\n",
            "[[34 23 17  8  6  9]\n",
            " [17 43  5  3  4 13]\n",
            " [15 27 13 17  8  3]\n",
            " [13 26  7 19  8  6]\n",
            " [ 6 22  8  7 22  7]\n",
            " [16 15 11  9  5 23]]\n",
            "Accuracy: 0.3111111111111111\n",
            "Total f1 over classes: 1.82901343439319\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.43      0.40        97\n",
            "           1       0.47      0.59      0.52        85\n",
            "           2       0.33      0.17      0.22        83\n",
            "           3       0.31      0.22      0.26        79\n",
            "           4       0.39      0.53      0.45        72\n",
            "           5       0.41      0.42      0.41        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.38      0.39      0.38       495\n",
            "weighted avg       0.38      0.39      0.38       495\n",
            "\n",
            "[[42 20  5  5 11 14]\n",
            " [13 50  1  5  8  8]\n",
            " [17 10 14 18 15  9]\n",
            " [16 13 11 17 16  6]\n",
            " [ 8  7  3  5 38 11]\n",
            " [17  6  9  4 10 33]]\n",
            "Accuracy: 0.39191919191919194\n",
            "Total f1 over classes: 2.260980352920079\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.29      0.31        97\n",
            "           1       0.41      0.22      0.29        85\n",
            "           2       0.24      0.08      0.12        83\n",
            "           3       0.28      0.13      0.17        79\n",
            "           4       0.30      0.60      0.40        72\n",
            "           5       0.34      0.67      0.45        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.33      0.29       495\n",
            "weighted avg       0.32      0.32      0.29       495\n",
            "\n",
            "[[28  7  5  3 17 37]\n",
            " [15 19  0  4 30 17]\n",
            " [18  6  7 10 20 22]\n",
            " [ 8  8 12 10 25 16]\n",
            " [10  3  2  5 43  9]\n",
            " [ 7  3  3  4  9 53]]\n",
            "Accuracy: 0.32323232323232326\n",
            "Total f1 over classes: 1.7480840787836167\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.37      0.34        97\n",
            "           1       0.43      0.47      0.45        85\n",
            "           2       0.26      0.14      0.18        83\n",
            "           3       0.36      0.27      0.30        79\n",
            "           4       0.48      0.49      0.48        72\n",
            "           5       0.40      0.54      0.46        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.37      0.38      0.37       495\n",
            "weighted avg       0.37      0.38      0.37       495\n",
            "\n",
            "[[36 17  8  7  9 20]\n",
            " [20 40  3  3  4 15]\n",
            " [22 13 12 15 11 10]\n",
            " [20 14  8 21  8  8]\n",
            " [ 7  6  6  7 35 11]\n",
            " [10  4 10  6  6 43]]\n",
            "Accuracy: 0.37777777777777777\n",
            "Total f1 over classes: 2.220637438600956\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.70      0.45        97\n",
            "           1       0.61      0.41      0.49        85\n",
            "           2       0.37      0.08      0.14        83\n",
            "           3       0.42      0.18      0.25        79\n",
            "           4       0.44      0.62      0.51        72\n",
            "           5       0.42      0.42      0.42        79\n",
            "\n",
            "    accuracy                           0.41       495\n",
            "   macro avg       0.43      0.40      0.38       495\n",
            "weighted avg       0.43      0.41      0.38       495\n",
            "\n",
            "[[68  6  2  1  8 12]\n",
            " [28 35  1  2 11  8]\n",
            " [35  6  7 12 14  9]\n",
            " [33  8  5 14 12  7]\n",
            " [14  1  0  3 45  9]\n",
            " [27  1  4  1 13 33]]\n",
            "Accuracy: 0.4080808080808081\n",
            "Total f1 over classes: 2.265211654158282\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.33      0.38        97\n",
            "           1       0.46      0.69      0.55        86\n",
            "           2       0.34      0.20      0.26        83\n",
            "           3       0.41      0.28      0.33        78\n",
            "           4       0.46      0.60      0.52        72\n",
            "           5       0.41      0.51      0.45        79\n",
            "\n",
            "    accuracy                           0.43       495\n",
            "   macro avg       0.42      0.43      0.42       495\n",
            "weighted avg       0.42      0.43      0.41       495\n",
            "\n",
            "[[32 24 10  7 10 14]\n",
            " [ 9 59  2  0 11  5]\n",
            " [ 9 13 17 14 10 20]\n",
            " [10  9 13 22 11 13]\n",
            " [ 3 11  5  4 43  6]\n",
            " [ 8 12  3  7  9 40]]\n",
            "Accuracy: 0.4303030303030303\n",
            "Total f1 over classes: 2.4913763714755235\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.31      0.33        97\n",
            "           1       0.34      0.63      0.44        86\n",
            "           2       0.20      0.14      0.17        83\n",
            "           3       0.19      0.19      0.19        78\n",
            "           4       0.44      0.33      0.38        72\n",
            "           5       0.39      0.28      0.32        79\n",
            "\n",
            "    accuracy                           0.32       495\n",
            "   macro avg       0.32      0.31      0.31       495\n",
            "weighted avg       0.32      0.32      0.31       495\n",
            "\n",
            "[[30 22  8 23  5  9]\n",
            " [ 9 54  5  9  6  3]\n",
            " [16 25 12 14  7  9]\n",
            " [11 22 15 15  5 10]\n",
            " [12 20  4  8 24  4]\n",
            " [ 7 16 16 10  8 22]]\n",
            "Accuracy: 0.31717171717171716\n",
            "Total f1 over classes: 1.830883794251098\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.40      0.42        97\n",
            "           1       0.57      0.65      0.61        86\n",
            "           2       0.27      0.20      0.23        83\n",
            "           3       0.41      0.26      0.31        78\n",
            "           4       0.47      0.67      0.55        72\n",
            "           5       0.38      0.44      0.41        79\n",
            "\n",
            "    accuracy                           0.43       495\n",
            "   macro avg       0.42      0.44      0.42       495\n",
            "weighted avg       0.42      0.43      0.42       495\n",
            "\n",
            "[[39 15 12  8 10 13]\n",
            " [12 56  3  1 10  4]\n",
            " [12  8 17 14 11 21]\n",
            " [ 7  4 19 20 13 15]\n",
            " [ 6  8  3  3 48  4]\n",
            " [14  7 10  3 10 35]]\n",
            "Accuracy: 0.43434343434343436\n",
            "Total f1 over classes: 2.53314196164445\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.23      0.26        97\n",
            "           1       0.34      0.60      0.43        86\n",
            "           2       0.22      0.34      0.27        83\n",
            "           3       0.26      0.06      0.10        78\n",
            "           4       0.54      0.46      0.50        72\n",
            "           5       0.47      0.35      0.41        79\n",
            "\n",
            "    accuracy                           0.34       495\n",
            "   macro avg       0.35      0.34      0.33       495\n",
            "weighted avg       0.35      0.34      0.32       495\n",
            "\n",
            "[[22 28 30  5  3  9]\n",
            " [11 52  7  6  8  2]\n",
            " [20 21 28  1  4  9]\n",
            " [10 12 34  5  7 10]\n",
            " [ 1 25 11  1 33  1]\n",
            " [11 17 16  1  6 28]]\n",
            "Accuracy: 0.3393939393939394\n",
            "Total f1 over classes: 1.9604222933881625\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.36      0.35        97\n",
            "           1       0.50      0.56      0.53        86\n",
            "           2       0.27      0.25      0.26        83\n",
            "           3       0.27      0.21      0.23        78\n",
            "           4       0.54      0.60      0.57        72\n",
            "           5       0.39      0.39      0.39        79\n",
            "\n",
            "    accuracy                           0.39       495\n",
            "   macro avg       0.39      0.39      0.39       495\n",
            "weighted avg       0.38      0.39      0.39       495\n",
            "\n",
            "[[35 18 13 14  3 14]\n",
            " [14 48  6  4 10  4]\n",
            " [17 11 21 13 10 11]\n",
            " [14  7 23 16  5 13]\n",
            " [ 7  4  5  6 43  7]\n",
            " [14  8 11  7  8 31]]\n",
            "Accuracy: 0.39191919191919194\n",
            "Total f1 over classes: 2.331624728997453\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.67      0.43        97\n",
            "           1       0.57      0.49      0.53        86\n",
            "           2       0.29      0.13      0.18        83\n",
            "           3       0.33      0.17      0.22        78\n",
            "           4       0.49      0.42      0.45        72\n",
            "           5       0.38      0.37      0.37        79\n",
            "\n",
            "    accuracy                           0.38       495\n",
            "   macro avg       0.39      0.37      0.36       495\n",
            "weighted avg       0.39      0.38      0.37       495\n",
            "\n",
            "[[65 11  2  6  4  9]\n",
            " [29 42  3  2  6  4]\n",
            " [34  5 11  8  7 18]\n",
            " [26  4 15 13  9 11]\n",
            " [19  9  2  6 30  6]\n",
            " [32  3  5  5  5 29]]\n",
            "Accuracy: 0.3838383838383838\n",
            "Total f1 over classes: 2.180543432371714\n",
            "Doing analysis with ANN\n",
            "Doing analysis with fully connected neural network\n",
            "[INFO] training network...\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.31      0.34        97\n",
            "           1       0.43      0.56      0.49        85\n",
            "           2       0.27      0.17      0.21        83\n",
            "           3       0.42      0.30      0.35        79\n",
            "           4       0.45      0.69      0.55        72\n",
            "           5       0.50      0.54      0.52        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.41      0.43      0.41       494\n",
            "weighted avg       0.41      0.42      0.40       494\n",
            "\n",
            "[[30 24 16  4 13 10]\n",
            " [11 48  4  3 12  7]\n",
            " [17 11 14 17 13 11]\n",
            " [13 10 13 24 10  9]\n",
            " [ 4  5  2  6 50  5]\n",
            " [ 5 13  3  3 12 42]]\n",
            "Accuracy: 0.42105263157894735\n",
            "Total f1 over classes: 2.457096621061868\n",
            "Doing analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.26      0.28        97\n",
            "           1       0.32      0.55      0.40        85\n",
            "           2       0.25      0.23      0.24        83\n",
            "           3       0.23      0.24      0.23        79\n",
            "           4       0.48      0.32      0.38        72\n",
            "           5       0.34      0.26      0.29        78\n",
            "\n",
            "    accuracy                           0.31       494\n",
            "   macro avg       0.32      0.31      0.31       494\n",
            "weighted avg       0.32      0.31      0.30       494\n",
            "\n",
            "[[25 25 15 14  4 14]\n",
            " [ 8 47  5 14  6  5]\n",
            " [20  9 19 19  3 13]\n",
            " [16 21 12 19  6  5]\n",
            " [ 5 26  9  7 23  2]\n",
            " [ 7 20 15 10  6 20]]\n",
            "Accuracy: 0.3097165991902834\n",
            "Total f1 over classes: 1.8347107194008774\n",
            "Doing analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.37      0.38        97\n",
            "           1       0.45      0.59      0.51        85\n",
            "           2       0.35      0.22      0.27        83\n",
            "           3       0.36      0.30      0.33        79\n",
            "           4       0.48      0.56      0.51        72\n",
            "           5       0.43      0.49      0.46        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.41      0.42      0.41       494\n",
            "weighted avg       0.41      0.42      0.41       494\n",
            "\n",
            "[[36 23 11  9  6 12]\n",
            " [12 50  2  8  7  6]\n",
            " [15  8 18 16 11 15]\n",
            " [13 10 12 24  8 12]\n",
            " [ 5  8  5  9 40  5]\n",
            " [10 13  4  1 12 38]]\n",
            "Accuracy: 0.41700404858299595\n",
            "Total f1 over classes: 2.4566785646782803\n",
            "Doing analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.28      0.30        97\n",
            "           1       0.42      0.41      0.41        85\n",
            "           2       0.12      0.04      0.06        83\n",
            "           3       0.33      0.48      0.39        79\n",
            "           4       0.41      0.49      0.45        72\n",
            "           5       0.36      0.47      0.41        78\n",
            "\n",
            "    accuracy                           0.35       494\n",
            "   macro avg       0.33      0.36      0.34       494\n",
            "weighted avg       0.33      0.35      0.33       494\n",
            "\n",
            "[[27 17  9 15 13 16]\n",
            " [16 35  2 11 12  9]\n",
            " [ 9  9  3 30  6 26]\n",
            " [15  5  2 38 10  9]\n",
            " [ 4 11  6 10 35  6]\n",
            " [10  7  4 11  9 37]]\n",
            "Accuracy: 0.354251012145749\n",
            "Total f1 over classes: 2.019070070445017\n",
            "Doing analysis with Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.34      0.35        97\n",
            "           1       0.41      0.48      0.45        85\n",
            "           2       0.32      0.24      0.28        83\n",
            "           3       0.30      0.27      0.28        79\n",
            "           4       0.62      0.64      0.63        72\n",
            "           5       0.41      0.51      0.45        78\n",
            "\n",
            "    accuracy                           0.41       494\n",
            "   macro avg       0.40      0.41      0.41       494\n",
            "weighted avg       0.40      0.41      0.40       494\n",
            "\n",
            "[[33 20 14 13  1 16]\n",
            " [12 41  6  9  7 10]\n",
            " [13 12 20 14  5 19]\n",
            " [16  9 16 21  8  9]\n",
            " [ 6  7  3  6 46  4]\n",
            " [10 10  3  8  7 40]]\n",
            "Accuracy: 0.4068825910931174\n",
            "Total f1 over classes: 2.4391378601959737\n",
            "Doing analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.64      0.44        97\n",
            "           1       0.49      0.48      0.49        85\n",
            "           2       0.42      0.17      0.24        83\n",
            "           3       0.35      0.22      0.27        79\n",
            "           4       0.53      0.51      0.52        72\n",
            "           5       0.50      0.46      0.48        78\n",
            "\n",
            "    accuracy                           0.42       494\n",
            "   macro avg       0.44      0.41      0.41       494\n",
            "weighted avg       0.43      0.42      0.40       494\n",
            "\n",
            "[[62 16  5  6  3  5]\n",
            " [25 41  0  7  7  5]\n",
            " [34  8 14  8  4 15]\n",
            " [26  7  9 17 11  9]\n",
            " [16  7  3  7 37  2]\n",
            " [25  4  2  3  8 36]]\n",
            "Accuracy: 0.4190283400809717\n",
            "Total f1 over classes: 2.4334055637347625\n",
            "Doing test analysis with ANN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.42      0.41        33\n",
            "           1       0.56      0.59      0.57        39\n",
            "           2       0.46      0.30      0.36        44\n",
            "           3       0.40      0.24      0.30        33\n",
            "           4       0.44      0.65      0.52        37\n",
            "           5       0.40      0.47      0.43        34\n",
            "\n",
            "    accuracy                           0.45       220\n",
            "   macro avg       0.44      0.45      0.43       220\n",
            "weighted avg       0.45      0.45      0.43       220\n",
            "\n",
            "[[14  9  1  0  5  4]\n",
            " [ 2 23  1  5  5  3]\n",
            " [ 7  4 13  5  9  6]\n",
            " [ 6  1  5  8  7  6]\n",
            " [ 2  1  4  1 24  5]\n",
            " [ 5  3  4  1  5 16]]\n",
            "Accuracy: 0.44545454545454544\n",
            "Total f1 over classes: 2.597966567880432\n",
            "Doing test analysis with SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.39      0.33        33\n",
            "           1       0.38      0.49      0.43        39\n",
            "           2       0.17      0.16      0.17        44\n",
            "           3       0.25      0.27      0.26        33\n",
            "           4       0.50      0.32      0.39        37\n",
            "           5       0.42      0.29      0.34        34\n",
            "\n",
            "    accuracy                           0.32       220\n",
            "   macro avg       0.33      0.32      0.32       220\n",
            "weighted avg       0.33      0.32      0.32       220\n",
            "\n",
            "[[13  4  6  3  1  6]\n",
            " [ 8 19  3  5  2  2]\n",
            " [11  8  7 11  4  3]\n",
            " [ 7  4  7  9  3  3]\n",
            " [ 3 11  8  3 12  0]\n",
            " [ 4  4  9  5  2 10]]\n",
            "Accuracy: 0.3181818181818182\n",
            "Total f1 over classes: 1.9218866572272386\n",
            "Doing test analysis with Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.48      0.43        33\n",
            "           1       0.57      0.59      0.58        39\n",
            "           2       0.42      0.23      0.29        44\n",
            "           3       0.28      0.27      0.28        33\n",
            "           4       0.44      0.51      0.48        37\n",
            "           5       0.36      0.41      0.38        34\n",
            "\n",
            "    accuracy                           0.41       220\n",
            "   macro avg       0.41      0.42      0.41       220\n",
            "weighted avg       0.41      0.41      0.41       220\n",
            "\n",
            "[[16  6  2  0  4  5]\n",
            " [ 5 23  0  6  4  1]\n",
            " [ 6  5 10 12  6  5]\n",
            " [ 5  1  7  9  4  7]\n",
            " [ 3  1  3  4 19  7]\n",
            " [ 7  4  2  1  6 14]]\n",
            "Accuracy: 0.41363636363636364\n",
            "Total f1 over classes: 2.438547515496842\n",
            "Doing test analysis with Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.30      0.28        33\n",
            "           1       0.33      0.23      0.27        39\n",
            "           2       0.17      0.05      0.07        44\n",
            "           3       0.24      0.39      0.30        33\n",
            "           4       0.41      0.38      0.39        37\n",
            "           5       0.26      0.41      0.32        34\n",
            "\n",
            "    accuracy                           0.28       220\n",
            "   macro avg       0.28      0.29      0.27       220\n",
            "weighted avg       0.28      0.28      0.26       220\n",
            "\n",
            "[[10  6  0  4  3 10]\n",
            " [10  9  5  9  5  1]\n",
            " [ 4  4  2 15  7 12]\n",
            " [ 4  3  1 13  2 10]\n",
            " [ 2  3  2  9 14  7]\n",
            " [ 9  2  2  4  3 14]]\n",
            "Accuracy: 0.2818181818181818\n",
            "Total f1 over classes: 1.6333322120111824\n",
            "Doing test analysis with Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.45      0.38        33\n",
            "           1       0.56      0.51      0.53        39\n",
            "           2       0.31      0.23      0.26        44\n",
            "           3       0.33      0.27      0.30        33\n",
            "           4       0.53      0.51      0.52        37\n",
            "           5       0.36      0.47      0.41        34\n",
            "\n",
            "    accuracy                           0.40       220\n",
            "   macro avg       0.40      0.41      0.40       220\n",
            "weighted avg       0.41      0.40      0.40       220\n",
            "\n",
            "[[15  5  3  0  3  7]\n",
            " [ 7 20  3  4  3  2]\n",
            " [11  4 10  9  3  7]\n",
            " [ 5  1  6  9  4  8]\n",
            " [ 2  4  7  1 19  4]\n",
            " [ 5  2  3  4  4 16]]\n",
            "Accuracy: 0.40454545454545454\n",
            "Total f1 over classes: 2.4119109681474495\n",
            "Doing test analysis with Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.82      0.43        33\n",
            "           1       0.57      0.44      0.49        39\n",
            "           2       0.33      0.14      0.19        44\n",
            "           3       0.26      0.18      0.21        33\n",
            "           4       0.59      0.43      0.50        37\n",
            "           5       0.32      0.26      0.29        34\n",
            "\n",
            "    accuracy                           0.37       220\n",
            "   macro avg       0.39      0.38      0.35       220\n",
            "weighted avg       0.40      0.37      0.35       220\n",
            "\n",
            "[[27  3  0  0  0  3]\n",
            " [ 9 17  1  6  4  2]\n",
            " [20  3  6  7  2  6]\n",
            " [14  2  7  6  2  2]\n",
            " [ 8  3  1  3 16  6]\n",
            " [16  2  3  1  3  9]]\n",
            "Accuracy: 0.36818181818181817\n",
            "Total f1 over classes: 2.1161071556097566\n"
          ]
        }
      ]
    }
  ]
}